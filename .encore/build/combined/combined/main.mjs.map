{
  "version": 3,
  "sources": ["../../../../bookmarks/types/domain.types.ts", "../../../../bookmarks/config/daily-digest.config.ts", "../../../../bookmarks/config/prompts.config.ts", "../../../../bookmarks/utils/token-estimator.util.ts", "../../../../bookmarks/services/map-reduce-digest.service.ts", "../../../../encore.gen/internal/clients/bookmarks/endpoints.js", "../../../../encore.gen/internal/clients/users/endpoints.js", "../../../../encore.gen/clients/index.js", "../../../../encore.gen/internal/entrypoints/combined/main.ts", "../../../../users/auth.ts", "../../../../users/config/supabase.config.ts", "../../../../bookmarks/api-test.ts", "../../../../bookmarks/db.ts", "../../../../bookmarks/repositories/bookmark.repository.ts", "../../../../bookmarks/events/bookmark-created.events.ts", "../../../../bookmarks/types/index.ts", "../../../../bookmarks/repositories/daily-digest.repository.ts", "../../../../bookmarks/services/daily-digest.service.ts", "../../../../bookmarks/api.ts", "../../../../encore.gen/internal/auth/auth.ts", "../../../../bookmarks/repositories/transcription.repository.ts", "../../../../users/api.ts", "../../../../users/db.ts", "../../../../users/repositories/user.repository.ts", "../../../../users/webhooks.ts", "../../../../bookmarks/processors/audio-download.processor.ts", "../../../../bookmarks/events/bookmark-source-classified.events.ts", "../../../../bookmarks/events/audio-downloaded.events.ts", "../../../../bookmarks/events/audio-transcribed.events.ts", "../../../../bookmarks/services/podcast-downloader.service.ts", "../../../../bookmarks/storage.ts", "../../../../bookmarks/utils/podcast-url.util.ts", "../../../../bookmarks/services/youtube-downloader.service.ts", "../../../../bookmarks/config/transcription.config.ts", "../../../../bookmarks/utils/youtube-url.util.ts", "../../../../bookmarks/services/gemini.service.ts", "../../../../bookmarks/processors/audio-transcription.processor.ts", "../../../../bookmarks/services/deepgram.service.ts", "../../../../bookmarks/utils/deepgram-extractor.util.ts", "../../../../bookmarks/processors/bookmark-classification.processor.ts", "../../../../bookmarks/utils/bookmark-classifier.util.ts", "../../../../bookmarks/processors/content-extraction.processor.ts", "../../../../bookmarks/events/content-extracted.events.ts", "../../../../bookmarks/services/firecrawl.service.ts", "../../../../bookmarks/config/firecrawl.config.ts", "../../../../bookmarks/repositories/web-content.repository.ts", "../../../../bookmarks/processors/content-summary.processor.ts", "../../../../bookmarks/services/openai.service.ts", "../../../../bookmarks/processors/summary-generation.processor.ts", "../../../../bookmarks/encore.service.ts", "../../../../bookmarks/cron/daily-digest.cron.ts", "../../../../users/encore.service.ts"],
  "sourcesContent": ["import { DeepgramResponse } from \"./deepgram.types\";\n\n// ============================================\n// Bookmark Domain Types\n// ============================================\n\n// Enum for bookmark sources\nexport enum BookmarkSource {\n  YOUTUBE = \"youtube\",\n  PODCAST = \"podcast\",\n  REDDIT = \"reddit\",\n  TWITTER = \"twitter\",\n  LINKEDIN = \"linkedin\",\n  BLOG = \"blog\",\n  WEB = \"web\",\n  OTHER = \"other\",\n}\n\n// Database row interface for bookmarks\nexport interface Bookmark {\n  id: number;\n  user_id: string; // UUID from Supabase users.id\n  url: string;\n  title: string | null;\n  source: BookmarkSource;\n  client_time: Date;\n  metadata: Record<string, any> | null;\n  created_at: Date;\n  updated_at: Date;\n}\n\n// ============================================\n// Transcription Domain Types\n// ============================================\n\n// Transcription processing status\nexport enum TranscriptionStatus {\n  PENDING = \"pending\",\n  PROCESSING = \"processing\",\n  COMPLETED = \"completed\",\n  FAILED = \"failed\",\n}\n\n// Transcription method (Gemini vs Deepgram)\nexport enum TranscriptionMethod {\n  GEMINI = \"gemini\",\n  DEEPGRAM = \"deepgram\",\n}\n\n// Database row interface for transcriptions\nexport interface Transcription {\n  id: number;\n  bookmark_id: number;\n\n  // Transcription data (Stage 2: Deepgram Transcription)\n  transcript: string | null;\n  deepgram_summary: string | null; // Deepgram generated summary\n  sentiment: \"positive\" | \"negative\" | \"neutral\" | null;\n  sentiment_score: number | null;\n  deepgram_response: DeepgramResponse | null;\n  duration: number | null;\n  confidence: number | null;\n\n  // Summary data (Stage 3: OpenAI Summary)\n  summary: string | null; // OpenAI generated summary\n\n  // Method tracking (Gemini vs Deepgram)\n  transcription_method: TranscriptionMethod | null;\n\n  // Status tracking\n  status: TranscriptionStatus;\n  error_message: string | null;\n  processing_started_at: Date | null;\n  processing_completed_at: Date | null;\n  created_at: Date;\n  updated_at: Date;\n}\n", "// ============================================\n// Daily Digest Configuration\n// ============================================\n\nexport const DAILY_DIGEST_CONFIG = {\n  // ============================================\n  // Scheduling Configuration\n  // ============================================\n\n  // Timezone for cron execution\n  timezone: \"GMT\" as const,\n\n  // Cron schedule: 9 PM GMT every day\n  cronSchedule: \"0 21 * * *\" as const,\n\n  // Look back period in hours (24 hours = yesterday's bookmarks)\n  lookbackHours: 24,\n\n  // ============================================\n  // Processing Configuration\n  // ============================================\n\n  // Maximum retries for failed digest generation\n  maxRetries: 3,\n\n  // Timeout for digest generation in seconds\n  timeoutSeconds: 300, // 5 minutes\n\n  // ============================================\n  // Summarization Configuration (Phase 2)\n  // ============================================\n\n  // OpenAI model for digest generation (premium model for best synthesis)\n  openaiModel: \"gpt-4.1\" as const,\n\n  // Maximum output tokens for digest (generous limit for quality)\n  maxOutputTokens: 4000,\n\n  // Temperature for creativity (0.0 = deterministic, 1.0 = creative)\n  temperature: 0.7,\n\n  // ============================================\n  // Map-Reduce Configuration\n  // ============================================\n\n  // Maximum tokens per batch for map phase (leaves room for prompt + output)\n  maxTokensPerBatch: 30000,\n\n  // ============================================\n  // Model Limits (GPT-4.1)\n  // ============================================\n\n  // Maximum input tokens (128K context - reserve buffer)\n  maxInputTokens: 120000,\n\n  // System instructions for digest generation\n  instructions: `You are an expert content curator creating insightful daily digests.\n\nYour goal is to help the user understand the VALUE and CONNECTIONS in their saved content, not just summarize it.\n\nFocus on:\n1. **Themes & Patterns**: What 2-3 main themes emerge across all content? How do they relate?\n2. **Key Insights**: What are the most valuable ideas, facts, or perspectives?\n3. **Connections**: How do different pieces relate or contradict each other?\n4. **Actionable Recommendations**: What should the user prioritize reading/watching based on depth and relevance?\n5. **Personal Relevance**: What might this content mean for the user's interests or goals?\n\nStructure:\n- Start with a compelling TL;DR (2-3 sentences)\n- Organize by themes, not chronologically\n- Use clear markdown sections\n- Be engaging and insightful, not just factual\n- Keep between 800-1200 words\n\nRemember: The user wants to understand WHY this content matters and HOW it fits together, not just WHAT it says.` as const,\n} as const;\n\n// ============================================\n// Helper Functions\n// ============================================\n\n/**\n * Calculate the date range for digest generation\n * @param referenceDate - The date to generate digest for (defaults to yesterday)\n * @returns Object with start and end timestamps\n */\nexport function getDigestDateRange(referenceDate?: Date): {\n  startDate: Date;\n  endDate: Date;\n  digestDate: Date;\n} {\n  const now = referenceDate || new Date();\n\n  // Calculate yesterday's date (the date we're generating digest for)\n  const digestDate = new Date(now);\n  digestDate.setDate(digestDate.getDate() - 1);\n  digestDate.setHours(0, 0, 0, 0); // Start of day\n\n  // Start of yesterday (00:00:00)\n  const startDate = new Date(digestDate);\n\n  // End of yesterday (23:59:59.999)\n  const endDate = new Date(digestDate);\n  endDate.setHours(23, 59, 59, 999);\n\n  return { startDate, endDate, digestDate };\n}\n\n/**\n * Format date to ISO date string (YYYY-MM-DD)\n * @param date - Date to format\n * @returns ISO date string\n */\nexport function formatDigestDate(date: Date): string {\n  return date.toISOString().split(\"T\")[0];\n}\n\n/**\n * Parse ISO date string to Date object\n * @param dateString - ISO date string (YYYY-MM-DD)\n * @returns Date object at start of day\n */\nexport function parseDigestDate(dateString: string): Date {\n  const date = new Date(dateString);\n  date.setHours(0, 0, 0, 0);\n  return date;\n}\n", "import { BookmarkSource } from \"../types/domain.types\";\n\n/**\n * Source-specific OpenAI prompt instructions\n * Each source can have customized summary generation instructions\n */\nexport const SUMMARY_PROMPTS: Record<BookmarkSource, string> = {\n  [BookmarkSource.YOUTUBE]: `You are a helpful assistant that creates concise, informative summaries of YouTube video transcripts.\nFocus on:\n- Main topics and key points discussed\n- Important takeaways and conclusions\n- Any actionable insights or recommendations\nKeep the summary clear and well-structured.`,\n\n  [BookmarkSource.PODCAST]: `You are a helpful assistant that creates concise, informative summaries of podcast episode transcripts.\nFocus on:\n- Main discussion topics and themes\n- Key arguments or insights from the hosts/guests\n- Important quotes or memorable moments\n- Actionable takeaways for listeners\nKeep the summary engaging and well-structured.`,\n\n  [BookmarkSource.REDDIT]: `You are a helpful assistant that summarizes Reddit discussions.\nFocus on:\n- Main post content and context\n- Top-voted comments and perspectives\n- Overall community sentiment\n- Key insights or conclusions from the thread\nKeep the summary balanced and informative.`,\n\n  [BookmarkSource.TWITTER]: `You are a helpful assistant that summarizes Twitter threads.\nFocus on:\n- Main argument or narrative of the thread\n- Key points made by the author\n- Important context or references\n- Overall message and takeaways\nKeep the summary concise and to-the-point.`,\n\n  [BookmarkSource.LINKEDIN]: `You are a helpful assistant that summarizes LinkedIn posts and articles.\nFocus on:\n- Professional insights and perspectives\n- Key business or career advice\n- Important data or examples shared\n- Actionable takeaways for professionals\nKeep the summary professional and value-focused.`,\n\n  [BookmarkSource.BLOG]: `You are a helpful assistant that summarizes blog posts and articles.\nFocus on:\n- Main argument or thesis\n- Key supporting points and evidence\n- Important examples or case studies\n- Conclusions and takeaways\nKeep the summary clear and comprehensive.`,\n\n  [BookmarkSource.WEB]: `You are a helpful assistant that creates concise, informative summaries of web content.\nFocus on:\n- Main topic and purpose\n- Key information and insights\n- Important details and context\n- Overall value and takeaways\nKeep the summary clear and well-structured.`,\n\n  [BookmarkSource.OTHER]: `You are a helpful assistant that creates concise, informative summaries.\nFocus on the main points, key insights, and important takeaways.\nKeep the summary clear and well-structured.`,\n};\n\n/**\n * Default prompt for unknown or new sources\n */\nexport const DEFAULT_SUMMARY_PROMPT = SUMMARY_PROMPTS[BookmarkSource.OTHER];\n\n// ============================================\n// Daily Digest Prompt Templates\n// ============================================\n\n/**\n * Map-Reduce Map Prompt\n * Applied to each batch of summaries to extract themes and insights\n */\nexport const MAP_REDUCE_MAP_PROMPT = `You are the editorial analyst for the audio show \"Memai Daily Briefing\".\nRead the item notes below and emit structured beats the host can stitch together.\n\nItem notes:\n{batch_summaries}\n\nReturn ONLY valid JSON (no prose) matching this schema â€” one object per item, in the SAME order:\n[\n  {{\n    \"item_number\": <integer provided in the notes>,\n    \"group_key\": \"<2-3 word slug in lowercase, reuse exact slug for related items>\",\n    \"theme_title\": \"<5-8 word compelling title>\",\n    \"one_sentence_summary\": \"<<=25 words capturing the core insight>\",\n    \"key_facts\": [\"<=18 word fact 1 with concrete details\", \"fact 2\", \"... optional fact 3\"],\n    \"context_and_implication\": \"<=30 words showing broader stakes/trend>\",\n    \"signals\": \"<=18 words highlighting forward-looking cue or question>\",\n    \"tags\": [\"markets\", \"earnings\", \"...\"],\n    \"source_notes\": \"<short mention of source type, e.g. 'YouTube deep dive'>\"\n  }},\n  ...\n]\n\nRules:\n- If an item lacks numbers, infer a concrete detail from context (e.g. \"signals a sentiment shift among retail investors\").\n- Tags must be lowercase single words; include at least one tag per item.\n- Do NOT add commentary outside the JSON payload.`;\n\n/**\n * Map-Reduce Reduce Prompt\n * Combines intermediate analyses into final digest\n */\nexport const CLUSTER_SUMMARY_PROMPT = `You are crafting a unified theme brief for \"Memai Daily Briefing\".\nCluster slug: {cluster_slug}\nCandidate titles: {candidate_titles}\nAggregate tags: {cluster_tags}\nItems:\n{cluster_items}\n\nRespond in VALID JSON with this exact shape:\n{{\n  \"cluster_title\": \"...\",\n  \"narrative_paragraph\": \"...\", \n  \"key_takeaways\": [\"...\", \"...\"],\n  \"bridge_sentence\": \"...\" \n}}\n\nRules:\n- The narrative paragraph should gracefully combine ALL item facts without bulleting or numbering.\n- Key takeaways must be punchy (<18 words) and non-redundant; include at least two distinct angles.\n- Bridge sentence should hint how to segue into another topic (even if hypothetical).`;\n\nexport const MAP_REDUCE_REDUCE_PROMPT = `You are the showrunner for \"Memai Daily Briefing\".\nDate: {digest_date}. Total items: {total_items} (Audio: {audio_count}, Articles: {article_count}).\n\nYou are given cluster briefs that already blend related items:\n{cluster_briefs}\n\nWrite the final host script as flowing prose (no headings, numbers, or bullet lists). Target 4-5 paragraphs:\n- Paragraph 1: Cold open weaving the strongest cluster into an overarching narrative hook.\n- Middle paragraphs: One per remaining cluster. Integrate their narratives, cite sources conversationally (e.g. \"a YouTube deep dive\" or \"today's blog breakdown\"), and explain the stakes and connections.\n- Closing paragraph: Synthesize the day, offer a forward-looking takeaway or question, and sign off with momentum.\n\nGuidelines:\n- Reuse key takeaways organically; do not repeat them verbatim.\n- Mention variety of sources only when it adds colour or credibility.\n- Keep total length 550-850 words with varied sentence rhythm.\n- Absolutely avoid explicit section labels, bullets, or enumerated lists.`;\n\n// ============================================\n// Metadata Formatting Helpers\n// ============================================\n\n/**\n * Format source name for display\n */\nexport function formatSourceName(source: BookmarkSource): string {\n  const sourceNames: Record<BookmarkSource, string> = {\n    [BookmarkSource.YOUTUBE]: \"YouTube Video\",\n    [BookmarkSource.PODCAST]: \"Podcast Episode\",\n    [BookmarkSource.REDDIT]: \"Reddit Post\",\n    [BookmarkSource.TWITTER]: \"Twitter Thread\",\n    [BookmarkSource.LINKEDIN]: \"LinkedIn Article\",\n    [BookmarkSource.BLOG]: \"Blog Post\",\n    [BookmarkSource.WEB]: \"Web Article\",\n    [BookmarkSource.OTHER]: \"Other Content\",\n  };\n\n  return sourceNames[source] || source;\n}\n", "/**\n * Token estimation utilities for OpenAI API usage\n * Uses character-based approximation (chars / 4) as recommended by OpenAI\n */\n\n/**\n * Estimates token count from text using character approximation\n * @param text - Text to estimate tokens for\n * @returns Estimated token count\n */\nexport function estimateTokenCount(text: string): number {\n  if (!text) return 0;\n  // OpenAI recommends: 1 token â‰ˆ 4 characters for English text\n  return Math.ceil(text.length / 4);\n}\n\n/**\n * Estimates total tokens for multiple text pieces\n * @param texts - Array of text strings\n * @returns Total estimated token count\n */\nexport function estimateTotalTokens(texts: string[]): number {\n  return texts.reduce((total, text) => total + estimateTokenCount(text), 0);\n}\n\n/**\n * Validates if text fits within context window limits\n * @param text - Text to validate\n * @param maxTokens - Maximum token limit for the model\n * @param reservedTokens - Tokens to reserve for prompt/system instructions (default: 1000)\n * @returns True if text fits within limits\n */\nexport function validateContextWindow(\n  text: string,\n  maxTokens: number,\n  reservedTokens: number = 1000\n): boolean {\n  const estimatedTokens = estimateTokenCount(text);\n  return estimatedTokens <= maxTokens - reservedTokens;\n}\n\n/**\n * Calculates optimal batch size for processing large sets of summaries\n * @param summaries - Array of summary texts\n * @param maxTokensPerBatch - Maximum tokens allowed per batch\n * @param overheadTokens - Tokens for prompt template overhead (default: 500)\n * @returns Optimal number of summaries per batch\n */\nexport function calculateBatchSize(\n  summaries: string[],\n  maxTokensPerBatch: number,\n  overheadTokens: number = 500\n): number {\n  if (summaries.length === 0) return 0;\n\n  const availableTokens = maxTokensPerBatch - overheadTokens;\n  const avgTokensPerSummary = estimateTotalTokens(summaries) / summaries.length;\n\n  const optimalBatchSize = Math.floor(availableTokens / avgTokensPerSummary);\n\n  // Return at least 1, but no more than total summaries\n  return Math.max(1, Math.min(optimalBatchSize, summaries.length));\n}\n\n/**\n * Splits summaries into batches based on token limits\n * @param summaries - Array of summary texts\n * @param maxTokensPerBatch - Maximum tokens per batch\n * @returns Array of batches\n */\nexport function batchSummaries(\n  summaries: string[],\n  maxTokensPerBatch: number\n): string[][] {\n  if (summaries.length === 0) return [];\n\n  const batches: string[][] = [];\n  let currentBatch: string[] = [];\n  let currentBatchTokens = 0;\n\n  for (const summary of summaries) {\n    const summaryTokens = estimateTokenCount(summary);\n\n    // If adding this summary would exceed limit, start new batch\n    if (\n      currentBatchTokens + summaryTokens > maxTokensPerBatch &&\n      currentBatch.length > 0\n    ) {\n      batches.push(currentBatch);\n      currentBatch = [];\n      currentBatchTokens = 0;\n    }\n\n    currentBatch.push(summary);\n    currentBatchTokens += summaryTokens;\n  }\n\n  // Add final batch if not empty\n  if (currentBatch.length > 0) {\n    batches.push(currentBatch);\n  }\n\n  return batches;\n}\n\n/**\n * Gets token usage statistics for a set of summaries\n * @param summaries - Array of summary texts\n * @returns Token statistics\n */\nexport function getTokenStats(summaries: string[]): {\n  totalTokens: number;\n  avgTokens: number;\n  minTokens: number;\n  maxTokens: number;\n  count: number;\n} {\n  if (summaries.length === 0) {\n    return {\n      totalTokens: 0,\n      avgTokens: 0,\n      minTokens: 0,\n      maxTokens: 0,\n      count: 0,\n    };\n  }\n\n  const tokenCounts = summaries.map(estimateTokenCount);\n  const totalTokens = tokenCounts.reduce((sum, count) => sum + count, 0);\n\n  return {\n    totalTokens,\n    avgTokens: Math.round(totalTokens / summaries.length),\n    minTokens: Math.min(...tokenCounts),\n    maxTokens: Math.max(...tokenCounts),\n    count: summaries.length,\n  };\n}\n", "import log from \"encore.dev/log\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { Document } from \"@langchain/core/documents\";\nimport { DigestContentItem } from \"../types/web-content.types\";\nimport { DAILY_DIGEST_CONFIG } from \"../config/daily-digest.config\";\nimport {\n  MAP_REDUCE_MAP_PROMPT,\n  MAP_REDUCE_REDUCE_PROMPT,\n  CLUSTER_SUMMARY_PROMPT,\n  formatSourceName,\n} from \"../config/prompts.config\";\nimport { batchSummaries, estimateTokenCount } from \"../utils/token-estimator.util\";\n\nexport interface DigestNarrativeContext {\n  digestDate?: string;\n  totalItems?: number;\n  audioCount?: number;\n  articleCount?: number;\n}\n\ninterface MapBeat {\n  itemNumber: number;\n  groupKey: string;\n  rawGroupKey: string;\n  themeTitle: string;\n  oneSentenceSummary: string;\n  keyFacts: string[];\n  contextAndImplication: string;\n  signals: string;\n  tags: string[];\n  sourceNotes: string;\n}\n\ninterface ThemeCluster {\n  slug: string;\n  beats: MapBeat[];\n  tags: Set<string>;\n}\n\ninterface ClusterSummary {\n  slug: string;\n  title: string;\n  narrative: string;\n  keyTakeaways: string[];\n  bridgeSentence: string;\n  tags: string[];\n}\n\nfunction slugify(value: string, fallback: string): string {\n  const base = value?.trim().toLowerCase() || fallback.trim().toLowerCase();\n  const slug = base\n    .replace(/[^a-z0-9\\s-]/g, \"\")\n    .replace(/\\s+/g, \"-\")\n    .replace(/-+/g, \"-\")\n    .replace(/^-|-$/g, \"\");\n  return slug || \"general\";\n}\n\nfunction normalizeTags(tags: string[] | undefined): string[] {\n  if (!tags) return [];\n  return Array.from(\n    new Set(\n      tags\n        .flatMap((tag) =>\n          tag\n            .split(/[,\\s]+/)\n            .map((t) => t.trim().toLowerCase())\n            .filter(Boolean)\n        )\n    )\n  );\n}\n\nfunction termOverlap(a: string, b: string): number {\n  const tokenize = (value: string) =>\n    value\n      .toLowerCase()\n      .split(/[^a-z0-9]+/)\n      .filter((token) => token.length > 3);\n\n  const setA = new Set(tokenize(a));\n  const setB = new Set(tokenize(b));\n  if (!setA.size || !setB.size) return 0;\n  let intersection = 0;\n  setA.forEach((token) => {\n    if (setB.has(token)) intersection += 1;\n  });\n  return intersection / Math.min(setA.size, setB.size);\n}\n\nfunction extractJsonPayload(text: string): any {\n  const trimmed = text.trim();\n  try {\n    return JSON.parse(trimmed);\n  } catch {\n    // Try fenced code block\n    const match = trimmed.match(/```(?:json)?\\s*([\\s\\S]*?)```/i);\n    if (match) {\n      return extractJsonPayload(match[1]);\n    }\n    // Try to locate array/object via first bracket\n    const firstBracket = trimmed.indexOf(\"[\");\n    const firstBrace = trimmed.indexOf(\"{\");\n    const start = firstBracket !== -1 ? firstBracket : firstBrace;\n    if (start !== -1) {\n      const snippet = trimmed.slice(start);\n      try {\n        return JSON.parse(snippet);\n      } catch {\n        // fall through\n      }\n    }\n    throw new Error(\"Unable to parse JSON payload from LLM response\");\n  }\n}\n\n/**\n * Format DigestContentItems with metadata for LLM prompts\n * Handles both audio and article content types\n */\nfunction formatContentItemsWithMetadata(\n  items: DigestContentItem[],\n  startIndex: number\n): string {\n  return items.map((item, idx) => {\n    const sourceName = formatSourceName(item.source);\n    const contentType = item.content_type === 'audio' ? 'ðŸŽ§ Audio' : 'ðŸ“„ Article';\n\n    const title = item.title || sourceName;\n    const createdAt = item.created_at\n      ? new Date(item.created_at).toISOString()\n      : \"unknown\";\n    const itemNumber = startIndex + idx + 1;\n\n    const durationStr =\n      item.content_type === \"audio\" && item.duration\n        ? `${Math.max(1, Math.round(item.duration / 60))} min runtime`\n        : null;\n\n    const readingStr =\n      item.content_type === \"article\" && item.reading_minutes\n        ? `${item.reading_minutes} min read`\n        : null;\n\n    const sentimentStr = item.sentiment\n      ? `tone: ${item.sentiment}`\n      : null;\n\n    const metaParts = [\n      durationStr,\n      readingStr,\n      sentimentStr,\n    ].filter(Boolean);\n\n    const metadataLine = metaParts.length\n      ? `Meta: ${metaParts.join(\" Â· \")}`\n      : null;\n\n    return `[ITEM ${itemNumber}]\nType: ${contentType}\nTitle: ${title}\nSource: ${sourceName}\nCaptured: ${createdAt}\n${metadataLine ? metadataLine + \"\\n\" : \"\"}Summary:\n${item.summary}\n---`;\n  }).join('\\n\\n');\n}\n\n/**\n * Universal Map-Reduce Digest Service with LangChain\n * Handles any number of bookmarks (1 to 1000+) using intelligent batching\n */\nexport class MapReduceDigestService {\n  private readonly llm: ChatOpenAI;\n\n  constructor(openaiApiKey: string) {\n    this.llm = new ChatOpenAI({\n      model: DAILY_DIGEST_CONFIG.openaiModel,\n      temperature: DAILY_DIGEST_CONFIG.temperature,\n      maxTokens: DAILY_DIGEST_CONFIG.maxOutputTokens,\n      apiKey: openaiApiKey,\n    });\n  }\n\n  private clusterBeats(beats: MapBeat[]): ThemeCluster[] {\n    const clusters: ThemeCluster[] = [];\n    const slugMap = new Map<string, ThemeCluster>();\n\n    for (const beat of beats) {\n      const normalizedSlug = beat.groupKey || slugify(beat.themeTitle, \"general\");\n      let cluster = slugMap.get(normalizedSlug);\n\n      if (!cluster) {\n        cluster = this.findClusterForBeat(normalizedSlug, beat, clusters);\n        if (!cluster) {\n          cluster = {\n            slug: normalizedSlug,\n            beats: [],\n            tags: new Set<string>(),\n          };\n          clusters.push(cluster);\n        }\n        slugMap.set(normalizedSlug, cluster);\n      }\n\n      cluster.beats.push(beat);\n      beat.tags.forEach((tag) => cluster!.tags.add(tag));\n    }\n\n    return clusters;\n  }\n\n  private findClusterForBeat(\n    slug: string,\n    beat: MapBeat,\n    clusters: ThemeCluster[]\n  ): ThemeCluster | undefined {\n    const beatTags = new Set(beat.tags);\n\n    // Prefer clusters with same slug already present\n    const directMatch = clusters.find((cluster) => cluster.slug === slug);\n    if (directMatch) {\n      return directMatch;\n    }\n\n    let bestMatch: { cluster: ThemeCluster; score: number } | undefined;\n\n    for (const cluster of clusters) {\n      const clusterTags = cluster.tags;\n      let overlap = 0;\n      beatTags.forEach((tag) => {\n        if (clusterTags.has(tag)) overlap += 1;\n      });\n\n      const tagScore =\n        beatTags.size && clusterTags.size\n          ? overlap / Math.min(beatTags.size, clusterTags.size)\n          : 0;\n\n      const titleScore = termOverlap(\n        beat.themeTitle,\n        cluster.beats[0]?.themeTitle || \"\"\n      );\n\n      const combinedScore = Math.max(tagScore, titleScore);\n\n      if (combinedScore >= 0.5) {\n        if (!bestMatch || combinedScore > bestMatch.score) {\n          bestMatch = { cluster, score: combinedScore };\n        }\n      }\n    }\n\n    return bestMatch?.cluster;\n  }\n\n  private async summarizeClusters(\n    clusters: ThemeCluster[]\n  ): Promise<ClusterSummary[]> {\n    const summaries: ClusterSummary[] = [];\n\n    for (const [index, cluster] of clusters.entries()) {\n      const candidateTitles = Array.from(\n        new Set(cluster.beats.map((beat) => beat.themeTitle).filter(Boolean))\n      );\n\n      const clusterItems = cluster.beats\n        .map((beat) => {\n          const facts = beat.keyFacts.map((fact) => `â€¢ ${fact}`).join(\"\\n\");\n          const tags = beat.tags.join(\", \") || \"none\";\n          return `Item ${beat.itemNumber}:\n  Title: ${beat.themeTitle}\n  Summary: ${beat.oneSentenceSummary}\n  Key facts:\n${facts ? facts : \"â€¢ (fact missing)\"}\n  Context: ${beat.contextAndImplication}\n  Signals: ${beat.signals}\n  Tags: ${tags}\n  Source: ${beat.sourceNotes}`;\n        })\n        .join(\"\\n\\n\");\n\n      const prompt = CLUSTER_SUMMARY_PROMPT.replace(\n        \"{cluster_slug}\",\n        cluster.slug\n      )\n        .replace(\"{candidate_titles}\", candidateTitles.join(\" | \") || cluster.slug)\n        .replace(\"{cluster_tags}\", Array.from(cluster.tags).join(\", \") || \"general\")\n        .replace(\"{cluster_items}\", clusterItems);\n\n      const response = await this.llm.invoke(prompt);\n      const payload = extractJsonPayload(response.content.toString());\n\n      if (!payload?.cluster_title || !payload?.narrative_paragraph) {\n        throw new Error(\n          `Cluster summary missing fields for slug ${cluster.slug}`\n        );\n      }\n\n      const keyTakeaways = Array.isArray(payload.key_takeaways)\n        ? payload.key_takeaways.map((t: any) => t.toString())\n        : [];\n\n      summaries.push({\n        slug: cluster.slug,\n        title: payload.cluster_title.toString(),\n        narrative: payload.narrative_paragraph.toString(),\n        keyTakeaways,\n        bridgeSentence: (payload.bridge_sentence || \"\").toString(),\n        tags: Array.from(cluster.tags),\n      });\n    }\n\n    return summaries;\n  }\n\n  /**\n   * Generates digest using LangChain map-reduce\n   * Intelligently handles any number of content items (audio + web)\n   * @param contentItems - Array of completed content items (audio transcriptions + web content)\n   * @returns Final digest text\n   */\n  async generateDigest(\n    contentItems: DigestContentItem[],\n    context?: DigestNarrativeContext\n  ): Promise<string> {\n    const audioCount = contentItems.filter(c => c.content_type === 'audio').length;\n    const articleCount = contentItems.filter(c => c.content_type === 'article').length;\n    const totalItems = contentItems.length;\n\n    log.info(\"Starting map-reduce digest generation\", {\n      contentItemCount: totalItems,\n      audioCount,\n      articleCount,\n    });\n\n    try {\n      // Step 1: Extract summaries from unified content items\n      const summaries = contentItems.map((item) => item.summary || \"No summary available\");\n\n      log.info(\"Prepared summaries for processing\", {\n        summaryCount: summaries.length,\n        totalTokensEstimate: summaries.reduce(\n          (sum, s) => sum + estimateTokenCount(s),\n          0\n        ),\n      });\n\n      // Step 2: Batch summaries based on token limits\n      const batches = batchSummaries(\n        summaries,\n        DAILY_DIGEST_CONFIG.maxTokensPerBatch\n      );\n\n      log.info(\"Created batches for map phase\", {\n        batchCount: batches.length,\n        avgBatchSize: Math.round(\n          batches.reduce((sum, b) => sum + b.length, 0) / batches.length\n        ),\n      });\n\n      // Step 3: Map phase - Process each batch into structured beats\n      const mapBeats = await this.mapPhase(batches, contentItems);\n\n      log.info(\"Map phase completed\", {\n        beatCount: mapBeats.length,\n      });\n\n      // Step 4: Cluster beats into coherent themes\n      const clusters = this.clusterBeats(mapBeats);\n\n      log.info(\"Clustering completed\", {\n        clusterCount: clusters.length,\n        clusterSlugs: clusters.map((c) => c.slug),\n      });\n\n      // Step 5: Summarise clusters into narrative-ready briefs\n      const clusterSummaries = await this.summarizeClusters(clusters);\n\n      log.info(\"Cluster summarisation completed\", {\n        clusterSummaryCount: clusterSummaries.length,\n      });\n\n      // Step 6: Reduce phase - Combine cluster summaries into final digest\n      const finalDigest = await this.reducePhase(clusterSummaries, {\n        digestDate: context?.digestDate,\n        totalItems: context?.totalItems ?? totalItems,\n        audioCount: context?.audioCount ?? audioCount,\n        articleCount: context?.articleCount ?? articleCount,\n      });\n\n      log.info(\"Map-reduce digest generation completed\", {\n        finalDigestLength: finalDigest.length,\n      });\n\n      return finalDigest;\n    } catch (error) {\n      log.error(error, \"Map-reduce digest generation failed\");\n      throw new Error(\n        `Map-reduce digest generation failed: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  }\n\n  /**\n   * Map phase: Process batches of summaries in parallel\n   * @param batches - Array of summary batches (strings)\n   * @param contentItems - Original content items for metadata\n   * @returns Array of intermediate summaries\n   */\n  private async mapPhase(\n    batches: string[][],\n    contentItems: DigestContentItem[]\n  ): Promise<MapBeat[]> {\n    log.info(\"Starting map phase\", { batchCount: batches.length });\n\n    const beats: MapBeat[] = [];\n    let currentIndex = 0;\n\n    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {\n      const batch = batches[batchIndex];\n      const batchContentItems = contentItems.slice(\n        currentIndex,\n        currentIndex + batch.length\n      );\n\n      log.info(\"Processing batch\", {\n        batchIndex,\n        batchSize: batch.length,\n        startIndex: currentIndex,\n      });\n\n      const formattedBatch = formatContentItemsWithMetadata(\n        batchContentItems,\n        currentIndex\n      );\n\n      const prompt = MAP_REDUCE_MAP_PROMPT.replace(\n        \"{batch_summaries}\",\n        formattedBatch\n      );\n\n      const response = await this.llm.invoke(prompt);\n      const payload = extractJsonPayload(response.content.toString());\n\n      if (!Array.isArray(payload)) {\n        throw new Error(\"Map phase response was not an array\");\n      }\n\n      if (payload.length !== batch.length) {\n        log.warn(\"Map response length mismatch, will align best effort\", {\n          expected: batch.length,\n          received: payload.length,\n        });\n      }\n\n      payload.forEach((rawBeat: any, idx: number) => {\n        const itemNumber =\n          typeof rawBeat?.item_number === \"number\"\n            ? rawBeat.item_number\n            : currentIndex + idx + 1;\n\n        const cleanKey = slugify(\n          rawBeat?.group_key || \"\",\n          rawBeat?.theme_title || \"\"\n        );\n\n        const tags = normalizeTags(rawBeat?.tags);\n\n        beats.push({\n          itemNumber,\n          groupKey: cleanKey,\n          rawGroupKey: (rawBeat?.group_key || \"\").toString(),\n          themeTitle: (rawBeat?.theme_title || \"\").toString(),\n          oneSentenceSummary: (rawBeat?.one_sentence_summary || \"\").toString(),\n          keyFacts:\n            Array.isArray(rawBeat?.key_facts) && rawBeat.key_facts.length\n              ? rawBeat.key_facts.map((f: any) => f.toString())\n              : [],\n          contextAndImplication: (\n            rawBeat?.context_and_implication || \"\"\n          ).toString(),\n          signals: (rawBeat?.signals || \"\").toString(),\n          tags,\n          sourceNotes: (rawBeat?.source_notes || \"\").toString(),\n        });\n      });\n\n      currentIndex += batch.length;\n    }\n\n    return beats;\n  }\n\n  private async reducePhase(\n    clusterSummaries: ClusterSummary[],\n    context: DigestNarrativeContext\n  ): Promise<string> {\n    log.info(\"Starting reduce phase\", {\n      clusterCount: clusterSummaries.length,\n    });\n\n    if (clusterSummaries.length === 0) {\n      throw new Error(\"No cluster summaries available for reduction\");\n    }\n\n    const clusterBriefs = clusterSummaries\n      .map((cluster, idx) => {\n        const takeaways = cluster.keyTakeaways\n          .map((t) => `- ${t}`)\n          .join(\"\\n\");\n        return `Cluster ${idx + 1} (${cluster.slug})\nTitle: ${cluster.title}\nNarrative: ${cluster.narrative}\nKeyTakeaways:\n${takeaways || \"-\"}\nBridge: ${cluster.bridgeSentence}\nTags: ${cluster.tags.join(\", \") || \"general\"}`;\n      })\n      .join(\"\\n\\n\");\n\n    const reducePrompt = MAP_REDUCE_REDUCE_PROMPT\n      .replace(\"{cluster_briefs}\", clusterBriefs)\n      .replace(\"{digest_date}\", context.digestDate || \"Today\")\n      .replace(\n        \"{total_items}\",\n        String(context.totalItems ?? clusterSummaries.length)\n      )\n      .replace(\"{audio_count}\", String(context.audioCount ?? 0))\n      .replace(\"{article_count}\", String(context.articleCount ?? 0));\n\n    const result = await this.llm.invoke(reducePrompt);\n    return result.content.toString();\n  }\n}\n", "import { apiCall, streamIn, streamOut, streamInOut } from \"encore.dev/internal/codegen/api\";\n\nconst TEST_ENDPOINTS = typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\"\n    ? await import(\"./endpoints_testing.js\")\n    : null;\n\nexport async function createTest(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.createTest(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"createTest\", params, opts);\n}\nexport async function generateDigestTest(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.generateDigestTest(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"generateDigestTest\", params, opts);\n}\nexport async function create(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.create(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"create\", params, opts);\n}\nexport async function get(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.get(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"get\", params, opts);\n}\nexport async function list(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.list(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"list\", params, opts);\n}\nexport async function update(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.update(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"update\", params, opts);\n}\nexport async function remove(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.remove(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"remove\", params, opts);\n}\nexport async function getDetails(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.getDetails(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"getDetails\", params, opts);\n}\nexport async function generateDailyDigest(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.generateDailyDigest(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"generateDailyDigest\", params, opts);\n}\nexport async function getDailyDigest(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.getDailyDigest(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"getDailyDigest\", params, opts);\n}\nexport async function listDailyDigests(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.listDailyDigests(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"listDailyDigests\", params, opts);\n}\nexport async function generateYesterdaysDigest(opts) {\n    const params = undefined;\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.generateYesterdaysDigest(params, opts);\n    }\n\n    return apiCall(\"bookmarks\", \"generateYesterdaysDigest\", params, opts);\n}\n", "import { apiCall, streamIn, streamOut, streamInOut } from \"encore.dev/internal/codegen/api\";\n\nconst TEST_ENDPOINTS = typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\"\n    ? await import(\"./endpoints_testing.js\")\n    : null;\n\nexport async function me(opts) {\n    const params = undefined;\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.me(params, opts);\n    }\n\n    return apiCall(\"users\", \"me\", params, opts);\n}\nexport async function updateProfile(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.updateProfile(params, opts);\n    }\n\n    return apiCall(\"users\", \"updateProfile\", params, opts);\n}\nexport async function getUserIds(opts) {\n    const params = undefined;\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.getUserIds(params, opts);\n    }\n\n    return apiCall(\"users\", \"getUserIds\", params, opts);\n}\nexport async function userCreated(params, opts) {\n    if (typeof ENCORE_DROP_TESTS === \"undefined\" && process.env.NODE_ENV === \"test\") {\n        return TEST_ENDPOINTS.userCreated(params, opts);\n    }\n\n    return apiCall(\"users\", \"userCreated\", params, opts);\n}\n", "export * as bookmarks from \"../internal/clients/bookmarks/endpoints.js\";\nexport * as users from \"../internal/clients/users/endpoints.js\";\n", "import { registerGateways, registerHandlers, run, type Handler } from \"encore.dev/internal/codegen/appinit\";\n\nimport { gateway as api_gatewayGW } from \"../../../../users/auth\";\nimport { createTest as bookmarks_createTestImpl0 } from \"../../../../bookmarks/api-test\";\nimport { generateDigestTest as bookmarks_generateDigestTestImpl1 } from \"../../../../bookmarks/api-test\";\nimport { create as bookmarks_createImpl2 } from \"../../../../bookmarks/api\";\nimport { get as bookmarks_getImpl3 } from \"../../../../bookmarks/api\";\nimport { list as bookmarks_listImpl4 } from \"../../../../bookmarks/api\";\nimport { update as bookmarks_updateImpl5 } from \"../../../../bookmarks/api\";\nimport { remove as bookmarks_removeImpl6 } from \"../../../../bookmarks/api\";\nimport { getDetails as bookmarks_getDetailsImpl7 } from \"../../../../bookmarks/api\";\nimport { generateDailyDigest as bookmarks_generateDailyDigestImpl8 } from \"../../../../bookmarks/api\";\nimport { getDailyDigest as bookmarks_getDailyDigestImpl9 } from \"../../../../bookmarks/api\";\nimport { listDailyDigests as bookmarks_listDailyDigestsImpl10 } from \"../../../../bookmarks/api\";\nimport { generateYesterdaysDigest as bookmarks_generateYesterdaysDigestImpl11 } from \"../../../../bookmarks/api\";\nimport { me as users_meImpl12 } from \"../../../../users/api\";\nimport { updateProfile as users_updateProfileImpl13 } from \"../../../../users/api\";\nimport { getUserIds as users_getUserIdsImpl14 } from \"../../../../users/api\";\nimport { userCreated as users_userCreatedImpl15 } from \"../../../../users/webhooks\";\nimport \"../../../../bookmarks/processors/audio-download.processor\";\nimport \"../../../../bookmarks/processors/audio-transcription.processor\";\nimport \"../../../../bookmarks/processors/bookmark-classification.processor\";\nimport \"../../../../bookmarks/processors/content-extraction.processor\";\nimport \"../../../../bookmarks/processors/content-summary.processor\";\nimport \"../../../../bookmarks/processors/summary-generation.processor\";\nimport * as bookmarks_service from \"../../../../bookmarks/encore.service\";\nimport * as users_service from \"../../../../users/encore.service\";\n\nconst gateways: any[] = [\n    api_gatewayGW,\n];\n\nconst handlers: Handler[] = [\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"createTest\",\n            handler:           bookmarks_createTestImpl0,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":false,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"generateDigestTest\",\n            handler:           bookmarks_generateDigestTestImpl1,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":false,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"create\",\n            handler:           bookmarks_createImpl2,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"get\",\n            handler:           bookmarks_getImpl3,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"list\",\n            handler:           bookmarks_listImpl4,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"update\",\n            handler:           bookmarks_updateImpl5,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"remove\",\n            handler:           bookmarks_removeImpl6,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"getDetails\",\n            handler:           bookmarks_getDetailsImpl7,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"generateDailyDigest\",\n            handler:           bookmarks_generateDailyDigestImpl8,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"getDailyDigest\",\n            handler:           bookmarks_getDailyDigestImpl9,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"listDailyDigests\",\n            handler:           bookmarks_listDailyDigestsImpl10,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"bookmarks\",\n            name:              \"generateYesterdaysDigest\",\n            handler:           bookmarks_generateYesterdaysDigestImpl11,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":false,\"auth\":false,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: bookmarks_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"users\",\n            name:              \"me\",\n            handler:           users_meImpl12,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: users_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"users\",\n            name:              \"updateProfile\",\n            handler:           users_updateProfileImpl13,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":true,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: users_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"users\",\n            name:              \"getUserIds\",\n            handler:           users_getUserIdsImpl14,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":false,\"auth\":false,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: users_service.default.cfg.middlewares || [],\n    },\n    {\n        apiRoute: {\n            service:           \"users\",\n            name:              \"userCreated\",\n            handler:           users_userCreatedImpl15,\n            raw:               false,\n            streamingRequest:  false,\n            streamingResponse: false,\n        },\n        endpointOptions: {\"expose\":true,\"auth\":false,\"isRaw\":false,\"isStream\":false,\"tags\":[]},\n        middlewares: users_service.default.cfg.middlewares || [],\n    },\n];\n\nregisterGateways(gateways);\nregisterHandlers(handlers);\n\nawait run(import.meta.url);\n", "import { Header, Gateway, APIError } from \"encore.dev/api\";\nimport { authHandler } from \"encore.dev/auth\";\nimport { jwtVerify, createRemoteJWKSet, decodeJwt } from \"jose\";\nimport log from \"encore.dev/log\";\nimport { appMeta } from \"encore.dev\";\nimport { SUPABASE_CONFIG } from \"./config/supabase.config\";\nimport { SupabaseJWTPayload } from \"./types\";\n\n/**\n * AuthParams defines the authentication parameters\n * We extract the Authorization header containing the JWT token\n */\ninterface AuthParams {\n  authorization: Header<\"Authorization\">;\n}\n\n/**\n * AuthData defines the authenticated user information returned by the auth handler\n * NOTE: userID must be a string per Encore.ts requirements\n */\ninterface AuthData {\n  userID: string; // Supabase user UUID\n  email: string;\n}\n\n/**\n * JWKS (JSON Web Key Set) for verifying Supabase JWTs\n * This is cached by jose library and fetched from Supabase\n * Uses asymmetric key validation (RS256) for security\n */\nconst SUPABASE_JWKS = createRemoteJWKSet(\n  new URL(SUPABASE_CONFIG.jwksEndpoint())\n);\n\n/**\n * Auth handler that validates Supabase JWT tokens\n * Called automatically by Encore for endpoints with auth: true\n *\n * Flow:\n * 1. Extract JWT from Authorization header\n * 2. Verify JWT using Supabase JWKS (asymmetric key validation) OR decode in test mode\n * 3. Extract user info from JWT claims\n * 4. Return auth data for use in protected endpoints\n *\n * Test Mode:\n * - In test environment (appMeta().environment.type === \"test\"), JWTs are decoded\n *   without signature verification to allow integration tests with test JWTs\n */\nexport const auth = authHandler<AuthParams, AuthData>(\n  async (params) => {\n    try {\n      // Extract token from \"Bearer <token>\" format\n      if (!params.authorization || !params.authorization.startsWith(\"Bearer \")) {\n        throw new Error(\n          \"Invalid Authorization header format. Expected: Bearer <token>\"\n        );\n      }\n\n      const token = params.authorization.substring(7); // Remove \"Bearer \" prefix\n\n      if (!token) {\n        throw new Error(\"No token provided in Authorization header\");\n      }\n\n      // Check if we're in test mode\n      // Test mode can be enabled either by:\n      // 1. Running in test environment (encore test)\n      // 2. Setting ENABLE_TEST_AUTH=true environment variable (for dev server during tests)\n      const env = appMeta().environment;\n      const isTest = env.type === \"test\" || process.env.ENABLE_TEST_AUTH === \"true\";\n\n      let supabasePayload: SupabaseJWTPayload;\n\n      if (isTest) {\n        // TEST MODE: Decode JWT without verification\n        // This allows integration tests to use test-generated JWTs\n        log.info(\"Test mode: decoding JWT without JWKS verification\");\n\n        const decoded = decodeJwt(token);\n        supabasePayload = decoded as unknown as SupabaseJWTPayload;\n      } else {\n        // PRODUCTION MODE: Verify JWT using Supabase JWKS\n        // This validates:\n        // - Signature (using public key from JWKS)\n        // - Expiration (exp claim)\n        // - Issuer (iss claim must match Supabase)\n        const { payload } = await jwtVerify(token, SUPABASE_JWKS, {\n          issuer: SUPABASE_CONFIG.authEndpoint(), // Verify issuer matches Supabase\n        });\n\n        supabasePayload = payload as unknown as SupabaseJWTPayload;\n      }\n\n      // Extract user info from JWT\n      const authData: AuthData = {\n        userID: supabasePayload.sub, // Supabase user UUID\n        email: supabasePayload.email || \"\",\n      };\n\n      log.info(\"User authenticated successfully\", {\n        userID: authData.userID,\n        email: authData.email,\n        testMode: isTest,\n      });\n\n      return authData;\n    } catch (error) {\n      log.warn(\"JWT validation failed\", {\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      // Throw unauthenticated error to reject the request\n      throw APIError.unauthenticated(\n        error instanceof Error ? error.message : \"Authentication failed\"\n      );\n    }\n  }\n);\n\n/**\n * Gateway configuration with auth handler\n * This makes the auth handler available to all services in the application\n */\nexport const gateway = new Gateway({\n  authHandler: auth,\n});\n", "import { secret } from \"encore.dev/config\";\n\n/**\n * Supabase Configuration\n * Manages Supabase project credentials and settings\n *\n * Setup Instructions:\n * 1. Create a Supabase project at https://supabase.com/dashboard\n * 2. Get credentials from: Project Settings â†’ API\n * 3. Set secrets locally:\n *    encore secret set --type local SupabaseURL\n *    encore secret set --type local SupabaseAnonKey\n *    encore secret set --type local SupabaseServiceRoleKey\n * 4. Set secrets for production:\n *    encore secret set --type prod SupabaseURL\n *    encore secret set --type prod SupabaseAnonKey\n *    encore secret set --type prod SupabaseServiceRoleKey\n */\n\n// Supabase project URL (e.g., https://xxxxx.supabase.co)\nconst supabaseURL = secret(\"SupabaseURL\");\n\n// Supabase anon/public key (safe for client-side use)\nconst supabaseAnonKey = secret(\"SupabaseAnonKey\");\n\n// Supabase service role key (admin privileges - NEVER expose to clients!)\nconst supabaseServiceRoleKey = secret(\"SupabaseServiceRoleKey\");\n\n/**\n * Supabase configuration object\n * Centralizes all Supabase-related settings\n */\nexport const SUPABASE_CONFIG = {\n  /**\n   * Project URL\n   * Format: https://xxxxx.supabase.co\n   */\n  url: () => supabaseURL(),\n\n  /**\n   * Anonymous/Public key\n   * Safe to use in frontend applications\n   * Respects Row Level Security (RLS) policies\n   */\n  anonKey: () => supabaseAnonKey(),\n\n  /**\n   * Service role key\n   * Full admin privileges, bypasses RLS\n   * ONLY use server-side for admin operations\n   * NEVER expose to clients or frontend\n   */\n  serviceRoleKey: () => supabaseServiceRoleKey(),\n\n  /**\n   * JWKS endpoint for JWT verification\n   * Used by auth handler to validate Supabase JWTs\n   * Format: https://xxxxx.supabase.co/auth/v1/.well-known/jwks.json\n   */\n  jwksEndpoint: () => `${supabaseURL()}/auth/v1/.well-known/jwks.json`,\n\n  /**\n   * Auth API endpoint\n   * Base URL for authentication operations\n   */\n  authEndpoint: () => `${supabaseURL()}/auth/v1`,\n} as const;\n", "import { api } from \"encore.dev/api\";\nimport log from \"encore.dev/log\";\nimport { db } from \"./db\";\nimport { BookmarkRepository } from \"./repositories/bookmark.repository\";\nimport { bookmarkCreatedTopic } from \"./events/bookmark-created.events\";\nimport { BookmarkSource } from \"./types\";\nimport { DailyDigestRepository } from \"./repositories/daily-digest.repository\";\nimport { DailyDigestService } from \"./services/daily-digest.service\";\n\n/**\n * TEST ENDPOINT - FOR DEVELOPMENT ONLY\n *\n * This endpoint bypasses authentication for local testing purposes.\n * DO NOT expose this in production!\n */\n\nconst bookmarkRepo = new BookmarkRepository(db);\nconst dailyDigestRepo = new DailyDigestRepository(db);\nconst dailyDigestService = new DailyDigestService(dailyDigestRepo);\n\nexport interface CreateBookmarkTestRequest {\n  url: string;\n  source?: BookmarkSource;\n  title?: string;\n  userId?: string; // Optional test user ID\n}\n\nexport interface CreateBookmarkTestResponse {\n  bookmarkId: number;\n  url: string;\n  source: string;\n  message: string;\n}\n\n/**\n * CREATE BOOKMARK (TEST) - No auth required\n * For local development and E2E testing only\n */\nexport const createTest = api(\n  { expose: true, method: \"POST\", path: \"/test/bookmarks\", auth: false },\n  async (req: CreateBookmarkTestRequest): Promise<CreateBookmarkTestResponse> => {\n    log.info(\"TEST: Creating bookmark without auth\", {\n      url: req.url,\n      source: req.source,\n    });\n\n    // Use test user ID or default\n    const userId = req.userId || \"550e8400-e29b-41d4-a716-446655440000\";\n    const source = req.source || BookmarkSource.WEB;\n\n    // Create the bookmark\n    const bookmark = await bookmarkRepo.create({\n      user_id: userId,\n      url: req.url,\n      title: req.title || null,\n      source,\n      client_time: new Date(),\n      metadata: null,\n    });\n\n    log.info(\"TEST: Bookmark created, publishing event\", {\n      bookmarkId: bookmark.id,\n      url: bookmark.url,\n      source: bookmark.source,\n    });\n\n    // Publish event to trigger processing pipeline\n    await bookmarkCreatedTopic.publish({\n      bookmarkId: bookmark.id,\n      url: bookmark.url,\n      source: bookmark.source,\n      title: bookmark.title || undefined,\n    });\n\n    return {\n      bookmarkId: bookmark.id,\n      url: bookmark.url,\n      source: bookmark.source,\n      message: \"Bookmark created and event published. Processing pipeline started.\",\n    };\n  }\n);\n\nexport interface GenerateDigestTestRequest {\n  date?: string; // YYYY-MM-DD format\n  userId?: string;\n}\n\nexport interface GenerateDigestTestResponse {\n  digest: any; // DailyDigest type\n  message: string;\n}\n\n/**\n * GENERATE DAILY DIGEST (TEST) - No auth required\n * For local development and E2E testing only\n */\nexport const generateDigestTest = api(\n  { expose: true, method: \"POST\", path: \"/test/digests/generate\", auth: false },\n  async (req: GenerateDigestTestRequest): Promise<GenerateDigestTestResponse> => {\n    log.info(\"TEST: Generating daily digest without auth\", {\n      date: req.date,\n      userId: req.userId,\n    });\n\n    // Use test user ID or default\n    const userId = req.userId || \"550e8400-e29b-41d4-a716-446655440000\";\n\n    // Parse date or default to today\n    const digestDate = req.date ? new Date(req.date) : new Date();\n\n    log.info(\"TEST: Calling daily digest service\", {\n      digestDate: digestDate.toISOString().split(\"T\")[0],\n      userId,\n    });\n\n    try {\n      const digest = await dailyDigestService.generateDailyDigest({\n        date: digestDate,\n        userId,\n        forceRegenerate: false,\n      });\n\n      log.info(\"TEST: Daily digest generated\", {\n        digestId: digest.id,\n        digestDate: digest.digest_date,\n        bookmarkCount: digest.bookmark_count,\n      });\n\n      return {\n        digest,\n        message: digest.digest_content\n          ? \"Daily digest generated successfully\"\n          : \"Daily digest scaffolding completed (summarization pending)\",\n      };\n    } catch (error) {\n      log.error(error, \"TEST: Failed to generate daily digest\", {\n        digestDate: digestDate.toISOString().split(\"T\")[0],\n        userId,\n      });\n\n      throw error;\n    }\n  }\n);\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\n\n// Initialize the bookmarks database\nexport const db = new SQLDatabase(\"bookmarks\", {\n  migrations: \"./migrations\",\n});\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\nimport { Bookmark, BookmarkSource } from \"../types\";\n\n/**\n * Repository for bookmark database operations\n */\nexport class BookmarkRepository {\n  constructor(private readonly db: SQLDatabase) {}\n\n  /**\n   * Creates a new bookmark\n   */\n  async create(data: {\n    user_id: string; // UUID from Supabase\n    url: string;\n    title: string | null;\n    source: BookmarkSource;\n    client_time: Date;\n    metadata: Record<string, any> | null;\n  }): Promise<Bookmark> {\n    const row = await this.db.queryRow<Bookmark>`\n      INSERT INTO bookmarks (user_id, url, title, source, client_time, metadata)\n      VALUES (\n        ${data.user_id},\n        ${data.url},\n        ${data.title},\n        ${data.source},\n        ${data.client_time},\n        ${data.metadata}\n      )\n      RETURNING *\n    `;\n\n    if (!row) {\n      throw new Error(\"Failed to create bookmark\");\n    }\n\n    return row;\n  }\n\n  /**\n   * Finds a bookmark by ID (filtered by user_id for data isolation)\n   */\n  async findById(id: number, userId: string): Promise<Bookmark | null> {\n    const row = await this.db.queryRow<Bookmark>`\n      SELECT * FROM bookmarks WHERE id = ${id} AND user_id = ${userId}\n    `;\n    return row || null;\n  }\n\n  /**\n   * Lists bookmarks with pagination and optional filtering (filtered by user_id)\n   */\n  async list(params: {\n    userId: string;\n    limit: number;\n    offset: number;\n    source?: BookmarkSource;\n  }): Promise<{ bookmarks: Bookmark[]; total: number }> {\n    const { userId, limit, offset, source } = params;\n\n    let bookmarksQuery;\n    let countQuery;\n\n    if (source) {\n      bookmarksQuery = this.db.query<Bookmark>`\n        SELECT * FROM bookmarks\n        WHERE user_id = ${userId} AND source = ${source}\n        ORDER BY created_at DESC\n        LIMIT ${limit} OFFSET ${offset}\n      `;\n\n      countQuery = this.db.queryRow<{ count: number }>`\n        SELECT COUNT(*)::int as count FROM bookmarks\n        WHERE user_id = ${userId} AND source = ${source}\n      `;\n    } else {\n      bookmarksQuery = this.db.query<Bookmark>`\n        SELECT * FROM bookmarks\n        WHERE user_id = ${userId}\n        ORDER BY created_at DESC\n        LIMIT ${limit} OFFSET ${offset}\n      `;\n\n      countQuery = this.db.queryRow<{ count: number }>`\n        SELECT COUNT(*)::int as count FROM bookmarks\n        WHERE user_id = ${userId}\n      `;\n    }\n\n    // Fetch bookmarks\n    const bookmarks: Bookmark[] = [];\n    for await (const bookmark of bookmarksQuery) {\n      bookmarks.push(bookmark);\n    }\n\n    // Get total count\n    const countResult = await countQuery;\n    const total = countResult?.count || 0;\n\n    return { bookmarks, total };\n  }\n\n  /**\n   * Updates a bookmark (filtered by user_id)\n   */\n  async update(\n    id: number,\n    userId: string,\n    data: {\n      url?: string;\n      title?: string;\n      source?: BookmarkSource;\n      metadata?: Record<string, any>;\n    }\n  ): Promise<Bookmark> {\n    // First get the existing bookmark\n    const existing = await this.findById(id, userId);\n    if (!existing) {\n      throw new Error(`Bookmark with id ${id} not found for user ${userId}`);\n    }\n\n    // Determine which fields to update (use existing if not provided)\n    const urlToUse = data.url !== undefined ? data.url : existing.url;\n    const titleToUse = data.title !== undefined ? data.title : existing.title;\n    const sourceToUse = data.source !== undefined ? data.source : existing.source;\n    const metadataToUse = data.metadata !== undefined ? data.metadata : existing.metadata;\n\n    // Update with new values or keep existing\n    const row = await this.db.queryRow<Bookmark>`\n      UPDATE bookmarks\n      SET\n        url = ${urlToUse},\n        title = ${titleToUse},\n        source = ${sourceToUse},\n        metadata = ${metadataToUse}\n      WHERE id = ${id} AND user_id = ${userId}\n      RETURNING *\n    `;\n\n    if (!row) {\n      throw new Error(\"Failed to update bookmark\");\n    }\n\n    return row;\n  }\n\n  /**\n   * Updates only the source field of a bookmark\n   * Used by classification processor to update detected source (internal operation)\n   * NOTE: This is an internal method called by processors, so it doesn't require userId filtering\n   */\n  async updateSource(id: number, source: BookmarkSource): Promise<void> {\n    // Direct update without user check (internal operation)\n    const result = await this.db.queryRow<{ id: number }>`\n      UPDATE bookmarks\n      SET source = ${source}\n      WHERE id = ${id}\n      RETURNING id\n    `;\n\n    if (!result) {\n      throw new Error(`Bookmark with id ${id} not found`);\n    }\n  }\n\n  /**\n   * Deletes a bookmark (filtered by user_id)\n   */\n  async delete(id: number, userId: string): Promise<void> {\n    // Check if bookmark exists for this user\n    const existing = await this.findById(id, userId);\n    if (!existing) {\n      throw new Error(`Bookmark with id ${id} not found for user ${userId}`);\n    }\n\n    await this.db.exec`\n      DELETE FROM bookmarks WHERE id = ${id} AND user_id = ${userId}\n    `;\n  }\n}\n", "import { Topic } from \"encore.dev/pubsub\";\nimport { BookmarkCreatedEvent } from \"../types\";\n\n/**\n * Bookmark Created Topic\n * Published when any bookmark is created (regardless of source)\n * Triggers classification and appropriate downstream processing\n */\nexport const bookmarkCreatedTopic = new Topic<BookmarkCreatedEvent>(\n  \"bookmark-created\",\n  {\n    deliveryGuarantee: \"at-least-once\",\n  }\n);\n", "// Re-export all types from organized modules\n\n// Domain types\nexport * from \"./domain.types\";\n\n// API types\nexport * from \"./api.types\";\n\n// Event types\nexport * from \"./event.types\";\n\n// Deepgram types\nexport * from \"./deepgram.types\";\n\n// Gemini types\nexport * from \"./gemini.types\";\n\n// Daily digest types\nexport * from \"./daily-digest.types\";\n\n// Web content types\nexport * from \"./web-content.types\";\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\nimport {\n  DailyDigest,\n  DigestStatus,\n  SourcesBreakdown,\n  ProcessingMetadata,\n  TranscriptionSummary,\n  BookmarkSource,\n  DigestContentItem,\n} from \"../types\";\n\n/**\n * Repository for daily digest database operations\n */\nexport class DailyDigestRepository {\n  constructor(private readonly db: SQLDatabase) {}\n\n  /**\n   * Creates a new daily digest record with pending status\n   * NOTE: Excludes JSONB fields from RETURNING due to Encore deserialization limitation\n   */\n  async create(data: {\n    digestDate: Date;\n    userId: string | null;\n    bookmarkCount: number;\n    sourcesBreakdown: SourcesBreakdown | null;\n    dateRangeStart: Date;\n    dateRangeEnd: Date;\n  }): Promise<DailyDigest> {\n    const row = await this.db.queryRow<Omit<DailyDigest, 'sources_breakdown' | 'processing_metadata'>>`\n      INSERT INTO daily_digests (\n        digest_date,\n        user_id,\n        status,\n        bookmark_count,\n        sources_breakdown,\n        date_range_start,\n        date_range_end\n      )\n      VALUES (\n        ${data.digestDate},\n        ${data.userId},\n        'pending',\n        ${data.bookmarkCount},\n        ${data.sourcesBreakdown},\n        ${data.dateRangeStart},\n        ${data.dateRangeEnd}\n      )\n      RETURNING\n        id, digest_date, user_id, status, bookmark_count,\n        date_range_start, date_range_end, digest_content, total_duration,\n        error_message, processing_started_at, processing_completed_at, created_at, updated_at\n    `;\n\n    if (!row) {\n      throw new Error(\"Failed to create daily digest\");\n    }\n\n    return {\n      ...row,\n      sources_breakdown: data.sourcesBreakdown,\n      processing_metadata: null\n    } as DailyDigest;\n  }\n\n  /**\n   * Finds a digest by date and optional user ID\n   * NOTE: Excludes JSONB fields due to Encore deserialization limitation\n   */\n  async findByDate(digestDate: Date, userId?: string): Promise<DailyDigest | null> {\n    const userIdValue = userId !== undefined ? userId : null;\n    const row = await this.db.queryRow<Omit<DailyDigest, 'sources_breakdown' | 'processing_metadata'>>`\n      SELECT\n        id, digest_date, user_id, status, bookmark_count,\n        date_range_start, date_range_end, digest_content, total_duration,\n        error_message, processing_started_at, processing_completed_at, created_at, updated_at\n      FROM daily_digests\n      WHERE digest_date = ${digestDate}\n        AND user_id IS NOT DISTINCT FROM ${userIdValue}\n    `;\n    return row ? {\n      ...row,\n      sources_breakdown: null,\n      processing_metadata: null\n    } as DailyDigest : null;\n  }\n\n  /**\n   * Finds digests within a date range\n   * NOTE: Excludes JSONB fields due to Encore deserialization limitation\n   */\n  async findByDateRange(\n    startDate: Date,\n    endDate: Date,\n    userId?: string\n  ): Promise<DailyDigest[]> {\n    const userIdValue = userId !== undefined ? userId : null;\n    const query = this.db.query<Omit<DailyDigest, 'sources_breakdown' | 'processing_metadata'>>`\n      SELECT\n        id, digest_date, user_id, status, bookmark_count,\n        date_range_start, date_range_end, digest_content, total_duration,\n        error_message, processing_started_at, processing_completed_at, created_at, updated_at\n      FROM daily_digests\n      WHERE digest_date >= ${startDate}\n        AND digest_date <= ${endDate}\n        AND user_id IS NOT DISTINCT FROM ${userIdValue}\n      ORDER BY digest_date DESC\n    `;\n\n    const digests: DailyDigest[] = [];\n    for await (const digest of query) {\n      digests.push({\n        ...digest,\n        sources_breakdown: null,\n        processing_metadata: null\n      } as DailyDigest);\n    }\n\n    return digests;\n  }\n\n  /**\n   * Lists digests with pagination\n   * NOTE: Excludes JSONB fields due to Encore deserialization limitation\n   */\n  async list(params: {\n    limit: number;\n    offset: number;\n    userId?: string;\n  }): Promise<{ digests: DailyDigest[]; total: number }> {\n    const { limit, offset, userId } = params;\n    const userIdValue = userId !== undefined ? userId : null;\n\n    const digestsQuery = this.db.query<Omit<DailyDigest, 'sources_breakdown' | 'processing_metadata'>>`\n      SELECT\n        id, digest_date, user_id, status, bookmark_count,\n        date_range_start, date_range_end, digest_content, total_duration,\n        error_message, processing_started_at, processing_completed_at, created_at, updated_at\n      FROM daily_digests\n      WHERE user_id IS NOT DISTINCT FROM ${userIdValue}\n      ORDER BY digest_date DESC\n      LIMIT ${limit} OFFSET ${offset}\n    `;\n\n    const countQuery = this.db.queryRow<{ count: number }>`\n      SELECT COUNT(*)::int as count FROM daily_digests\n      WHERE user_id IS NOT DISTINCT FROM ${userIdValue}\n    `;\n\n    // Fetch digests\n    const digests: DailyDigest[] = [];\n    for await (const digest of digestsQuery) {\n      digests.push({\n        ...digest,\n        sources_breakdown: null,\n        processing_metadata: null\n      } as DailyDigest);\n    }\n\n    // Get total count\n    const countResult = await countQuery;\n    const total = countResult?.count || 0;\n\n    return { digests, total };\n  }\n\n  /**\n   * Checks if a digest exists for a given date\n   */\n  async existsForDate(digestDate: Date, userId?: string): Promise<boolean> {\n    const userIdValue = userId !== undefined ? userId : null;\n    const row = await this.db.queryRow<{ exists: boolean }>`\n      SELECT EXISTS(\n        SELECT 1 FROM daily_digests\n        WHERE digest_date = ${digestDate}\n          AND user_id IS NOT DISTINCT FROM ${userIdValue}\n      ) as exists\n    `;\n    return row?.exists || false;\n  }\n\n  /**\n   * Updates digest status\n   */\n  async updateStatus(\n    id: number,\n    status: DigestStatus,\n    errorMessage?: string\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE daily_digests\n      SET\n        status = ${status},\n        error_message = ${errorMessage || null},\n        processing_completed_at = ${status === DigestStatus.COMPLETED || status === DigestStatus.FAILED ? new Date() : null}\n      WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * Marks digest as processing\n   */\n  async markAsProcessing(id: number): Promise<void> {\n    await this.db.exec`\n      UPDATE daily_digests\n      SET\n        status = 'processing',\n        processing_started_at = NOW()\n      WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * Marks digest as completed with content\n   */\n  async markAsCompleted(\n    id: number,\n    content: string | null,\n    totalDuration: number | null,\n    metadata: ProcessingMetadata\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE daily_digests\n      SET\n        status = 'completed',\n        digest_content = ${content},\n        total_duration = ${totalDuration},\n        processing_metadata = ${metadata},\n        processing_completed_at = NOW()\n      WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * Marks digest as failed with error message\n   */\n  async markAsFailed(id: number, errorMessage: string): Promise<void> {\n    await this.db.exec`\n      UPDATE daily_digests\n      SET\n        status = 'failed',\n        error_message = ${errorMessage},\n        processing_completed_at = NOW()\n      WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * Updates digest content (for Phase 2 summarization)\n   */\n  async updateContent(\n    id: number,\n    content: string,\n    metadata: ProcessingMetadata\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE daily_digests\n      SET\n        digest_content = ${content},\n        processing_metadata = ${metadata}\n      WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * Deletes a digest by ID\n   */\n  async delete(id: number): Promise<void> {\n    const existing = await this.db.queryRow<{ id: number }>`\n      SELECT id FROM daily_digests WHERE id = ${id}\n    `;\n\n    if (!existing) {\n      throw new Error(`Daily digest with id ${id} not found`);\n    }\n\n    await this.db.exec`\n      DELETE FROM daily_digests WHERE id = ${id}\n    `;\n  }\n\n  // ============================================\n  // Query Helpers for Transcriptions\n  // ============================================\n\n  /**\n   * Gets completed transcriptions within a date range\n   * Joins with bookmarks to get source information\n   * CRITICAL: Filters by userId to ensure users only see their own data\n   */\n  async getCompletedTranscriptionsInRange(\n    startDate: Date,\n    endDate: Date,\n    userId?: string\n  ): Promise<TranscriptionSummary[]> {\n    const userIdValue = userId !== undefined ? userId : null;\n\n    const query = this.db.query<TranscriptionSummary>`\n      SELECT\n        t.bookmark_id,\n        t.transcript,\n        t.summary,\n        t.deepgram_summary,\n        b.title AS bookmark_title,\n        t.duration::numeric::float AS duration,\n        t.sentiment,\n        t.created_at,\n        b.source\n      FROM transcriptions t\n      INNER JOIN bookmarks b ON t.bookmark_id = b.id\n      WHERE t.status = 'completed'\n        AND t.created_at >= ${startDate}\n        AND t.created_at <= ${endDate}\n        AND b.user_id IS NOT DISTINCT FROM ${userIdValue}\n      ORDER BY t.created_at DESC\n    `;\n\n    const transcriptions: TranscriptionSummary[] = [];\n    for await (const transcription of query) {\n      transcriptions.push(transcription);\n    }\n\n    return transcriptions;\n  }\n\n  /**\n   * Gets completed web content within a date range\n   * Returns DigestContentItem format for unified processing\n   * CRITICAL: Filters by userId to ensure users only see their own data\n   */\n  async getCompletedWebContentInRange(\n    startDate: Date,\n    endDate: Date,\n    userId?: string\n  ): Promise<DigestContentItem[]> {\n    const userIdValue = userId !== undefined ? userId : null;\n\n    const query = this.db.query<DigestContentItem>`\n      SELECT\n        wc.bookmark_id,\n        'article' as content_type,\n        wc.summary,\n        b.source,\n        wc.page_title as title,\n        wc.word_count,\n        wc.estimated_reading_minutes as reading_minutes,\n        wc.created_at\n      FROM web_contents wc\n      INNER JOIN bookmarks b ON wc.bookmark_id = b.id\n      WHERE wc.status = 'completed'\n        AND wc.summary IS NOT NULL\n        AND wc.created_at >= ${startDate}\n        AND wc.created_at <= ${endDate}\n        AND b.user_id IS NOT DISTINCT FROM ${userIdValue}\n      ORDER BY wc.created_at DESC\n    `;\n\n    const items: DigestContentItem[] = [];\n    for await (const item of query) {\n      items.push(item);\n    }\n\n    return items;\n  }\n}\n", "import log from \"encore.dev/log\";\nimport {\n  DailyDigest,\n  DigestStatus,\n  SourcesBreakdown,\n  ProcessingMetadata,\n  TranscriptionSummary,\n  BookmarkSource,\n  DigestGenerationOptions,\n  DigestContentItem,\n} from \"../types\";\nimport { DailyDigestRepository } from \"../repositories/daily-digest.repository\";\nimport {\n  DAILY_DIGEST_CONFIG,\n  getDigestDateRange,\n  formatDigestDate,\n} from \"../config/daily-digest.config\";\n\n/**\n * Service for daily digest generation and management\n * Orchestrates adaptive 3-tier summarization strategy based on volume\n */\nexport class DailyDigestService {\n  constructor(private readonly digestRepo: DailyDigestRepository) {}\n\n  /**\n   * Main orchestration method for generating a daily digest\n   * @param options - Configuration for digest generation\n   * @returns Generated daily digest\n   */\n  async generateDailyDigest(options: DigestGenerationOptions): Promise<DailyDigest> {\n    const { date, userId, forceRegenerate } = options;\n    const digestDateStr = formatDigestDate(date);\n\n    log.info(\"Starting daily digest generation\", {\n      digestDate: digestDateStr,\n      userId: userId || \"global\",\n      forceRegenerate,\n    });\n\n    try {\n      // Step 1: Check if digest already exists\n      const existingDigest = await this.checkIfDigestExists(date, userId);\n\n      if (existingDigest && !forceRegenerate) {\n        if (existingDigest.status === DigestStatus.COMPLETED) {\n          log.info(\"Digest already exists and is completed, returning existing\", {\n            digestId: existingDigest.id,\n            digestDate: digestDateStr,\n          });\n          return existingDigest;\n        }\n\n        if (existingDigest.status === DigestStatus.PROCESSING) {\n          log.warn(\"Digest is currently being processed, returning existing\", {\n            digestId: existingDigest.id,\n            digestDate: digestDateStr,\n          });\n          return existingDigest;\n        }\n\n        // If failed or pending, we'll regenerate\n        log.info(\"Existing digest found but not completed, regenerating\", {\n          digestId: existingDigest.id,\n          status: existingDigest.status,\n        });\n      }\n\n      // Step 2: Calculate date range for transcriptions\n      const { startDate, endDate } = this.calculateDateRange(date);\n\n      log.info(\"Calculated date range for digest\", {\n        digestDate: digestDateStr,\n        startDate: startDate.toISOString(),\n        endDate: endDate.toISOString(),\n      });\n\n      // Step 3: Fetch all content (audio + web)\n      const contentItems = await this.fetchContentForDate(\n        startDate,\n        endDate,\n        userId\n      );\n\n      const audioItemCount = contentItems.filter(c => c.content_type === 'audio').length;\n      const articleItemCount = contentItems.filter(c => c.content_type === 'article').length;\n\n      log.info(\"Fetched content for digest\", {\n        digestDate: digestDateStr,\n        totalItems: contentItems.length,\n        audioCount: audioItemCount,\n        articleCount: articleItemCount,\n      });\n\n      // Step 4: Calculate metadata\n      const bookmarkCount = contentItems.length;\n      const sourcesBreakdown = this.calculateSourcesBreakdown(contentItems);\n      const totalDuration = this.calculateTotalDuration(contentItems);\n\n      log.info(\"Calculated digest metadata\", {\n        digestDate: digestDateStr,\n        bookmarkCount,\n        sourcesBreakdown,\n        totalDuration,\n      });\n\n      // Step 5: Create or update digest record\n      let digest: DailyDigest;\n\n      if (existingDigest) {\n        // Update existing digest\n        await this.digestRepo.markAsProcessing(existingDigest.id);\n        digest = existingDigest;\n      } else {\n        // Create new digest\n        digest = await this.digestRepo.create({\n          digestDate: date,\n          userId: userId || null,\n          bookmarkCount: bookmarkCount,\n          sourcesBreakdown: sourcesBreakdown,\n          dateRangeStart: startDate,\n          dateRangeEnd: endDate,\n        });\n      }\n\n      log.info(\"Created/updated digest record\", {\n        digestId: digest.id,\n        digestDate: digestDateStr,\n        status: \"processing\",\n      });\n\n      // Step 6: Generate unified summary using map-reduce (audio + web content)\n      const startTime = Date.now();\n      const digestContent = await this.generateUnifiedSummary(contentItems, {\n        digestDate: digestDateStr,\n        totalItems: bookmarkCount,\n        audioCount: audioItemCount,\n        articleCount: articleItemCount,\n      });\n      const processingDurationMs = Date.now() - startTime;\n\n      // Step 7: Prepare processing metadata\n      const processingMetadata: ProcessingMetadata = {\n        modelUsed: DAILY_DIGEST_CONFIG.openaiModel,\n        summarizationStrategy: \"map-reduce\",\n        processingDurationMs,\n      };\n\n      // Step 8: Mark digest as completed\n      await this.digestRepo.markAsCompleted(\n        digest.id,\n        digestContent,\n        totalDuration,\n        processingMetadata\n      );\n\n      log.info(\"Daily digest generation completed\", {\n        digestId: digest.id,\n        digestDate: digestDateStr,\n        bookmarkCount,\n        totalDuration,\n      });\n\n      // Fetch and return the updated digest\n      const completedDigest = await this.digestRepo.findByDate(date, userId);\n      if (!completedDigest) {\n        throw new Error(\"Failed to retrieve completed digest\");\n      }\n\n      return completedDigest;\n    } catch (error) {\n      const errorMessage =\n        error instanceof Error ? error.message : String(error);\n\n      log.error(error, \"Daily digest generation failed\", {\n        digestDate: digestDateStr,\n        userId: userId || \"global\",\n        errorMessage,\n      });\n\n      throw new Error(`Failed to generate daily digest: ${errorMessage}`);\n    }\n  }\n\n  // ============================================\n  // Private Helper Methods\n  // ============================================\n\n  /**\n   * Checks if a digest already exists for the given date\n   */\n  private async checkIfDigestExists(\n    date: Date,\n    userId?: string\n  ): Promise<DailyDigest | null> {\n    return await this.digestRepo.findByDate(date, userId);\n  }\n\n  /**\n   * Calculates the date range for fetching transcriptions\n   */\n  private calculateDateRange(date: Date): { startDate: Date; endDate: Date } {\n    // Use the provided date as the digest date\n    const digestDate = new Date(date);\n    digestDate.setHours(0, 0, 0, 0);\n\n    // Start of day (00:00:00)\n    const startDate = new Date(digestDate);\n\n    // End of day (23:59:59.999)\n    const endDate = new Date(digestDate);\n    endDate.setHours(23, 59, 59, 999);\n\n    return { startDate, endDate };\n  }\n\n  /**\n   * Fetches all content (audio transcriptions + web content) for the given date range\n   * Returns unified DigestContentItem format\n   */\n  private async fetchContentForDate(\n    startDate: Date,\n    endDate: Date,\n    userId?: string\n  ): Promise<DigestContentItem[]> {\n    // Fetch audio transcriptions\n    const transcriptions = await this.digestRepo.getCompletedTranscriptionsInRange(\n      startDate,\n      endDate,\n      userId\n    );\n\n    // Fetch web content\n    const webContent = await this.digestRepo.getCompletedWebContentInRange(\n      startDate,\n      endDate,\n      userId\n    );\n\n    // Convert transcriptions to unified format\n    const transcriptionItems: DigestContentItem[] = transcriptions.map(t => ({\n      bookmark_id: t.bookmark_id,\n      content_type: 'audio' as const,\n      summary: t.summary || t.deepgram_summary || \"\",\n      source: t.source,\n      title: t.bookmark_title ?? null,\n      duration: t.duration || undefined,\n      sentiment: t.sentiment || undefined,\n      created_at: t.created_at,\n    }));\n\n    // Combine both content types\n    const allItems = [...transcriptionItems, ...webContent];\n\n    log.info(\"Fetched content for digest\", {\n      audioCount: transcriptionItems.length,\n      articleCount: webContent.length,\n      totalCount: allItems.length,\n      startDate: startDate.toISOString(),\n      endDate: endDate.toISOString(),\n    });\n\n    return allItems;\n  }\n\n  /**\n   * Calculates breakdown of bookmarks by source\n   * Works with unified DigestContentItem format (audio + web)\n   */\n  private calculateSourcesBreakdown(\n    items: DigestContentItem[]\n  ): SourcesBreakdown {\n    const breakdown: SourcesBreakdown = {};\n\n    for (const item of items) {\n      const source = item.source;\n      breakdown[source] = (breakdown[source] || 0) + 1;\n    }\n\n    return breakdown;\n  }\n\n  /**\n   * Calculates total duration of all audio content in seconds\n   * Web content (articles) do not have duration\n   */\n  private calculateTotalDuration(items: DigestContentItem[]): number {\n    return items.reduce((total, item) => {\n      // Only audio items have duration\n      if (item.content_type === 'audio' && item.duration) {\n        return total + item.duration;\n      }\n      return total;\n    }, 0);\n  }\n\n  // ============================================\n  // Summarization Methods\n  // ============================================\n\n  /**\n   * Generates a unified summary from all content (audio + web)\n   * Uses map-reduce for intelligent synthesis across any volume\n   *\n   * @param contentItems - All completed content items (audio + web) for the date\n   * @returns Unified digest summary text\n   */\n  private async generateUnifiedSummary(\n    contentItems: DigestContentItem[],\n    context: {\n      digestDate: string;\n      totalItems: number;\n      audioCount: number;\n      articleCount: number;\n    }\n  ): Promise<string | null> {\n    // If no content, return null\n    if (contentItems.length === 0) {\n      log.info(\"No content items to summarize\");\n      return null;\n    }\n\n    log.info(\"Generating unified summary with map-reduce\", {\n      digestDate: context.digestDate,\n      contentItemCount: context.totalItems,\n      audioCount: context.audioCount,\n      articleCount: context.articleCount,\n    });\n\n    try {\n      // Import OpenAI API key from Encore secrets\n      const { secret } = await import(\"encore.dev/config\");\n      const openaiApiKey = secret(\"OpenAIAPIKey\");\n\n      // Use MapReduceDigestService for all digests (now handles unified content)\n      const { MapReduceDigestService } = await import(\"./map-reduce-digest.service\");\n      const mapReduceService = new MapReduceDigestService(openaiApiKey());\n\n      const digest = await mapReduceService.generateDigest(contentItems, context);\n\n      log.info(\"Unified summary generated successfully\", {\n        digestDate: context.digestDate,\n        contentItemCount: context.totalItems,\n        audioCount: context.audioCount,\n        articleCount: context.articleCount,\n        digestLength: digest.length,\n      });\n\n      return digest;\n    } catch (error) {\n      log.error(error, \"Failed to generate unified summary\", {\n        digestDate: context.digestDate,\n        contentItemCount: context.totalItems,\n        errorMessage: error instanceof Error ? error.message : String(error),\n      });\n\n      // Re-throw to be handled by caller\n      throw error;\n    }\n  }\n\n  // ============================================\n  // Helper Methods\n  // ============================================\n\n  /**\n   * Gets a digest by date\n   */\n  async getDigestByDate(date: Date, userId?: string): Promise<DailyDigest | null> {\n    return await this.digestRepo.findByDate(date, userId);\n  }\n\n  /**\n   * Lists digests with pagination\n   */\n  async listDigests(params: {\n    limit: number;\n    offset: number;\n    userId?: string;\n  }): Promise<{ digests: DailyDigest[]; total: number }> {\n    return await this.digestRepo.list(params);\n  }\n}\n", "import { api, APIError } from \"encore.dev/api\";\nimport { getAuthData } from \"~encore/auth\";\nimport log from \"encore.dev/log\";\nimport { db } from \"./db\";\nimport {\n  CreateBookmarkRequest,\n  BookmarkResponse,\n  GetBookmarkRequest,\n  UpdateBookmarkRequest,\n  ListBookmarksRequest,\n  ListBookmarksResponse,\n  DeleteBookmarkRequest,\n  DeleteBookmarkResponse,\n  GetBookmarkDetailsRequest,\n  BookmarkDetailsResponse,\n  TranscriptionDetails,\n  BookmarkSource,\n  GenerateDailyDigestRequest,\n  GenerateDailyDigestResponse,\n  GetDailyDigestRequest,\n  GetDailyDigestResponse,\n  ListDailyDigestsRequest,\n  ListDailyDigestsResponse,\n} from \"./types\";\nimport { bookmarkCreatedTopic } from \"./events/bookmark-created.events\";\nimport { BookmarkRepository } from \"./repositories/bookmark.repository\";\nimport { TranscriptionRepository } from \"./repositories/transcription.repository\";\nimport { DailyDigestRepository } from \"./repositories/daily-digest.repository\";\nimport { Transcription } from \"./types/domain.types\";\nimport { DailyDigestService } from \"./services/daily-digest.service\";\nimport { getDigestDateRange, parseDigestDate } from \"./config/daily-digest.config\";\n\n// Initialize repositories\nconst bookmarkRepo = new BookmarkRepository(db);\nconst transcriptionRepo = new TranscriptionRepository(db);\nconst dailyDigestRepo = new DailyDigestRepository(db);\n\n// Initialize services\nconst dailyDigestService = new DailyDigestService(dailyDigestRepo);\n\n/**\n * Maps a full Transcription domain object to a client-facing TranscriptionDetails view\n * Excludes internal implementation details like deepgram_response and processing timestamps\n */\nfunction toTranscriptionDetails(t: Transcription): TranscriptionDetails {\n  return {\n    transcript: t.transcript,\n    deepgram_summary: t.deepgram_summary,\n    summary: t.summary,\n    sentiment: t.sentiment,\n    sentiment_score: t.sentiment_score,\n    duration: t.duration,\n    confidence: t.confidence,\n    status: t.status,\n    error_message: t.error_message,\n    created_at: t.created_at,\n    updated_at: t.updated_at,\n  };\n}\n\n// CREATE - Add a new bookmark\nexport const create = api(\n  { expose: true, method: \"POST\", path: \"/bookmarks\", auth: true },\n  async (req: CreateBookmarkRequest): Promise<BookmarkResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    // Validate required fields\n    if (!req.url || !req.client_time) {\n      throw APIError.invalidArgument(\"url and client_time are required\");\n    }\n\n    // Default source to 'web' if not provided (triggers auto-classification)\n    const source = req.source || BookmarkSource.WEB;\n\n    // Create the bookmark\n    const bookmark = await bookmarkRepo.create({\n      user_id: userId,\n      url: req.url,\n      title: req.title || null,\n      source,\n      client_time: req.client_time,\n      metadata: req.metadata || null,\n    });\n\n    log.info(\"Bookmark created, publishing event for classification\", {\n      bookmarkId: bookmark.id,\n      url: bookmark.url,\n      source: bookmark.source,\n    });\n\n    // Publish bookmark-created event for all bookmarks\n    // Classification processor will handle source detection and downstream pipelines\n    try {\n      const messageId = await bookmarkCreatedTopic.publish({\n        bookmarkId: bookmark.id,\n        url: bookmark.url,\n        source: bookmark.source,\n        title: bookmark.title || undefined,\n      });\n\n      log.info(\"Successfully published bookmark-created event\", {\n        bookmarkId: bookmark.id,\n        messageId,\n      });\n    } catch (error) {\n      log.error(error, \"Failed to publish bookmark-created event\", {\n        bookmarkId: bookmark.id,\n      });\n      // Don't fail the bookmark creation if event publishing fails\n    }\n\n    return { bookmark };\n  }\n);\n\n// READ - Get a bookmark by ID\nexport const get = api(\n  { expose: true, method: \"GET\", path: \"/bookmarks/:id\", auth: true },\n  async (req: GetBookmarkRequest): Promise<BookmarkResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    const bookmark = await bookmarkRepo.findById(req.id, userId);\n\n    if (!bookmark) {\n      throw APIError.notFound(`Bookmark with id ${req.id} not found`);\n    }\n\n    return { bookmark };\n  }\n);\n\n// LIST - Get all bookmarks with pagination and filtering\nexport const list = api(\n  { expose: true, method: \"GET\", path: \"/bookmarks\", auth: true },\n  async (req: ListBookmarksRequest): Promise<ListBookmarksResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    const limit = req.limit || 50;\n    const offset = req.offset || 0;\n\n    const { bookmarks, total } = await bookmarkRepo.list({\n      userId,\n      limit,\n      offset,\n      source: req.source as BookmarkSource | undefined,\n    });\n\n    return { bookmarks, total };\n  }\n);\n\n// UPDATE - Update a bookmark\nexport const update = api(\n  { expose: true, method: \"PUT\", path: \"/bookmarks/:id\", auth: true },\n  async (req: UpdateBookmarkRequest): Promise<BookmarkResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    // Validate that at least one field is provided\n    if (\n      req.url === undefined &&\n      req.title === undefined &&\n      req.source === undefined &&\n      req.metadata === undefined\n    ) {\n      throw APIError.invalidArgument(\"No fields to update\");\n    }\n\n    const bookmark = await bookmarkRepo.update(req.id, userId, {\n      url: req.url,\n      title: req.title,\n      source: req.source,\n      metadata: req.metadata,\n    });\n\n    return { bookmark };\n  }\n);\n\n// DELETE - Delete a bookmark\nexport const remove = api(\n  { expose: true, method: \"DELETE\", path: \"/bookmarks/:id\", auth: true },\n  async (req: DeleteBookmarkRequest): Promise<DeleteBookmarkResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    await bookmarkRepo.delete(req.id, userId);\n    return { success: true };\n  }\n);\n\n// GET DETAILS - Get a bookmark with all enriched data (transcription, etc.)\nexport const getDetails = api(\n  { expose: true, method: \"GET\", path: \"/bookmarks/:id/details\", auth: true },\n  async (req: GetBookmarkDetailsRequest): Promise<BookmarkDetailsResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    // Fetch the bookmark\n    const bookmark = await bookmarkRepo.findById(req.id, userId);\n\n    if (!bookmark) {\n      throw APIError.notFound(`Bookmark with id ${req.id} not found`);\n    }\n\n    // Fetch transcription if it exists (only relevant for YouTube bookmarks)\n    const transcription = await transcriptionRepo.findByBookmarkId(req.id);\n\n    log.info(\"Fetched bookmark details\", {\n      bookmarkId: req.id,\n      source: bookmark.source,\n      hasTranscription: !!transcription,\n    });\n\n    return {\n      bookmark,\n      transcription: transcription ? toTranscriptionDetails(transcription) : null,\n    };\n  }\n);\n\n// ============================================\n// Daily Digest Endpoints\n// ============================================\n\n// GENERATE DAILY DIGEST - Generate or retrieve daily digest for a date\n// (Used for manual triggering with optional date parameter)\nexport const generateDailyDigest = api(\n  { expose: true, method: \"POST\", path: \"/digests/generate\", auth: true },\n  async (req?: GenerateDailyDigestRequest): Promise<GenerateDailyDigestResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    // Determine the date for digest generation\n    // If date is provided, use it; otherwise use yesterday\n    let digestDate: Date;\n\n    if (req?.date) {\n      try {\n        digestDate = parseDigestDate(req.date);\n      } catch (error) {\n        throw APIError.invalidArgument(\"Invalid date format. Use YYYY-MM-DD\");\n      }\n    } else {\n      // Default: generate digest for yesterday\n      const { digestDate: yesterday } = getDigestDateRange();\n      digestDate = yesterday;\n    }\n\n    log.info(\"Generating daily digest\", {\n      digestDate: digestDate.toISOString().split(\"T\")[0],\n      userId,\n    });\n\n    try {\n      const digest = await dailyDigestService.generateDailyDigest({\n        date: digestDate,\n        userId,\n        forceRegenerate: false,\n      });\n\n      return {\n        digest,\n        message: digest.digest_content\n          ? \"Daily digest generated successfully\"\n          : \"Daily digest scaffolding completed (summarization pending)\",\n      };\n    } catch (error) {\n      log.error(error, \"Failed to generate daily digest\", {\n        digestDate: digestDate.toISOString().split(\"T\")[0],\n      });\n\n      throw APIError.internal(\n        `Failed to generate daily digest: ${\n          error instanceof Error ? error.message : String(error)\n        }`\n      );\n    }\n  }\n);\n\n// GET DAILY DIGEST - Get digest for a specific date\nexport const getDailyDigest = api(\n  { expose: true, method: \"GET\", path: \"/digests/:date\", auth: true },\n  async (req: GetDailyDigestRequest): Promise<GetDailyDigestResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    let digestDate: Date;\n\n    try {\n      digestDate = parseDigestDate(req.date);\n    } catch (error) {\n      throw APIError.invalidArgument(\"Invalid date format. Use YYYY-MM-DD\");\n    }\n\n    log.info(\"Fetching daily digest\", {\n      digestDate: req.date,\n      userId,\n    });\n\n    const digest = await dailyDigestService.getDigestByDate(\n      digestDate,\n      userId\n    );\n\n    if (!digest) {\n      log.info(\"Daily digest not found\", {\n        digestDate: req.date,\n        userId,\n      });\n    }\n\n    return { digest };\n  }\n);\n\n// LIST DAILY DIGESTS - List all digests with pagination\nexport const listDailyDigests = api(\n  { expose: true, method: \"GET\", path: \"/digests\", auth: true },\n  async (req: ListDailyDigestsRequest): Promise<ListDailyDigestsResponse> => {\n    // Get authenticated user\n    const auth = getAuthData();\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n    const userId = auth.userID; // UUID from Supabase JWT\n\n    const limit = req.limit || 30;\n    const offset = req.offset || 0;\n\n    log.info(\"Listing daily digests\", {\n      limit,\n      offset,\n      userId,\n    });\n\n    const { digests, total } = await dailyDigestService.listDigests({\n      limit,\n      offset,\n      userId,\n    });\n\n    return { digests, total };\n  }\n);\n\n// CRON JOB ENDPOINT - Generate yesterday's digest for ALL users\n// This endpoint is called by the daily cron job at 9 PM GMT\nexport const generateYesterdaysDigest = api(\n  { expose: false, method: \"POST\", path: \"/digests/generate-yesterday\" },\n  async (): Promise<GenerateDailyDigestResponse> => {\n    // Generate digest for yesterday (default behavior)\n    const { digestDate } = getDigestDateRange();\n    const digestDateStr = digestDate.toISOString().split(\"T\")[0];\n\n    log.info(\"Cron job triggered: Generating yesterday's digest for all users\", {\n      digestDate: digestDateStr,\n    });\n\n    try {\n      // Import users service client for service-to-service call\n      const { users } = await import(\"~encore/clients\");\n\n      // Fetch all user IDs\n      const { userIds } = await users.getUserIds();\n\n      log.info(\"Fetched users for digest generation\", {\n        digestDate: digestDateStr,\n        userCount: userIds.length,\n      });\n\n      // Track results\n      const results = {\n        total: userIds.length,\n        successful: 0,\n        failed: 0,\n        errors: [] as Array<{ userId: string; error: string }>,\n      };\n\n      // Generate digest for each user\n      for (const userId of userIds) {\n        try {\n          await dailyDigestService.generateDailyDigest({\n            date: digestDate,\n            userId,\n            forceRegenerate: false,\n          });\n\n          results.successful++;\n\n          log.info(\"Generated digest for user\", {\n            userId,\n            digestDate: digestDateStr,\n          });\n        } catch (error) {\n          results.failed++;\n          const errorMessage =\n            error instanceof Error ? error.message : String(error);\n\n          results.errors.push({ userId, error: errorMessage });\n\n          log.error(error, \"Failed to generate digest for user\", {\n            userId,\n            digestDate: digestDateStr,\n            errorMessage,\n          });\n\n          // Continue processing other users even if one fails\n        }\n      }\n\n      // Log summary\n      log.info(\"Cron job completed: Daily digest generation summary\", {\n        digestDate: digestDateStr,\n        total: results.total,\n        successful: results.successful,\n        failed: results.failed,\n        successRate: `${((results.successful / results.total) * 100).toFixed(1)}%`,\n      });\n\n      // If all failed, throw error\n      if (results.failed === results.total && results.total > 0) {\n        throw new Error(\n          `All ${results.total} digest generation attempts failed`\n        );\n      }\n\n      // Return summary as digest (using the first user's digest if available, or a summary)\n      // Note: This is a cron job, so the return value is mainly for logging\n      const firstUserId = userIds[0];\n      const digest = firstUserId\n        ? await dailyDigestService.getDigestByDate(digestDate, firstUserId)\n        : null;\n\n      return {\n        digest: digest || ({} as any), // Placeholder for cron job response\n        message: `Cron job completed: Generated ${results.successful}/${results.total} digests successfully`,\n      };\n    } catch (error) {\n      log.error(error, \"Cron job failed to generate yesterday's digest\", {\n        digestDate: digestDateStr,\n      });\n\n      throw APIError.internal(\n        `Failed to generate yesterday's digest: ${\n          error instanceof Error ? error.message : String(error)\n        }`\n      );\n    }\n  }\n);\n", "import { getAuthData as _getAuthData } from \"encore.dev/internal/codegen/auth\";\nimport { auth as _auth_auth } from \"../../../users/auth.js\";\n\nexport type AuthData = Awaited<ReturnType<typeof _auth_auth>>;\n\nexport function getAuthData(): AuthData | null {\n    return _getAuthData()\n}\n\ndeclare module \"encore.dev/api\" {\n  interface CallOpts {\n    authData?: AuthData;\n  }\n}\n\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\nimport {\n  Transcription,\n  TranscriptionStatus,\n  TranscriptionMethod,\n  DeepgramResponse,\n} from \"../types\";\n\n/**\n * Repository for transcription database operations\n */\nexport class TranscriptionRepository {\n  constructor(private readonly db: SQLDatabase) {}\n\n  /**\n   * Creates a pending transcription record\n   */\n  async createPending(bookmarkId: number): Promise<void> {\n    await this.db.exec`\n      INSERT INTO transcriptions (bookmark_id, status)\n      VALUES (${bookmarkId}, 'pending')\n    `;\n  }\n\n  /**\n   * Updates transcription status to processing\n   */\n  async markAsProcessing(bookmarkId: number): Promise<void> {\n    await this.db.exec`\n      UPDATE transcriptions\n      SET status = 'processing', processing_started_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Updates transcription status to failed with error message\n   */\n  async markAsFailed(bookmarkId: number, errorMessage: string): Promise<void> {\n    await this.db.exec`\n      UPDATE transcriptions\n      SET\n        status = 'failed',\n        error_message = ${errorMessage},\n        processing_completed_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Finds a transcription by bookmark ID\n   * NOTE: Excludes deepgram_response JSONB field due to Encore deserialization limitation\n   */\n  async findByBookmarkId(bookmarkId: number): Promise<Transcription | null> {\n    const row = await this.db.queryRow<Omit<Transcription, 'deepgram_response'>>`\n      SELECT\n        id, bookmark_id, transcript, deepgram_summary, sentiment, sentiment_score,\n        duration, confidence, summary, status, error_message,\n        processing_started_at, processing_completed_at, created_at, updated_at\n      FROM transcriptions\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n    return row ? { ...row, deepgram_response: null } as Transcription : null;\n  }\n\n  // ============================================\n  // Stage-Specific Update Methods\n  // ============================================\n\n  /**\n   * Stage 2: Update transcription data after Deepgram processing\n   */\n  async updateTranscriptionData(\n    bookmarkId: number,\n    data: {\n      transcript: string;\n      deepgramSummary: string | null;\n      sentiment: \"positive\" | \"negative\" | \"neutral\" | null;\n      sentimentScore: number | null;\n      deepgramResponse: any;\n      duration: number;\n      confidence: number;\n    }\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE transcriptions\n      SET\n        transcript = ${data.transcript},\n        deepgram_summary = ${data.deepgramSummary},\n        sentiment = ${data.sentiment},\n        sentiment_score = ${data.sentimentScore},\n        deepgram_response = ${data.deepgramResponse},\n        duration = ${data.duration},\n        confidence = ${data.confidence},\n        transcription_method = 'deepgram',\n        status = 'processing'\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Stage 3: Update summary after OpenAI processing\n   */\n  async updateSummary(bookmarkId: number, summary: string): Promise<void> {\n    await this.db.exec`\n      UPDATE transcriptions\n      SET\n        summary = ${summary},\n        status = 'completed',\n        processing_completed_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  // ============================================\n  // Gemini-Specific Methods\n  // ============================================\n\n  /**\n   * Stage 2 (Gemini): Update transcription data after Gemini processing\n   * Simpler than Deepgram - Gemini provides just transcript, no audio intelligence\n   */\n  async updateGeminiTranscriptionData(\n    bookmarkId: number,\n    data: {\n      transcript: string;\n      confidence: number;\n    }\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE transcriptions\n      SET\n        transcript = ${data.transcript},\n        confidence = ${data.confidence},\n        transcription_method = 'gemini',\n        status = 'processing'\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n}\n", "import { api, APIError } from \"encore.dev/api\";\nimport { getAuthData } from \"~encore/auth\";\nimport log from \"encore.dev/log\";\nimport { db } from \"./db\";\nimport { UserRepository } from \"./repositories/user.repository\";\nimport { MeResponse, UpdateProfileRequest, UpdateProfileResponse, GetUserIdsResponse } from \"./types\";\n\n/**\n * API Endpoints for User Management\n *\n * NOTE: Signup and Login are now handled by Supabase Auth\n * - Signup: Use Supabase client library or POST to https://wykjjshvcwfiyvzmvocf.supabase.co/auth/v1/signup\n * - Login: Use Supabase client library or POST to https://wykjjshvcwfiyvzmvocf.supabase.co/auth/v1/token\n * - Password Reset: Handled by Supabase (auth.resetPasswordForEmail())\n * - Email Verification: Handled by Supabase automatically\n *\n * This API only provides:\n * - GET /users/me - Fetch user profile from local database\n */\n\n// Initialize repository\nconst userRepo = new UserRepository(db);\n\n/**\n * ARCHIVED ENDPOINTS:\n * The following endpoints have been replaced by Supabase Auth:\n * - POST /users/signup â†’ Use Supabase signUp()\n * - POST /users/login â†’ Use Supabase signInWithPassword()\n *\n * See users/_archived/ for old implementation\n *\n * Frontend Integration Example:\n *   import { createClient } from '@supabase/supabase-js'\n *   const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY)\n *\n *   // Signup\n *   const { data, error } = await supabase.auth.signUp({\n *     email: 'user@example.com',\n *     password: 'password123'\n *   })\n *\n *   // Login\n *   const { data, error } = await supabase.auth.signInWithPassword({\n *     email: 'user@example.com',\n *     password: 'password123'\n *   })\n *\n *   // Password Reset\n *   await supabase.auth.resetPasswordForEmail('user@example.com')\n */\n\n/**\n * Get Current User Endpoint\n * Returns the currently authenticated user's information from local database\n *\n * GET /users/me\n * Headers: Authorization: Bearer <supabase-token>\n * Returns: { user: SafeUser }\n *\n * NOTE: This endpoint fetches user data from YOUR database, not Supabase.\n * The JWT is validated by Supabase, then we look up additional user data locally.\n */\nexport const me = api(\n  { expose: true, method: \"GET\", path: \"/users/me\", auth: true },\n  async (): Promise<MeResponse> => {\n    // Get authenticated user data from auth handler\n    // auth.userID is now the Supabase user UUID (not the integer ID)\n    const auth = getAuthData();\n\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n\n    try {\n      // Look up user by Supabase UUID (which is now the primary key)\n      const user = await userRepo.findById(auth.userID);\n\n      if (!user) {\n        log.warn(\"User not found in local database\", {\n          userId: auth.userID,\n          email: auth.email,\n        });\n        throw new Error(\n          `User ${auth.userID} authenticated with Supabase but not found in local database. User may need to be created.`\n        );\n      }\n\n      log.info(\"User fetched current user info\", {\n        userId: user.id,\n      });\n\n      const safeUser = {\n        id: user.id, // UUID from Supabase\n        email: user.email,\n        name: user.name,\n        migrated_to_supabase: user.migrated_to_supabase,\n        created_at: user.created_at,\n        updated_at: user.updated_at,\n      };\n\n      return { user: safeUser };\n    } catch (error) {\n      log.error(error, \"Failed to fetch current user\", {\n        userId: auth.userID,\n      });\n\n      throw APIError.notFound(\"User not found in local database\");\n    }\n  }\n);\n\n/**\n * Update Current User Profile Endpoint\n * Updates the authenticated user's profile information\n *\n * PATCH /users/me\n * Headers: Authorization: Bearer <supabase-token>\n * Body: { name?: string }\n * Returns: { user: SafeUser }\n *\n * NOTE: This endpoint updates the user's profile in our local database.\n * Email changes should be handled directly via Supabase client SDK.\n */\nexport const updateProfile = api(\n  { expose: true, method: \"PATCH\", path: \"/users/me\", auth: true },\n  async (req: UpdateProfileRequest): Promise<UpdateProfileResponse> => {\n    // Get authenticated user data from auth handler\n    const auth = getAuthData();\n\n    if (!auth) {\n      throw APIError.unauthenticated(\"Authentication required\");\n    }\n\n    try {\n      // Validate that at least one field is provided\n      if (req.name === undefined) {\n        throw APIError.invalidArgument(\n          \"At least one field must be provided for update\"\n        );\n      }\n\n      // Update user profile\n      const updatedUser = await userRepo.update(auth.userID, {\n        name: req.name,\n      });\n\n      log.info(\"User updated profile\", {\n        userId: updatedUser.id,\n        updatedFields: Object.keys(req),\n      });\n\n      const safeUser = {\n        id: updatedUser.id,\n        email: updatedUser.email,\n        name: updatedUser.name,\n        migrated_to_supabase: updatedUser.migrated_to_supabase,\n        created_at: updatedUser.created_at,\n        updated_at: updatedUser.updated_at,\n      };\n\n      return { user: safeUser };\n    } catch (error) {\n      log.error(error, \"Failed to update user profile\", {\n        userId: auth.userID,\n      });\n\n      if (error instanceof APIError) {\n        throw error;\n      }\n\n      throw APIError.internal(\"Failed to update user profile\");\n    }\n  }\n);\n\n/**\n * Get All User IDs Endpoint\n * Returns all user IDs in the system\n *\n * GET /users/ids\n * Auth: Not required (service-to-service)\n * Returns: { userIds: string[] }\n *\n * NOTE: This endpoint is used by other services (e.g., cron jobs) to\n * perform batch operations for all users. It is intentionally not authenticated\n * to allow service-to-service calls. In production, consider using Encore's\n * service-to-service authentication or restricting this to internal traffic only.\n */\nexport const getUserIds = api(\n  { expose: false, method: \"GET\", path: \"/users/ids\", auth: false },\n  async (): Promise<GetUserIdsResponse> => {\n    try {\n      const userIds = await userRepo.listAllUserIds();\n\n      log.info(\"Fetched all user IDs for batch operation\", {\n        count: userIds.length,\n      });\n\n      return { userIds };\n    } catch (error) {\n      log.error(error, \"Failed to fetch user IDs\");\n      throw APIError.internal(\"Failed to fetch user IDs\");\n    }\n  }\n);\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\n\n/**\n * Users database instance\n * Manages user accounts and authentication data\n */\nexport const db = new SQLDatabase(\"users\", {\n  migrations: \"./migrations\",\n});\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\nimport { User } from \"../types\";\n\n/**\n * UserRepository handles all database operations for users\n * Uses UUID as primary key (Supabase user ID)\n */\nexport class UserRepository {\n  constructor(private readonly db: SQLDatabase) {}\n\n  /**\n   * Create a new user in the database\n   * Called after Supabase user creation to store additional user data\n   * @param data User data including Supabase UUID, email, and optional name\n   * @returns The created user\n   * @throws Error if user creation fails (e.g., duplicate id/email)\n   */\n  async create(data: {\n    id: string; // UUID from Supabase\n    email: string;\n    name?: string | null;\n  }): Promise<User> {\n    const row = await this.db.queryRow<User>`\n      INSERT INTO users (id, email, name, migrated_to_supabase)\n      VALUES (${data.id}, ${data.email}, ${data.name || null}, TRUE)\n      RETURNING *\n    `;\n\n    if (!row) {\n      throw new Error(\"Failed to create user\");\n    }\n\n    return row;\n  }\n\n  /**\n   * Find a user by their ID (UUID from Supabase)\n   * @param id User UUID\n   * @returns User if found, null otherwise\n   */\n  async findById(id: string): Promise<User | null> {\n    return (\n      (await this.db.queryRow<User>`\n      SELECT * FROM users WHERE id = ${id}\n    `) || null\n    );\n  }\n\n  /**\n   * Find a user by their email address\n   * @param email Email address\n   * @returns User if found, null otherwise\n   */\n  async findByEmail(email: string): Promise<User | null> {\n    return (\n      (await this.db.queryRow<User>`\n      SELECT * FROM users WHERE email = ${email}\n    `) || null\n    );\n  }\n\n  /**\n   * Alias for findById for backward compatibility\n   * Used during Supabase JWT authentication\n   * @param supabaseUserId Supabase user UUID\n   * @returns User if found, null otherwise\n   */\n  async findBySupabaseId(supabaseUserId: string): Promise<User | null> {\n    return this.findById(supabaseUserId);\n  }\n\n  /**\n   * Check if a user exists with the given email\n   * @param email Email address\n   * @returns true if user exists, false otherwise\n   */\n  async existsByEmail(email: string): Promise<boolean> {\n    const row = await this.db.queryRow<{ count: number }>`\n      SELECT COUNT(*) as count FROM users WHERE email = ${email}\n    `;\n    return row ? row.count > 0 : false;\n  }\n\n  /**\n   * Check if a user exists with the given ID\n   * @param id User UUID\n   * @returns true if user exists, false otherwise\n   */\n  async existsById(id: string): Promise<boolean> {\n    const row = await this.db.queryRow<{ count: number }>`\n      SELECT COUNT(*) as count FROM users WHERE id = ${id}\n    `;\n    return row ? row.count > 0 : false;\n  }\n\n  /**\n   * Update user information\n   * @param id User UUID\n   * @param data Partial user data to update\n   * @returns Updated user\n   * @throws Error if user not found\n   */\n  async update(\n    id: string,\n    data: {\n      email?: string;\n      name?: string | null;\n    }\n  ): Promise<User> {\n    // Build dynamic update query\n    const updates: string[] = [];\n\n    if (data.email !== undefined) {\n      updates.push('email');\n    }\n    if (data.name !== undefined) {\n      updates.push('name');\n    }\n\n    if (updates.length === 0) {\n      throw new Error(\"No fields to update\");\n    }\n\n    // Create update query based on provided fields\n    let query;\n    if (data.email !== undefined && data.name !== undefined) {\n      query = this.db.queryRow<User>`\n        UPDATE users\n        SET email = ${data.email}, name = ${data.name}\n        WHERE id = ${id}\n        RETURNING *\n      `;\n    } else if (data.email !== undefined) {\n      query = this.db.queryRow<User>`\n        UPDATE users\n        SET email = ${data.email}\n        WHERE id = ${id}\n        RETURNING *\n      `;\n    } else {\n      query = this.db.queryRow<User>`\n        UPDATE users\n        SET name = ${data.name}\n        WHERE id = ${id}\n        RETURNING *\n      `;\n    }\n\n    const row = await query;\n\n    if (!row) {\n      throw new Error(`User with id ${id} not found`);\n    }\n\n    return row;\n  }\n\n  /**\n   * Delete a user by ID\n   * @param id User UUID\n   * @throws Error if user not found\n   */\n  async delete(id: string): Promise<void> {\n    await this.db.exec`\n      DELETE FROM users WHERE id = ${id}\n    `;\n  }\n\n  /**\n   * List all user IDs\n   * Used by cron jobs and batch operations\n   * @returns Array of user UUIDs\n   */\n  async listAllUserIds(): Promise<string[]> {\n    const query = this.db.query<{ id: string }>`\n      SELECT id FROM users ORDER BY created_at ASC\n    `;\n\n    const userIds: string[] = [];\n    for await (const row of query) {\n      userIds.push(row.id);\n    }\n\n    return userIds;\n  }\n}\n", "import { api, APIError } from \"encore.dev/api\";\nimport log from \"encore.dev/log\";\nimport { db } from \"./db\";\nimport { UserRepository } from \"./repositories/user.repository\";\nimport {\n  CustomAccessTokenHookPayload,\n  CustomAccessTokenHookResponse,\n} from \"./types\";\n\n/**\n * Supabase Auth Webhooks\n *\n * These endpoints are called by Supabase Auth Hooks when authentication events occur.\n * They allow us to automatically sync user data to our local database.\n *\n * Configuration:\n * 1. Go to Supabase Dashboard â†’ Authentication â†’ Hooks\n * 2. Create a new Hook of type \"Custom Access Token\"\n * 3. Set the URL to: https://your-api.com/webhooks/auth/user-created\n * 4. The hook will be called after user signup/confirmation\n *\n * @see https://supabase.com/docs/guides/auth/auth-hooks\n */\n\n// Initialize repository\nconst userRepo = new UserRepository(db);\n\n/**\n * Custom Access Token Hook\n * Called by Supabase before issuing a JWT token\n *\n * Flow:\n * 1. User signs up/logs in via Supabase\n * 2. Supabase calls this hook BEFORE issuing the JWT\n * 3. We sync the user to our local database\n * 4. We return updated claims including our custom claim\n * 5. Supabase issues the JWT with the updated claims\n *\n * POST /webhooks/auth/user-created\n * No authentication required (validates via Supabase webhook signature)\n *\n * @param payload Custom Access Token hook payload with user_id and existing claims\n * @returns Updated claims object with local_db_synced added\n */\nexport const userCreated = api(\n  {\n    expose: true,\n    method: \"POST\",\n    path: \"/webhooks/auth/user-created\",\n    auth: false, // No auth required for webhooks\n  },\n  async (payload: CustomAccessTokenHookPayload): Promise<CustomAccessTokenHookResponse> => {\n    try {\n      log.info(\"Received Custom Access Token hook\", {\n        userId: payload.user_id,\n        authMethod: payload.authentication_method,\n      });\n\n      // Extract user ID\n      const userId = payload.user_id;\n\n      // Get email and name from existing claims\n      const email = payload.claims.email as string;\n      const name = payload.claims.user_metadata?.name as string | undefined;\n\n      // Check if user already exists (idempotency)\n      const existingUser = await userRepo.findById(userId);\n\n      if (existingUser) {\n        log.info(\"User already exists in local database\", {\n          userId,\n          email,\n        });\n\n        // Return existing claims + our custom claim\n        return {\n          claims: {\n            ...payload.claims,\n            local_db_synced: true,\n          },\n        };\n      }\n\n      // Create user in local database\n      const user = await userRepo.create({\n        id: userId,\n        email,\n        name,\n      });\n\n      log.info(\"User created in local database\", {\n        userId: user.id,\n        email: user.email,\n      });\n\n      // Return existing claims + our custom claim\n      // IMPORTANT: Must preserve all existing claims!\n      return {\n        claims: {\n          ...payload.claims,\n          local_db_synced: true,\n        },\n      };\n    } catch (error) {\n      log.error(error, \"Failed to process Custom Access Token hook\", {\n        userId: payload.user_id,\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      // Don't throw error - return existing claims unchanged\n      // This prevents Supabase from failing authentication\n      return { claims: payload.claims };\n    }\n  }\n);\n\n/**\n * TODO: Add webhook signature verification\n *\n * For production, we should verify that the webhook request\n * is actually coming from Supabase by validating the signature.\n *\n * Supabase includes a JWT in the Authorization header that we can verify\n * using the same JWKS endpoint we use for user authentication.\n *\n * Implementation:\n * 1. Extract JWT from Authorization header\n * 2. Verify using Supabase JWKS\n * 3. Check that the JWT has a specific claim indicating it's a webhook\n * 4. Only process the webhook if verification passes\n */\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport { secret } from \"encore.dev/config\";\nimport log from \"encore.dev/log\";\nimport { db } from \"../db\";\nimport { bookmarkSourceClassifiedTopic } from \"../events/bookmark-source-classified.events\";\nimport { audioDownloadedTopic } from \"../events/audio-downloaded.events\";\nimport { audioTranscribedTopic } from \"../events/audio-transcribed.events\";\nimport { BookmarkSourceClassifiedEvent } from \"../types\";\nimport { PodcastDownloaderService } from \"../services/podcast-downloader.service\";\nimport { YouTubeDownloaderService } from \"../services/youtube-downloader.service\";\nimport { GeminiService } from \"../services/gemini.service\";\nimport { TranscriptionRepository } from \"../repositories/transcription.repository\";\nimport { extractYouTubeVideoId } from \"../utils/youtube-url.util\";\nimport { buildYouTubeUrl } from \"../utils/youtube-url.util\";\nimport { audioFilesBucket } from \"../storage\";\nimport { BookmarkSource } from \"../types/domain.types\";\n\n// Secrets\nconst geminiApiKey = secret(\"GeminiApiKey\");\n\n// Initialize services\nconst podcastDownloader = new PodcastDownloaderService();\nconst youtubeDownloader = new YouTubeDownloaderService();\nconst geminiService = new GeminiService(geminiApiKey());\nconst transcriptionRepo = new TranscriptionRepository(db);\n\n/**\n * Unified Audio Download Processor\n * Handles audio processing for different sources:\n * - YouTube: Uses Gemini API for direct transcription, falls back to Deepgram if Gemini fails\n * - Podcast: Downloads audio and processes with Deepgram\n *\n * State-of-the-art approach: Gemini (fast, 30s) â†’ Deepgram (highest accuracy, 5.26% WER)\n *\n * Exported for testing purposes\n */\nexport async function handleAudioDownload(event: BookmarkSourceClassifiedEvent) {\n  const { bookmarkId, source, url, title } = event;\n  let audioBucketKey: string | null = null;\n\n  try {\n    log.info(\"Starting audio download\", { bookmarkId, source, url });\n\n    // Check if this source requires audio processing\n    if (source !== BookmarkSource.YOUTUBE && source !== BookmarkSource.PODCAST) {\n      log.info(\"Source does not require audio processing, skipping\", {\n        bookmarkId,\n        source,\n      });\n      return;\n    }\n\n    // Check for duplicate processing (idempotency)\n    const existing = await transcriptionRepo.findByBookmarkId(bookmarkId);\n    if (existing && existing.status !== 'pending') {\n      log.warn(\"Transcription already processed, skipping duplicate event\", {\n        bookmarkId,\n        currentStatus: existing.status,\n      });\n      return;\n    }\n\n    // Create pending transcription record if not exists\n    if (!existing) {\n      await transcriptionRepo.createPending(bookmarkId);\n      log.info(\"Created pending transcription record\", { bookmarkId });\n    }\n\n    // Mark as processing\n    await transcriptionRepo.markAsProcessing(bookmarkId);\n\n    // Download audio using appropriate service based on source\n    let metadata: Record<string, string> = {};\n\n    if (source === BookmarkSource.YOUTUBE) {\n      // YouTube-specific processing: Gemini first, Deepgram fallback\n      const videoId = extractYouTubeVideoId(url);\n      if (!videoId) {\n        throw new Error(\"Invalid YouTube URL: could not extract video ID\");\n      }\n\n      log.info(\"Attempting Gemini transcription\", { bookmarkId, videoId });\n      const videoUrl = buildYouTubeUrl(videoId);\n      const geminiResult = await geminiService.transcribeYouTubeVideo(videoUrl, videoId);\n\n      if (geminiResult.error) {\n        // Gemini failed - fall back to Deepgram (state-of-the-art 5.26% WER)\n        log.warn(\"Gemini transcription failed, falling back to Deepgram\", {\n          bookmarkId,\n          videoId,\n          geminiError: geminiResult.error,\n        });\n\n        // Download audio and upload to bucket for Deepgram processing\n        audioBucketKey = await youtubeDownloader.downloadAndUpload(\n          videoId,\n          bookmarkId\n        );\n\n        log.info(\"YouTube audio downloaded for Deepgram fallback\", {\n          bookmarkId,\n          videoId,\n          audioBucketKey,\n        });\n\n        // Publish to audio-downloaded topic for Deepgram processing\n        await audioDownloadedTopic.publish({\n          bookmarkId,\n          audioBucketKey,\n          source,\n          metadata: { videoId, geminiFailure: geminiResult.error },\n        });\n\n        log.info(\"Published to Deepgram processing stage\", {\n          bookmarkId,\n          videoId,\n        });\n\n        return; // Exit - Deepgram stage will handle transcription\n      }\n\n      // SUCCESS: Gemini worked! Store transcript\n      log.info(\"Gemini transcription successful\", {\n        bookmarkId,\n        videoId,\n        processingTime: geminiResult.processingTime,\n        transcriptLength: geminiResult.transcript.length,\n      });\n\n      // Store Gemini transcript\n      await transcriptionRepo.updateGeminiTranscriptionData(bookmarkId, {\n        transcript: geminiResult.transcript,\n        confidence: geminiResult.confidence,\n      });\n\n      // Publish directly to audio-transcribed event (skip audio download stage)\n      await audioTranscribedTopic.publish({\n        bookmarkId,\n        transcript: geminiResult.transcript,\n        source,\n      });\n\n      log.info(\"Published audio-transcribed event\", { bookmarkId });\n      return; // Exit early - success!\n    } else if (source === BookmarkSource.PODCAST) {\n      // Podcast-specific download\n      log.info(\"Downloading podcast audio\", { bookmarkId, url });\n      audioBucketKey = await podcastDownloader.downloadAndUpload(url, bookmarkId);\n      metadata = { episodeUrl: url };\n    } else {\n      throw new Error(`Unsupported audio source: ${source}`);\n    }\n\n    log.info(\"Audio download completed\", {\n      bookmarkId,\n      source,\n      audioBucketKey,\n    });\n\n    // Publish event for next stage: Audio Transcription\n    const messageId = await audioDownloadedTopic.publish({\n      bookmarkId,\n      audioBucketKey,\n      source,\n      metadata,\n    });\n\n    log.info(\"Published audio-downloaded event\", {\n      bookmarkId,\n      messageId,\n    });\n  } catch (error) {\n    const errorMessage =\n      error instanceof Error ? error.message : String(error);\n\n    log.error(error, \"Audio download failed\", {\n      bookmarkId,\n      source,\n      errorMessage,\n    });\n\n    // Clean up bucket object if it was uploaded but publish failed\n    if (audioBucketKey) {\n      try {\n        await audioFilesBucket.remove(audioBucketKey);\n        log.info(\"Cleaned up bucket object after failure\", {\n          bookmarkId,\n          audioBucketKey,\n        });\n      } catch (cleanupError) {\n        log.warn(cleanupError, \"Failed to clean up bucket object\", {\n          bookmarkId,\n          audioBucketKey,\n        });\n      }\n    }\n\n    // Mark as failed\n    await transcriptionRepo.markAsFailed(\n      bookmarkId,\n      `Audio download failed: ${errorMessage}`\n    );\n  }\n}\n\n/**\n * Subscription to bookmark-source-classified topic\n * Processes bookmarks that require audio download (YouTube, Podcast)\n */\nexport const audioDownloadSubscription = new Subscription(\n  bookmarkSourceClassifiedTopic,\n  \"audio-download-processor\",\n  {\n    handler: handleAudioDownload,\n  }\n);\n", "import { Topic } from \"encore.dev/pubsub\";\nimport { BookmarkSourceClassifiedEvent } from \"../types\";\n\n/**\n * Bookmark Source Classified Topic\n * Published after classification processor identifies bookmark source\n * Triggers source-specific downstream processing (e.g., audio download for YouTube/Podcast)\n */\nexport const bookmarkSourceClassifiedTopic = new Topic<BookmarkSourceClassifiedEvent>(\n  \"bookmark-source-classified\",\n  {\n    deliveryGuarantee: \"at-least-once\",\n  }\n);\n", "import { Topic } from \"encore.dev/pubsub\";\nimport { AudioDownloadedEvent } from \"../types\";\n\n/**\n * Audio Downloaded Topic\n * Published after audio download completes (YouTube, Podcast, or other sources)\n * Triggers audio transcription with Deepgram\n */\nexport const audioDownloadedTopic = new Topic<AudioDownloadedEvent>(\n  \"audio-downloaded\",\n  {\n    deliveryGuarantee: \"at-least-once\",\n  }\n);\n", "import { Topic } from \"encore.dev/pubsub\";\nimport { AudioTranscribedEvent } from \"../types\";\n\n/**\n * Audio Transcribed Topic\n * Published after Deepgram transcription completes\n * Triggers summary generation with OpenAI\n */\nexport const audioTranscribedTopic = new Topic<AudioTranscribedEvent>(\n  \"audio-transcribed\",\n  {\n    deliveryGuarantee: \"at-least-once\",\n  }\n);\n", "import { spawn } from \"child_process\";\nimport fs from \"fs\";\nimport log from \"encore.dev/log\";\nimport { audioFilesBucket } from \"../storage\";\nimport { parsePodcastUrl, getApplePodcastRss } from \"../utils/podcast-url.util\";\nimport parsePodcast from \"node-podcast-parser\";\nimport ogs from \"open-graph-scraper\";\nimport fuzzysort from \"fuzzysort\";\n\n// Configuration constants\nconst MAX_FILE_SIZE = 500 * 1024 * 1024; // 500MB limit\nconst DOWNLOAD_TIMEOUT = 120000; // 2 minutes\nconst FETCH_TIMEOUT = 30000; // 30 seconds for HTTP requests\nconst MIN_FUZZY_MATCH_SCORE = -1000; // Minimum score for episode matching\n\n/**\n * Fetch with timeout support\n */\nasync function fetchWithTimeout(url: string, timeoutMs = FETCH_TIMEOUT): Promise<Response> {\n  const controller = new AbortController();\n  const timeout = setTimeout(() => controller.abort(), timeoutMs);\n\n  try {\n    const response = await fetch(url, { signal: controller.signal });\n    clearTimeout(timeout);\n    return response;\n  } catch (error: any) {\n    clearTimeout(timeout);\n    if (error.name === 'AbortError') {\n      throw new Error(`Request timeout after ${timeoutMs}ms: ${url}`);\n    }\n    throw error;\n  }\n}\n\n/**\n * Service for downloading podcast audio and uploading to Encore bucket\n * Mirrors YouTubeDownloaderService but for podcast episodes\n */\nexport class PodcastDownloaderService {\n  /**\n   * Downloads audio from podcast episode and uploads to bucket\n   * Full flow: Episode URL â†’ RSS feed â†’ Episode matching â†’ Audio download â†’ Bucket upload\n   * @param episodeUrl - Podcast episode URL (or RSS feed URL for latest episode)\n   * @param bookmarkId - Bookmark ID (for unique bucket key)\n   * @returns Bucket key for the uploaded audio file\n   * @throws Error if download or upload fails\n   */\n  async downloadAndUpload(\n    episodeUrl: string,\n    bookmarkId: number\n  ): Promise<string> {\n    log.info(\"Processing podcast episode\", { episodeUrl, bookmarkId });\n\n    // 1. Get RSS feed URL from episode URL\n    const rssFeedUrl = await this.getRssFeedUrl(episodeUrl);\n    log.info(\"Resolved RSS feed URL\", { rssFeedUrl, bookmarkId });\n\n    // 2. Get episode title from URL (OpenGraph metadata)\n    const episodeTitle = await this.getEpisodeTitle(episodeUrl);\n    log.info(\"Extracted episode title\", { episodeTitle, bookmarkId });\n\n    // 3. Parse RSS feed and find episode by title\n    const audioUrl = await this.findEpisodeAudioUrl(rssFeedUrl, episodeTitle);\n    log.info(\"Found episode audio URL\", { audioUrl, bookmarkId });\n\n    // 4. Download audio from direct URL and upload to bucket\n    return await this.downloadAudioFromUrl(audioUrl, bookmarkId);\n  }\n\n  /**\n   * Converts episode URL to RSS feed URL based on platform\n   * @param episodeUrl - Podcast episode or feed URL\n   * @returns RSS feed URL\n   * @throws Error if platform not supported or RSS feed not found\n   */\n  private async getRssFeedUrl(episodeUrl: string): Promise<string> {\n    const urlInfo = parsePodcastUrl(episodeUrl);\n\n    switch (urlInfo.platform) {\n      case 'rss':\n        return urlInfo.feedUrl!;\n\n      case 'apple':\n        return await getApplePodcastRss(urlInfo.showId!);\n\n      case 'google':\n        return urlInfo.feedUrl!;\n\n      default:\n        throw new Error(`Unsupported podcast URL format: ${episodeUrl}`);\n    }\n  }\n\n  /**\n   * Extracts episode title from URL using OpenGraph metadata\n   * Falls back to latest episode if extraction fails (e.g., Apple Podcasts blocks scraping)\n   * @param episodeUrl - Podcast episode URL\n   * @returns Episode title, or empty string to use latest episode\n   */\n  private async getEpisodeTitle(episodeUrl: string): Promise<string> {\n    // If it's an RSS feed URL, return empty string (will use latest episode)\n    if (\n      episodeUrl.includes('.xml') ||\n      episodeUrl.includes('/feed/') ||\n      episodeUrl.includes('/rss/')\n    ) {\n      return '';\n    }\n\n    try {\n      const { result, error: ogsError } = await ogs({\n        url: episodeUrl,\n        timeout: 10000, // 10 second timeout\n        fetchOptions: {\n          headers: {\n            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n          },\n        },\n      });\n\n      if (ogsError) {\n        throw new Error(`OpenGraph scraping failed: ${result.error || 'Unknown error'}`);\n      }\n\n      const title = result.ogTitle || result.twitterTitle || '';\n\n      if (!title) {\n        throw new Error('No title found in OpenGraph metadata');\n      }\n\n      // Clean up title (remove show name suffix if present)\n      return this.cleanEpisodeTitle(title);\n    } catch (error: any) {\n      const errorMsg = error.error || error.message || String(error);\n      log.warn(\"Failed to extract episode title, will use latest episode from RSS feed\", {\n        episodeUrl,\n        errorMsg,\n      });\n\n      // Fallback: use latest episode from RSS feed (return empty string)\n      // This handles cases where platforms (like Apple Podcasts) block web scraping\n      return '';\n    }\n  }\n\n  /**\n   * Cleans episode title by removing common show name suffixes\n   * Example: \"Episode 123: Title - Show Name\" â†’ \"Episode 123: Title\"\n   */\n  private cleanEpisodeTitle(title: string): string {\n    return title\n      .replace(/\\s*[-â€“|]\\s*[^-â€“|]+\\s*(podcast|show)\\s*$/i, '')\n      .trim();\n  }\n\n  /**\n   * Finds episode audio URL by fuzzy matching title in RSS feed\n   * @param rssFeedUrl - Podcast RSS feed URL\n   * @param episodeTitle - Episode title to search for (empty string = latest episode)\n   * @returns Direct audio file URL\n   * @throws Error if episode not found or RSS parsing fails\n   */\n  private async findEpisodeAudioUrl(\n    rssFeedUrl: string,\n    episodeTitle: string\n  ): Promise<string> {\n    // Fetch RSS feed with timeout\n    const response = await fetchWithTimeout(rssFeedUrl);\n    if (!response.ok) {\n      throw new Error(`Failed to fetch RSS feed: ${response.status} ${response.statusText}`);\n    }\n\n    const xmlData = await response.text();\n\n    // Parse RSS feed using node-podcast-parser\n    const podcast = await new Promise<import('node-podcast-parser').ParsedPodcast>(\n      (resolve, reject) => {\n        parsePodcast(xmlData, (err, data) => {\n          if (err) reject(err);\n          else resolve(data);\n        });\n      }\n    );\n\n    if (!podcast.episodes || podcast.episodes.length === 0) {\n      throw new Error('No episodes found in RSS feed');\n    }\n\n    // If episodeTitle is empty (RSS feed URL provided), use latest episode\n    if (!episodeTitle) {\n      log.info(\"Using latest episode from RSS feed\", { rssFeedUrl });\n      const latestEpisode = podcast.episodes[0];\n\n      if (!latestEpisode.enclosure?.url) {\n        throw new Error('Latest episode has no audio URL');\n      }\n\n      log.info(\"Selected latest episode\", {\n        title: latestEpisode.title,\n        audioUrl: latestEpisode.enclosure.url,\n      });\n\n      return latestEpisode.enclosure.url;\n    }\n\n    // Fuzzy match episode title with minimum score threshold\n    const episodeTitles = podcast.episodes.map((ep) => ep.title);\n    const results = fuzzysort.go(episodeTitle, episodeTitles, {\n      threshold: MIN_FUZZY_MATCH_SCORE,\n      limit: 5, // Get top 5 matches for logging\n    });\n\n    if (results.length === 0 || results[0].score < MIN_FUZZY_MATCH_SCORE) {\n      log.warn(\"No good episode match found\", {\n        searchTitle: episodeTitle,\n        availableTitles: episodeTitles.slice(0, 5),\n        bestScore: results[0]?.score,\n      });\n      throw new Error(\n        `Episode \"${episodeTitle}\" not found in RSS feed (no close matches)`\n      );\n    }\n\n    // Get best match (highest score)\n    const bestMatchIndex = results[0].target ? episodeTitles.indexOf(results[0].target) : 0;\n    const matchedEpisode = podcast.episodes[bestMatchIndex];\n\n    if (!matchedEpisode.enclosure?.url) {\n      throw new Error('Matched episode has no audio URL');\n    }\n\n    log.info(\"Matched episode by title\", {\n      searchTitle: episodeTitle,\n      matchedTitle: matchedEpisode.title,\n      score: results[0].score,\n      audioUrl: matchedEpisode.enclosure.url,\n      alternativeMatches: results.slice(1, 3).map(r => ({\n        title: r.target,\n        score: r.score\n      })),\n    });\n\n    return matchedEpisode.enclosure.url;\n  }\n\n  /**\n   * Downloads audio from direct URL to temp file, uploads to bucket, and cleans up\n   * Secure implementation using spawn to prevent command injection\n   * @param audioUrl - Direct URL to audio file\n   * @param bookmarkId - Bookmark ID for unique bucket key\n   * @returns Bucket key for uploaded audio\n   * @throws Error if download or upload fails\n   */\n  private async downloadAudioFromUrl(\n    audioUrl: string,\n    bookmarkId: number\n  ): Promise<string> {\n    const tempPath = `/tmp/podcast-${bookmarkId}.mp3`;\n    const bucketKey = `audio-${bookmarkId}-podcast.mp3`;\n\n    // Validate URL format and protocol\n    let parsedUrl: URL;\n    try {\n      parsedUrl = new URL(audioUrl);\n    } catch {\n      throw new Error(`Invalid audio URL format: ${audioUrl}`);\n    }\n\n    // Only allow HTTP/HTTPS to prevent file:// or other protocol exploits\n    if (!['http:', 'https:'].includes(parsedUrl.protocol)) {\n      throw new Error(`Unsupported protocol: ${parsedUrl.protocol}. Only HTTP(S) allowed.`);\n    }\n\n    log.info(\"Downloading podcast audio\", {\n      audioUrl,\n      bookmarkId,\n      tempPath,\n      bucketKey,\n    });\n\n    try {\n      // Download with spawn (avoids shell injection)\n      await this.downloadWithCurl(audioUrl, tempPath);\n\n      // Verify file was downloaded\n      if (!fs.existsSync(tempPath)) {\n        throw new Error('Audio file not downloaded');\n      }\n\n      const fileSize = fs.statSync(tempPath).size;\n\n      // Validate file size\n      if (fileSize === 0) {\n        fs.unlinkSync(tempPath);\n        throw new Error('Downloaded file is empty');\n      }\n\n      if (fileSize > MAX_FILE_SIZE) {\n        fs.unlinkSync(tempPath);\n        throw new Error(\n          `Audio file too large: ${(fileSize / 1024 / 1024).toFixed(2)}MB (max ${MAX_FILE_SIZE / 1024 / 1024}MB)`\n        );\n      }\n\n      log.info(\"Audio download completed, uploading to bucket\", {\n        bookmarkId,\n        tempPath,\n        fileSize,\n        bucketKey,\n      });\n\n      // Upload to Encore bucket (file size already validated)\n      const audioBuffer = fs.readFileSync(tempPath);\n      await audioFilesBucket.upload(bucketKey, audioBuffer, {\n        contentType: \"audio/mpeg\",\n      });\n\n      log.info(\"Audio uploaded to bucket\", {\n        bookmarkId,\n        bucketKey,\n        size: fileSize,\n      });\n\n      // Clean up temp file\n      fs.unlinkSync(tempPath);\n\n      return bucketKey;\n    } catch (error) {\n      // Try to clean up temp file even on error\n      try {\n        if (fs.existsSync(tempPath)) {\n          fs.unlinkSync(tempPath);\n        }\n      } catch (cleanupError) {\n        log.warn(cleanupError, \"Failed to clean up temp file\", { tempPath });\n      }\n\n      log.error(error, \"Failed to download and upload podcast audio\", {\n        audioUrl,\n        bookmarkId,\n      });\n      throw new Error(\n        `Failed to download podcast audio: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  }\n\n  /**\n   * Downloads file using curl via spawn (secure, no shell injection)\n   * @param url - URL to download\n   * @param outputPath - Where to save the file\n   */\n  private downloadWithCurl(url: string, outputPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      // Use spawn with array arguments (no shell interpretation)\n      const curl = spawn('curl', [\n        '-L',              // Follow redirects\n        '-f',              // Fail on HTTP errors\n        '--max-time', String(DOWNLOAD_TIMEOUT / 1000), // Timeout in seconds\n        '--max-filesize', String(MAX_FILE_SIZE),        // Max file size\n        '-o', outputPath,  // Output file\n        url                // URL (passed as separate argument, not interpolated)\n      ]);\n\n      let stderr = '';\n\n      curl.stderr.on('data', (data) => {\n        stderr += data.toString();\n      });\n\n      curl.on('error', (error) => {\n        reject(new Error(`curl spawn error: ${error.message}`));\n      });\n\n      curl.on('close', (code) => {\n        if (code !== 0) {\n          reject(new Error(`curl exited with code ${code}: ${stderr}`));\n        } else {\n          if (stderr) {\n            log.debug(\"curl stderr output\", { stderr });\n          }\n          resolve();\n        }\n      });\n    });\n  }\n}\n", "import { Bucket } from \"encore.dev/storage/objects\";\n\n/**\n * Audio Files Bucket\n * Temporary storage for YouTube audio files during transcription pipeline\n * Files are automatically cleaned up after Stage 2 (transcription) completes\n */\nexport const audioFilesBucket = new Bucket(\"audio-files\", {\n  versioned: false,\n});\n", "export type PodcastPlatform = 'rss' | 'apple' | 'google' | 'unknown';\n\nexport interface PodcastUrlInfo {\n  platform: PodcastPlatform;\n  showId?: string;\n  feedUrl?: string;\n}\n\n/**\n * iTunes API response type\n */\ninterface ITunesLookupResponse {\n  resultCount: number;\n  results: Array<{\n    feedUrl?: string;\n    collectionName?: string;\n    artistName?: string;\n  }>;\n}\n\n/**\n * Detects podcast platform and extracts IDs/feed URL from URL\n * @param url - Podcast episode or feed URL\n * @returns Parsed URL information with platform and IDs\n */\nexport function parsePodcastUrl(url: string): PodcastUrlInfo {\n  // Apple Podcasts: https://podcasts.apple.com/us/podcast/name/id123456?i=789\n  const appleMatch = url.match(/podcasts\\.apple\\.com.*\\/id(\\d+)/);\n  if (appleMatch) {\n    return {\n      platform: 'apple',\n      showId: appleMatch[1],\n    };\n  }\n\n  // Google Podcasts: https://podcasts.google.com/feed/[base64_rss_url]\n  const googleMatch = url.match(/podcasts\\.google\\.com\\/feed\\/([^\\/\\?]+)/);\n  if (googleMatch) {\n    try {\n      const feedUrl = Buffer.from(googleMatch[1], 'base64').toString('utf-8');\n\n      // Validate decoded URL\n      new URL(feedUrl); // Throws if invalid\n\n      return { platform: 'google', feedUrl };\n    } catch (error) {\n      throw new Error(\n        `Invalid Google Podcasts URL format: failed to decode feed URL (${error instanceof Error ? error.message : String(error)})`\n      );\n    }\n  }\n\n  // RSS Direct: Contains .xml or /feed/ or /rss/\n  if (url.includes('.xml') || url.includes('/feed') || url.includes('/rss')) {\n    return { platform: 'rss', feedUrl: url };\n  }\n\n  return { platform: 'unknown' };\n}\n\n/**\n * Gets RSS feed URL from Apple Podcasts show ID using iTunes API\n * @param showId - Apple Podcasts show ID\n * @returns RSS feed URL\n * @throws Error if RSS feed not found or API fails\n */\nexport async function getApplePodcastRss(showId: string): Promise<string> {\n  const response = await fetch(\n    `https://itunes.apple.com/lookup?id=${showId}&entity=podcast`\n  );\n\n  if (!response.ok) {\n    throw new Error(\n      `iTunes API error: ${response.status} ${response.statusText}`\n    );\n  }\n\n  const data = (await response.json()) as ITunesLookupResponse;\n\n  if (!data.results || data.results.length === 0) {\n    throw new Error(\n      `No podcast found for Apple Podcasts ID: ${showId}`\n    );\n  }\n\n  const feedUrl = data.results[0]?.feedUrl;\n  if (!feedUrl) {\n    throw new Error('RSS feed not found for this Apple Podcast');\n  }\n\n  return feedUrl;\n}\n", "import { promisify } from \"util\";\nimport { exec as execCallback } from \"child_process\";\nimport fs from \"fs\";\nimport log from \"encore.dev/log\";\nimport { YOUTUBE_CONFIG } from \"../config/transcription.config\";\nimport { buildYouTubeUrl } from \"../utils/youtube-url.util\";\nimport { audioFilesBucket } from \"../storage\";\n\nconst exec = promisify(execCallback);\n\n/**\n * Find yt-dlp binary path\n * Checks common installation locations in order\n */\nfunction findYtDlpPath(): string {\n  const paths = [\n    \"/opt/homebrew/bin/yt-dlp\", // ARM Mac (M1/M2/M3)\n    \"/usr/local/bin/yt-dlp\",    // Intel Mac\n    \"yt-dlp\",                    // Fallback to PATH\n  ];\n\n  for (const path of paths) {\n    try {\n      if (path.startsWith(\"/\") && fs.existsSync(path)) {\n        return path;\n      }\n    } catch {\n      // Skip if can't check existence\n    }\n  }\n\n  // Fallback to just \"yt-dlp\" and let PATH handle it\n  return \"yt-dlp\";\n}\n\nconst YT_DLP_PATH = findYtDlpPath();\n\n// Log detected path on startup\nlog.info(\"YouTube downloader initialized\", {\n  ytDlpPath: YT_DLP_PATH,\n  isFullPath: YT_DLP_PATH.startsWith(\"/\"),\n});\n\n/**\n * Service for downloading YouTube audio and uploading to Encore bucket\n */\nexport class YouTubeDownloaderService {\n  /**\n   * Downloads audio from YouTube and uploads to bucket\n   * @param videoId - YouTube video ID\n   * @param bookmarkId - Bookmark ID (for unique bucket key)\n   * @returns Bucket key for the uploaded audio file\n   * @throws Error if download or upload fails\n   */\n  async downloadAndUpload(\n    videoId: string,\n    bookmarkId: number\n  ): Promise<string> {\n    const youtubeUrl = buildYouTubeUrl(videoId);\n    const tempPath = YOUTUBE_CONFIG.getTempPath(videoId);\n    const bucketKey = `audio-${bookmarkId}-${videoId}.mp3`;\n\n    log.info(\"Downloading YouTube audio with yt-dlp\", {\n      videoId,\n      bookmarkId,\n      youtubeUrl,\n      tempPath,\n      bucketKey,\n    });\n\n    try {\n      // Download audio to temp file\n      const { stdout, stderr } = await exec(\n        `${YT_DLP_PATH} -x --audio-format ${YOUTUBE_CONFIG.audioFormat} --audio-quality ${YOUTUBE_CONFIG.audioQuality} -o \"${tempPath}\" \"${youtubeUrl}\"`\n      );\n\n      if (stderr && !stderr.includes(\"Deleting original file\")) {\n        log.warn(\"yt-dlp stderr output\", { stderr });\n      }\n\n      const fileSize = fs.statSync(tempPath).size;\n      log.info(\"Audio download completed, uploading to bucket\", {\n        videoId,\n        bookmarkId,\n        tempPath,\n        fileSize,\n        bucketKey,\n      });\n\n      // Upload to Encore bucket\n      const audioBuffer = fs.readFileSync(tempPath);\n      await audioFilesBucket.upload(bucketKey, audioBuffer, {\n        contentType: \"audio/mpeg\",\n      });\n\n      log.info(\"Audio uploaded to bucket\", {\n        videoId,\n        bookmarkId,\n        bucketKey,\n        size: audioBuffer.length,\n      });\n\n      // Clean up temp file\n      fs.unlinkSync(tempPath);\n\n      return bucketKey;\n    } catch (error) {\n      // Try to clean up temp file even on error\n      try {\n        if (fs.existsSync(tempPath)) {\n          fs.unlinkSync(tempPath);\n        }\n      } catch (cleanupError) {\n        log.warn(cleanupError, \"Failed to clean up temp file\", { tempPath });\n      }\n\n      log.error(error, \"Failed to download and upload YouTube audio\", {\n        videoId,\n        bookmarkId,\n      });\n      throw new Error(\n        `Failed to download YouTube audio: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  }\n}\n", "import os from \"os\";\nimport path from \"path\";\n\n// ============================================\n// Deepgram Configuration\n// ============================================\n\nexport const DEEPGRAM_CONFIG = {\n  model: \"nova-3\" as const,\n  smartFormat: true,\n  paragraphs: true,\n  punctuate: true,\n  diarize: true,\n  sentiment: true,\n  summarize: \"v2\" as const,\n  intents: true,\n  topics: true,\n  language: \"en\" as const,\n} as const;\n\n// ============================================\n// OpenAI Configuration\n// ============================================\n\nexport const OPENAI_CONFIG = {\n  model: \"gpt-4.1-mini\" as const,\n  temperature: 0.7,\n  maxOutputTokens: 500,\n  instructions:\n    \"You are a helpful assistant that creates concise, informative summaries of video transcripts. Focus on the main points and key takeaways.\",\n} as const;\n\n// ============================================\n// YouTube Downloader Configuration\n// ============================================\n\nexport const YOUTUBE_CONFIG = {\n  audioFormat: \"mp3\" as const,\n  audioQuality: \"0\" as const, // Best quality\n  getTempPath: (videoId: string) => path.join(os.tmpdir(), `${videoId}.mp3`),\n} as const;\n\n// ============================================\n// URL Patterns\n// ============================================\n\nexport const YOUTUBE_URL_PATTERNS = [\n  /(?:youtube\\.com\\/watch\\?v=|youtu\\.be\\/)([^&\\s?]+)/,\n  /youtube\\.com\\/embed\\/([^&\\s?]+)/,\n  /youtube\\.com\\/v\\/([^&\\s?]+)/,\n] as const;\n\n// ============================================\n// Gemini Configuration\n// ============================================\n\nexport const GEMINI_CONFIG = {\n  model: \"gemini-2.5-flash\" as const, // Stable Gemini 2.5 Flash with API key support\n  maxVideoLength: 7200, // 2 hours max (Gemini limit)\n  timeout: 120000, // 2 minutes timeout\n  retries: 2, // Retry twice on failure\n} as const;\n", "import { YOUTUBE_URL_PATTERNS } from \"../config/transcription.config\";\n\n/**\n * Extracts YouTube video ID from various YouTube URL formats\n * @param url - YouTube URL\n * @returns Video ID or null if invalid\n */\nexport function extractYouTubeVideoId(url: string): string | null {\n  for (const pattern of YOUTUBE_URL_PATTERNS) {\n    const match = url.match(pattern);\n    if (match && match[1]) {\n      return match[1];\n    }\n  }\n  return null;\n}\n\n/**\n * Constructs a standard YouTube watch URL from a video ID\n * @param videoId - YouTube video ID\n * @returns Standard YouTube watch URL\n */\nexport function buildYouTubeUrl(videoId: string): string {\n  return `https://www.youtube.com/watch?v=${videoId}`;\n}\n", "import { GoogleGenerativeAI } from \"@google/generative-ai\";\nimport log from \"encore.dev/log\";\nimport { GEMINI_CONFIG } from \"../config/transcription.config\";\nimport { GeminiTranscriptResponse, GeminiErrorType } from \"../types/gemini.types\";\n\n/**\n * Gemini Service\n * Handles YouTube video transcription using Google Gemini 2.5 Flash\n *\n * Uses @google/generative-ai SDK - specifically designed for Gemini Developer API\n * with API key authentication (not Vertex AI / OAuth2)\n *\n * Benefits over yt-dlp + Deepgram:\n * - No audio download required (direct YouTube URL processing)\n * - Faster: ~30 seconds vs 2-5 minutes\n * - Cheaper: $0.02-0.05 per hour vs $0.10\n * - Simpler: Single API call vs multi-stage pipeline\n *\n * Limitations:\n * - Only works with public YouTube videos\n * - Max 2 hours of video\n * - Free tier: 8 hours of YouTube video per day\n */\nexport class GeminiService {\n  private readonly client: GoogleGenerativeAI;\n\n  constructor(apiKey: string) {\n    // Validate API key\n    if (!apiKey || apiKey.trim() === '') {\n      throw new Error('Gemini API key is empty or undefined');\n    }\n\n    // Log key info for debugging (first/last 4 chars only)\n    const maskedKey = apiKey.length > 8\n      ? `${apiKey.substring(0, 4)}...${apiKey.substring(apiKey.length - 4)}`\n      : '***masked***';\n\n    log.info(\"Initializing Gemini service\", {\n      apiKeyLength: apiKey.length,\n      apiKeyPreview: maskedKey,\n      sdkVersion: \"@google/generative-ai v0.24.1\",\n    });\n\n    this.client = new GoogleGenerativeAI(apiKey);\n  }\n\n  /**\n   * Transcribes a YouTube video using Gemini 2.5 Flash\n   *\n   * @param videoUrl - Full YouTube URL (e.g., https://youtube.com/watch?v=VIDEO_ID)\n   * @param videoId - YouTube video ID (for logging)\n   * @returns Transcript with metadata or error\n   */\n  async transcribeYouTubeVideo(\n    videoUrl: string,\n    videoId: string\n  ): Promise<GeminiTranscriptResponse> {\n    const startTime = Date.now();\n\n    try {\n      log.info(\"Starting Gemini transcription\", {\n        videoId,\n        videoUrl,\n        model: GEMINI_CONFIG.model,\n      });\n\n      const model = this.client.getGenerativeModel({\n        model: GEMINI_CONFIG.model,\n      });\n\n      // Create prompt with explicit instructions\n      const prompt = `Please provide a complete, accurate transcript of this YouTube video.\n\nRequirements:\n1. Include ALL spoken words verbatim\n2. Use proper punctuation and paragraph breaks\n3. Do NOT include timestamps or speaker labels\n4. Do NOT add commentary or analysis\n5. Just provide the raw transcript text\n\nReturn ONLY the transcript, nothing else.`;\n\n      // Make API call with timeout\n      // Using the official API format from Google AI documentation\n      // This ensures we're using the Gemini API endpoint, not Vertex AI\n      const result = await Promise.race([\n        model.generateContent([\n          prompt,\n          {\n            fileData: {\n              mimeType: \"video/*\", // Generic video MIME type for YouTube URLs\n              fileUri: videoUrl,\n            },\n          },\n        ]),\n        this.createTimeout(GEMINI_CONFIG.timeout),\n      ]);\n\n      // Check if timeout occurred\n      if (result === \"TIMEOUT\") {\n        throw new Error(GeminiErrorType.TIMEOUT);\n      }\n\n      const response = result.response;\n      const transcript = response.text().trim();\n\n      // Validate transcript\n      if (!transcript || transcript.length < 10) {\n        throw new Error(\"Empty or invalid transcript received\");\n      }\n\n      const processingTime = Date.now() - startTime;\n\n      log.info(\"Gemini transcription successful\", {\n        videoId,\n        transcriptLength: transcript.length,\n        processingTime,\n        wordsCount: transcript.split(/\\s+/).length,\n      });\n\n      return {\n        transcript,\n        confidence: 0.95, // Gemini is highly accurate\n        processingTime,\n        method: \"gemini\",\n      };\n    } catch (error) {\n      const processingTime = Date.now() - startTime;\n      const errorMessage = this.parseError(error);\n      const errorType = this.classifyError(error);\n\n      // Enhanced error logging for debugging production issues\n      const errorDetails: Record<string, unknown> = {\n        videoId,\n        videoUrl,\n        model: GEMINI_CONFIG.model,\n        errorMessage,\n        errorType,\n        processingTime,\n      };\n\n      // Capture full error details\n      if (error instanceof Error) {\n        errorDetails.errorName = error.name;\n        errorDetails.errorStack = error.stack;\n\n        // Check for response errors from the API\n        const anyError = error as any;\n        if (anyError.response) {\n          errorDetails.responseStatus = anyError.response.status;\n          errorDetails.responseStatusText = anyError.response.statusText;\n          errorDetails.responseData = JSON.stringify(anyError.response.data || {});\n        }\n        if (anyError.message) {\n          errorDetails.fullErrorMessage = anyError.message;\n        }\n        if (anyError.code) {\n          errorDetails.errorCode = anyError.code;\n        }\n      }\n\n      log.error(\"Gemini transcription failed - Full error details\", errorDetails);\n\n      return {\n        transcript: \"\",\n        confidence: 0,\n        processingTime,\n        method: \"gemini\",\n        error: errorMessage,\n      };\n    }\n  }\n\n  /**\n   * Creates a timeout promise\n   */\n  private createTimeout(ms: number): Promise<\"TIMEOUT\"> {\n    return new Promise((resolve) => {\n      setTimeout(() => resolve(\"TIMEOUT\"), ms);\n    });\n  }\n\n  /**\n   * Parses error to human-readable message\n   */\n  private parseError(error: unknown): string {\n    if (error instanceof Error) {\n      return error.message;\n    }\n    return String(error);\n  }\n\n  /**\n   * Classifies error type for analytics\n   */\n  private classifyError(error: unknown): GeminiErrorType {\n    const errorStr = String(error).toLowerCase();\n\n    if (errorStr.includes(\"private\") || errorStr.includes(\"unavailable\")) {\n      return GeminiErrorType.PRIVATE_VIDEO;\n    }\n    if (errorStr.includes(\"rate limit\") || errorStr.includes(\"quota\")) {\n      return GeminiErrorType.RATE_LIMIT;\n    }\n    if (errorStr.includes(\"timeout\")) {\n      return GeminiErrorType.TIMEOUT;\n    }\n    if (errorStr.includes(\"invalid\") || errorStr.includes(\"url\")) {\n      return GeminiErrorType.INVALID_URL;\n    }\n    if (errorStr.includes(\"too long\") || errorStr.includes(\"duration\")) {\n      return GeminiErrorType.VIDEO_TOO_LONG;\n    }\n    if (errorStr.includes(\"network\") || errorStr.includes(\"connection\")) {\n      return GeminiErrorType.NETWORK_ERROR;\n    }\n\n    return GeminiErrorType.API_ERROR;\n  }\n}\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport { secret } from \"encore.dev/config\";\nimport log from \"encore.dev/log\";\nimport { db } from \"../db\";\nimport { audioDownloadedTopic } from \"../events/audio-downloaded.events\";\nimport { audioTranscribedTopic } from \"../events/audio-transcribed.events\";\nimport { AudioDownloadedEvent } from \"../types\";\nimport { DeepgramService } from \"../services/deepgram.service\";\nimport { TranscriptionRepository } from \"../repositories/transcription.repository\";\nimport { extractDeepgramData } from \"../utils/deepgram-extractor.util\";\nimport { audioFilesBucket } from \"../storage\";\n\n// Secrets\nconst deepgramApiKey = secret(\"DeepgramAPIKey\");\n\n// Initialize services\nconst deepgramService = new DeepgramService(deepgramApiKey());\nconst transcriptionRepo = new TranscriptionRepository(db);\n\n/**\n * Audio Transcription Processor\n * Transcribes audio with Deepgram and publishes transcription event\n * Independent: Works with any audio source, uses source metadata for tracking\n *\n * Exported for testing purposes\n */\nexport async function handleAudioTranscription(event: AudioDownloadedEvent) {\n  const { bookmarkId, audioBucketKey, source, metadata } = event;\n\n  try {\n    log.info(\"Starting audio transcription\", {\n      bookmarkId,\n      source,\n      audioBucketKey,\n      metadata,\n    });\n\n    // Download audio from bucket\n    const audioBuffer = await audioFilesBucket.download(audioBucketKey);\n    log.info(\"Audio downloaded from bucket\", {\n      bookmarkId,\n      audioBucketKey,\n      bufferSize: audioBuffer.length,\n    });\n\n    // Transcribe with Deepgram\n    const deepgramResponse = await deepgramService.transcribe(\n      audioBuffer,\n      audioBucketKey\n    );\n\n    // Extract data from Deepgram response\n    const {\n      transcript,\n      confidence,\n      duration,\n      sentiment,\n      sentimentScore,\n      deepgramSummary,\n    } = extractDeepgramData(deepgramResponse);\n\n    log.info(\"Transcription completed\", {\n      bookmarkId,\n      transcriptLength: transcript.length,\n      confidence,\n      duration,\n      sentiment,\n      sentimentScore,\n      hasSummary: !!deepgramSummary,\n      hasIntents: !!deepgramResponse.results.intents,\n      hasTopics: !!deepgramResponse.results.topics,\n    });\n\n    // Store transcription data in database\n    // Note: This marks the method as 'deepgram' in the updateTranscriptionData method\n    await transcriptionRepo.updateTranscriptionData(bookmarkId, {\n      transcript,\n      deepgramSummary,\n      sentiment,\n      sentimentScore,\n      deepgramResponse,\n      duration,\n      confidence,\n    });\n\n    log.info(\"Transcription data stored in database\", { bookmarkId });\n\n    // Delete audio from bucket (no longer needed)\n    await audioFilesBucket.remove(audioBucketKey);\n    log.info(\"Audio deleted from bucket\", { bookmarkId, audioBucketKey });\n\n    // Publish audio-transcribed event\n    const messageId = await audioTranscribedTopic.publish({\n      bookmarkId,\n      transcript,\n      source,\n    });\n\n    log.info(\"Audio transcription completed, published event\", {\n      bookmarkId,\n      source,\n      messageId,\n    });\n  } catch (error) {\n    const errorMessage =\n      error instanceof Error ? error.message : String(error);\n\n    log.error(error, \"Audio transcription failed\", {\n      bookmarkId,\n      source,\n      errorMessage,\n    });\n\n    // Mark as failed\n    await transcriptionRepo.markAsFailed(\n      bookmarkId,\n      `Transcription failed: ${errorMessage}`\n    );\n\n    // Try to clean up bucket object even on failure\n    try {\n      await audioFilesBucket.remove(audioBucketKey);\n      log.info(\"Audio deleted from bucket after failure\", {\n        bookmarkId,\n        audioBucketKey,\n      });\n    } catch (cleanupError) {\n      log.warn(cleanupError, \"Failed to delete audio from bucket\", {\n        bookmarkId,\n        audioBucketKey,\n      });\n    }\n  }\n}\n\n// Subscription to audio-downloaded topic\nexport const audioTranscriptionSubscription = new Subscription(\n  audioDownloadedTopic,\n  \"audio-transcription-processor\",\n  {\n    handler: handleAudioTranscription,\n  }\n);\n", "import { createClient } from \"@deepgram/sdk\";\nimport log from \"encore.dev/log\";\nimport { DeepgramResponse } from \"../types\";\nimport { DEEPGRAM_CONFIG } from \"../config/transcription.config\";\n\n/**\n * Service for Deepgram transcription with Audio Intelligence\n */\nexport class DeepgramService {\n  constructor(private readonly apiKey: string) {}\n\n  /**\n   * Transcribes audio buffer using Deepgram with all Audio Intelligence features\n   * @param audioBuffer - Audio file as Buffer\n   * @param audioKey - Identifier for logging (e.g., bucket key or video ID)\n   * @returns Deepgram response with transcription and audio intelligence data\n   * @throws Error if transcription fails\n   */\n  async transcribe(\n    audioBuffer: Buffer,\n    audioKey: string\n  ): Promise<DeepgramResponse> {\n    const deepgram = createClient(this.apiKey);\n\n    log.info(\n      \"Transcribing audio with Deepgram Nova-3 and Audio Intelligence\",\n      { audioKey, bufferSize: audioBuffer.length }\n    );\n\n    try {\n      // Transcribe with Deepgram using latest model and audio intelligence features\n      const { result, error } = await deepgram.listen.prerecorded.transcribeFile(\n        audioBuffer,\n        DEEPGRAM_CONFIG\n      );\n\n      if (error) {\n        throw new Error(`Deepgram API error: ${error.message}`);\n      }\n\n      // Cast to our custom type that includes audio intelligence features\n      const response = result as unknown as DeepgramResponse;\n\n      log.info(\"Deepgram transcription completed\", {\n        duration: response.metadata.duration,\n        channels: response.metadata.channels,\n        hasSentiment: !!response.results.sentiments,\n        hasSummary: !!response.results.summary,\n        hasIntents: !!response.results.intents,\n        hasTopics: !!response.results.topics,\n      });\n\n      return response;\n    } catch (error) {\n      log.error(error, \"Failed to transcribe with Deepgram\", { audioKey });\n      throw error;\n    }\n  }\n}\n", "import { DeepgramResponse } from \"../types\";\n\n/**\n * Extracted transcription data from Deepgram response\n */\nexport interface ExtractedTranscriptionData {\n  transcript: string;\n  confidence: number;\n  duration: number;\n  sentiment: \"positive\" | \"negative\" | \"neutral\" | null;\n  sentimentScore: number | null;\n  deepgramSummary: string | null;\n}\n\n/**\n * Extracts transcription and audio intelligence data from Deepgram response\n * @param response - Deepgram API response\n * @returns Extracted data in a structured format\n * @throws Error if no transcript is found\n */\nexport function extractDeepgramData(\n  response: DeepgramResponse\n): ExtractedTranscriptionData {\n  // Extract transcript\n  const transcript =\n    response.results.channels[0]?.alternatives[0]?.transcript || \"\";\n\n  if (!transcript) {\n    throw new Error(\"No transcript returned from Deepgram\");\n  }\n\n  // Extract confidence and duration\n  const confidence =\n    response.results.channels[0]?.alternatives[0]?.confidence || 0;\n  const duration = response.metadata.duration;\n\n  // Extract audio intelligence data\n  const sentiment = response.results.sentiments?.average?.sentiment || null;\n  const sentimentScore =\n    response.results.sentiments?.average?.sentiment_score || null;\n  const deepgramSummary = response.results.summary?.short || null;\n\n  return {\n    transcript,\n    confidence,\n    duration,\n    sentiment,\n    sentimentScore,\n    deepgramSummary,\n  };\n}\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport log from \"encore.dev/log\";\nimport { bookmarkCreatedTopic } from \"../events/bookmark-created.events\";\nimport { bookmarkSourceClassifiedTopic } from \"../events/bookmark-source-classified.events\";\nimport { BookmarkRepository } from \"../repositories/bookmark.repository\";\nimport { db } from \"../db\";\nimport { classifyBookmarkUrl } from \"../utils/bookmark-classifier.util\";\nimport { BookmarkSource } from \"../types/domain.types\";\n\nconst bookmarkRepo = new BookmarkRepository(db);\n\n/**\n * Bookmark Classification Processor\n * Single responsibility: Classify bookmark URL and update source\n * Independent: Publishes generic event, doesn't know about downstream processors\n *\n * Exported for testing purposes\n */\nexport async function handleBookmarkClassification(event: {\n  bookmarkId: number;\n  url: string;\n  source: string;\n  title?: string;\n}): Promise<void> {\n  const { bookmarkId, url, source, title } = event;\n\n  log.info(\"Received bookmark for classification\", {\n    bookmarkId,\n    url,\n    currentSource: source,\n  });\n\n  let finalSource = source;\n\n  // Classify URL if source is unknown ('web')\n  if (source === BookmarkSource.WEB) {\n    const detectedSource = classifyBookmarkUrl(url);\n\n    log.info(\"URL classification completed\", {\n      bookmarkId,\n      url,\n      detectedSource,\n    });\n\n    // Update source in database if different from 'web'\n    if (detectedSource !== BookmarkSource.WEB) {\n      try {\n        await bookmarkRepo.updateSource(bookmarkId, detectedSource);\n        log.info(\"Updated bookmark source\", {\n          bookmarkId,\n          oldSource: source,\n          newSource: detectedSource,\n        });\n        finalSource = detectedSource;\n      } catch (error) {\n        log.error(error, \"Failed to update bookmark source\", {\n          bookmarkId,\n          detectedSource,\n        });\n        // Continue with original source even if update fails\n      }\n    }\n  } else {\n    log.info(\"Skipping classification, source already known\", {\n      bookmarkId,\n      source,\n    });\n  }\n\n  // Publish generic classified event\n  // Downstream processors decide if they need to act on this source\n  try {\n    const messageId = await bookmarkSourceClassifiedTopic.publish({\n      bookmarkId,\n      source: finalSource,\n      url,\n      title,\n    });\n\n    log.info(\"Published bookmark-source-classified event\", {\n      bookmarkId,\n      source: finalSource,\n      messageId,\n    });\n  } catch (error) {\n    log.error(error, \"Failed to publish bookmark-source-classified event\", {\n      bookmarkId,\n      source: finalSource,\n    });\n    throw error; // Rethrow to trigger retry\n  }\n}\n\n/**\n * Subscription to bookmark-created topic\n * Processes all newly created bookmarks for classification\n */\nexport const bookmarkClassificationSubscription = new Subscription(\n  bookmarkCreatedTopic,\n  \"bookmark-classification-processor\",\n  {\n    handler: handleBookmarkClassification,\n  }\n);\n", "import { BookmarkSource } from \"../types/domain.types\";\nimport { extractYouTubeVideoId } from \"./youtube-url.util\";\nimport { parsePodcastUrl } from \"./podcast-url.util\";\n\n/**\n * Classifies a bookmark URL to determine its source type\n * Uses heuristics and regex patterns to detect YouTube, Podcast, and other sources\n * Future: Can be replaced with ML-based classification\n *\n * @param url - The bookmark URL to classify\n * @returns The detected BookmarkSource type\n */\nexport function classifyBookmarkUrl(url: string): BookmarkSource {\n  // YouTube detection\n  const youtubeVideoId = extractYouTubeVideoId(url);\n  if (youtubeVideoId) {\n    return BookmarkSource.YOUTUBE;\n  }\n\n  // Podcast detection\n  const podcastInfo = parsePodcastUrl(url);\n  if (podcastInfo.platform !== 'unknown') {\n    return BookmarkSource.PODCAST;\n  }\n\n  // Reddit detection\n  if (url.includes('reddit.com/r/') || url.includes('redd.it/')) {\n    return BookmarkSource.REDDIT;\n  }\n\n  // Twitter/X detection\n  if (url.includes('twitter.com/') || url.includes('x.com/')) {\n    return BookmarkSource.TWITTER;\n  }\n\n  // LinkedIn detection\n  if (url.includes('linkedin.com/')) {\n    return BookmarkSource.LINKEDIN;\n  }\n\n  // Blog detection heuristics\n  // Common blog platforms and patterns\n  if (\n    url.includes('medium.com/') ||\n    url.includes('substack.com/') ||\n    url.includes('wordpress.com/') ||\n    url.includes('blogspot.com/') ||\n    url.includes('ghost.io/') ||\n    url.includes('/blog/') ||\n    url.includes('/article/')\n  ) {\n    return BookmarkSource.BLOG;\n  }\n\n  // Default to web if no specific pattern matches\n  return BookmarkSource.WEB;\n}\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport log from \"encore.dev/log\";\nimport { secret } from \"encore.dev/config\";\nimport { bookmarkSourceClassifiedTopic } from \"../events/bookmark-source-classified.events\";\nimport { contentExtractedTopic } from \"../events/content-extracted.events\";\nimport { BookmarkSourceClassifiedEvent, BookmarkSource, ContentStatus } from \"../types\";\nimport { FirecrawlService } from \"../services/firecrawl.service\";\nimport { WebContentRepository } from \"../repositories/web-content.repository\";\nimport { calculateReadingTime } from \"../config/firecrawl.config\";\nimport { db } from \"../db\";\n\n// Initialize dependencies\nconst firecrawlApiKey = secret(\"FirecrawlAPIKey\");\nconst firecrawlService = new FirecrawlService(firecrawlApiKey());\nconst webContentRepo = new WebContentRepository(db);\n\n/**\n * Textual sources that require web content extraction\n * Audio sources (YouTube, Podcast) are handled by audio-download processor\n */\nconst TEXTUAL_SOURCES = [\n  BookmarkSource.BLOG,\n  BookmarkSource.WEB,\n  BookmarkSource.REDDIT,\n  BookmarkSource.TWITTER,\n  BookmarkSource.LINKEDIN,\n];\n\n/**\n * Content Extraction Processor\n *\n * Responsibility:\n * - Receives bookmark-source-classified events\n * - Filters for textual sources\n * - Extracts content using FireCrawl API\n * - Stores content in web_contents table\n * - Publishes content-extracted event for summarization\n *\n * Idempotency: Checks existing status before processing\n * Fault Tolerance: Marks as failed on error, allowing manual retry\n *\n * Exported for testing purposes\n */\nexport async function handleContentExtraction(\n  event: BookmarkSourceClassifiedEvent\n): Promise<void> {\n  const { bookmarkId, source, url, title } = event;\n\n  // Skip non-textual sources (audio handled by separate processor)\n  if (!TEXTUAL_SOURCES.includes(source as BookmarkSource)) {\n    log.info(\"Skipping content extraction for non-textual source\", {\n      bookmarkId,\n      source,\n    });\n    return;\n  }\n\n  try {\n    log.info(\"Starting content extraction\", {\n      bookmarkId,\n      url,\n      source,\n    });\n\n    // Check for idempotency - avoid duplicate processing\n    const existing = await webContentRepo.findByBookmarkId(bookmarkId);\n\n    if (existing && existing.status !== ContentStatus.PENDING) {\n      log.warn(\"Content already processed or in progress, skipping\", {\n        bookmarkId,\n        currentStatus: existing.status,\n      });\n      return;\n    }\n\n    // Create pending record if doesn't exist\n    if (!existing) {\n      await webContentRepo.createPending(bookmarkId);\n      log.info(\"Created pending web content record\", { bookmarkId });\n    }\n\n    // Mark as processing\n    await webContentRepo.markAsProcessing(bookmarkId);\n\n    // Extract content using FireCrawl\n    const scraped = await firecrawlService.scrape(url);\n\n    if (!scraped.success || !scraped.data.markdown) {\n      throw new Error(\"FireCrawl returned unsuccessful response or missing markdown\");\n    }\n\n    // Calculate content metrics\n    const markdown = scraped.data.markdown;\n    const wordCount = markdown.split(/\\s+/).filter(w => w.length > 0).length;\n    const charCount = markdown.length;\n    const estimatedReadingMinutes = calculateReadingTime(wordCount);\n\n    // Safely extract metadata with fallbacks for missing/undefined metadata\n    const metadata = scraped.data.metadata || {};\n\n    log.info(\"Content extraction metrics calculated\", {\n      bookmarkId,\n      wordCount,\n      charCount,\n      estimatedReadingMinutes,\n      hasMetadata: !!scraped.data.metadata,\n    });\n\n    // Store extracted content\n    await webContentRepo.updateContent(bookmarkId, {\n      raw_markdown: markdown,\n      raw_html: scraped.data.html || \"\",\n      // Safe metadata access with fallback chain: metadata â†’ event title â†’ \"Untitled\"\n      page_title: metadata.title || title || \"Untitled\",\n      page_description: metadata.description || \"\",\n      language: metadata.language || \"en\",\n      word_count: wordCount,\n      char_count: charCount,\n      estimated_reading_minutes: estimatedReadingMinutes,\n      // Store full metadata object (could be empty {})\n      metadata: metadata,\n    });\n\n    log.info(\"Content extraction completed successfully\", {\n      bookmarkId,\n      wordCount,\n      pageTitle: metadata.title || title || \"Untitled\",\n    });\n\n    // Publish event for summarization stage\n    const messageId = await contentExtractedTopic.publish({\n      bookmarkId,\n      content: markdown,\n      wordCount,\n      source,\n    });\n\n    log.info(\"Published content-extracted event\", {\n      bookmarkId,\n      messageId,\n    });\n\n  } catch (error) {\n    const errorMessage = error instanceof Error ? error.message : String(error);\n\n    log.error(error, \"Content extraction failed\", {\n      bookmarkId,\n      url,\n      source,\n      errorMessage,\n    });\n\n    // Mark as failed for visibility and potential manual retry\n    await webContentRepo.markAsFailed(\n      bookmarkId,\n      `Extraction failed: ${errorMessage}`\n    );\n  }\n}\n\n/**\n * Subscription to bookmark-source-classified topic\n * Processes all newly classified bookmarks, filtering for textual sources\n */\nexport const contentExtractionSubscription = new Subscription(\n  bookmarkSourceClassifiedTopic,\n  \"content-extraction-processor\",\n  {\n    handler: handleContentExtraction,\n  }\n);\n", "import { Topic } from \"encore.dev/pubsub\";\n\n/**\n * Event published after FireCrawl successfully extracts web content\n * Consumed by content-summary processor for AI summarization\n */\nexport interface ContentExtractedEvent {\n  bookmarkId: number;\n  content: string; // Markdown content for summarization\n  wordCount: number; // For content type classification\n  source: string; // Original bookmark source (blog, reddit, etc.)\n}\n\n/**\n * Topic for content extraction events\n * Delivery guarantee: at-least-once (processors must be idempotent)\n */\nexport const contentExtractedTopic = new Topic<ContentExtractedEvent>(\n  \"content-extracted\",\n  {\n    deliveryGuarantee: \"at-least-once\",\n  }\n);\n", "import log from \"encore.dev/log\";\nimport { FIRECRAWL_CONFIG } from \"../config/firecrawl.config\";\nimport type { FirecrawlScrapeResponse } from \"../types/web-content.types\";\n\n/**\n * Service for interacting with FireCrawl API\n * Handles web content extraction with retry logic and error handling\n */\nexport class FirecrawlService {\n  constructor(private readonly apiKey: string) {}\n\n  /**\n   * Scrapes a URL and returns clean markdown content with metadata\n   *\n   * @param url - The URL to scrape\n   * @returns FireCrawl response with markdown, HTML, and metadata\n   * @throws Error if scraping fails after all retries\n   */\n  async scrape(url: string): Promise<FirecrawlScrapeResponse> {\n    const startTime = Date.now();\n\n    log.info(\"Starting FireCrawl scrape\", { url });\n\n    try {\n      const response = await this.fetchWithRetry(url);\n      const durationMs = Date.now() - startTime;\n\n      log.info(\"FireCrawl scrape successful\", {\n        url,\n        durationMs,\n        contentLength: response.data.markdown?.length || 0,\n        hasMetadata: !!response.data.metadata,\n      });\n\n      return response;\n    } catch (error) {\n      const durationMs = Date.now() - startTime;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n\n      log.error(error, \"FireCrawl scrape failed\", {\n        url,\n        durationMs,\n        errorMessage,\n      });\n\n      throw new Error(`Failed to scrape URL ${url}: ${errorMessage}`);\n    }\n  }\n\n  /**\n   * Internal method with retry logic and exponential backoff\n   */\n  private async fetchWithRetry(\n    url: string,\n    attempt = 1\n  ): Promise<FirecrawlScrapeResponse> {\n    try {\n      // Set up timeout\n      const controller = new AbortController();\n      const timeoutId = setTimeout(\n        () => controller.abort(),\n        FIRECRAWL_CONFIG.timeout\n      );\n\n      // Make API request\n      const response = await fetch(`${FIRECRAWL_CONFIG.baseUrl}/scrape`, {\n        method: \"POST\",\n        headers: {\n          \"Authorization\": `Bearer ${this.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          url,\n          formats: FIRECRAWL_CONFIG.formats,\n          onlyMainContent: FIRECRAWL_CONFIG.onlyMainContent,\n        }),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeoutId);\n\n      // Handle non-200 responses\n      if (!response.ok) {\n        const errorText = await response.text();\n\n        // Check for rate limiting\n        if (response.status === 429) {\n          const retryAfter = response.headers.get('Retry-After');\n          throw new Error(\n            `Rate limited. Retry after: ${retryAfter || 'unknown'}`\n          );\n        }\n\n        throw new Error(\n          `FireCrawl API error (${response.status}): ${errorText}`\n        );\n      }\n\n      // Parse and validate response\n      const data = (await response.json()) as FirecrawlScrapeResponse;\n\n      if (!data.success) {\n        throw new Error(\"FireCrawl returned unsuccessful response\");\n      }\n\n      return data;\n\n    } catch (error) {\n      // Retry logic with exponential backoff\n      if (attempt < FIRECRAWL_CONFIG.retries.maxAttempts) {\n        const delay = this.calculateBackoffDelay(attempt);\n\n        log.warn(\"Retrying FireCrawl request\", {\n          url,\n          attempt,\n          nextAttempt: attempt + 1,\n          delayMs: delay,\n          errorMessage: error instanceof Error ? error.message : String(error),\n        });\n\n        // Wait before retry\n        await new Promise(resolve => setTimeout(resolve, delay));\n\n        // Recursive retry\n        return this.fetchWithRetry(url, attempt + 1);\n      }\n\n      // All retries exhausted\n      throw error;\n    }\n  }\n\n  /**\n   * Calculates exponential backoff delay with jitter\n   */\n  private calculateBackoffDelay(attempt: number): number {\n    const { baseDelayMs, maxDelayMs } = FIRECRAWL_CONFIG.retries;\n\n    // Exponential backoff: baseDelay * 2^(attempt - 1)\n    const exponentialDelay = baseDelayMs * Math.pow(2, attempt - 1);\n\n    // Add jitter (random 0-25% of delay)\n    const jitter = exponentialDelay * Math.random() * 0.25;\n\n    // Cap at maxDelay\n    return Math.min(exponentialDelay + jitter, maxDelayMs);\n  }\n}\n", "import type { ContentType } from \"../types/web-content.types\";\n\n/**\n * FireCrawl API configuration\n * Environment-specific settings for web content extraction\n */\nexport const FIRECRAWL_CONFIG = {\n  // API endpoint\n  baseUrl: \"https://api.firecrawl.dev/v1\",\n\n  // Request timeout (30 seconds)\n  timeout: 30000,\n\n  // Output formats to request\n  formats: [\"markdown\", \"html\"] as const,\n\n  // Extract main content only (skip nav, footer, ads)\n  onlyMainContent: true,\n\n  // Include page metadata\n  includeMetadata: true,\n\n  // Retry configuration\n  retries: {\n    maxAttempts: 3,\n    baseDelayMs: 1000,    // Start at 1 second\n    maxDelayMs: 10000,     // Cap at 10 seconds\n    exponentialBackoff: true,\n  },\n} as const;\n\n/**\n * Word count thresholds for content classification\n * Internal use only - used by getContentType()\n */\nconst CONTENT_TYPE_THRESHOLDS = {\n  SHORT_POST: 500,      // < 500 words = short post (tweets, reddit comments)\n  ARTICLE: 2000,        // 500-2000 words = article (blog posts)\n  LONG_FORM: Infinity,  // > 2000 words = long-form (essays, documentation)\n} as const;\n\n/**\n * Reading speed for time estimation (words per minute)\n * Internal use only - used by calculateReadingTime()\n */\nconst READING_SPEED_WPM = 200;\n\n/**\n * Determines content type based on word count\n * Used for prompt selection in summarization\n */\nexport function getContentType(wordCount: number): ContentType {\n  if (wordCount < CONTENT_TYPE_THRESHOLDS.SHORT_POST) {\n    return 'short_post';\n  }\n  if (wordCount < CONTENT_TYPE_THRESHOLDS.ARTICLE) {\n    return 'article';\n  }\n  return 'long_form';\n}\n\n/**\n * Calculates estimated reading time in minutes\n */\nexport function calculateReadingTime(wordCount: number): number {\n  return Math.ceil(wordCount / READING_SPEED_WPM);\n}\n", "import { SQLDatabase } from \"encore.dev/storage/sqldb\";\nimport { WebContent, ContentStatus } from \"../types\";\n\n/**\n * Repository for web_contents table operations\n * Follows the same pattern as TranscriptionRepository for consistency\n */\nexport class WebContentRepository {\n  constructor(private readonly db: SQLDatabase) {}\n\n  /**\n   * Creates a pending web content record for a bookmark\n   * Uses ON CONFLICT DO NOTHING for idempotency\n   */\n  async createPending(bookmarkId: number): Promise<WebContent> {\n    const row = await this.db.queryRow<WebContent>`\n      INSERT INTO web_contents (bookmark_id, status)\n      VALUES (${bookmarkId}, 'pending')\n      ON CONFLICT (bookmark_id) DO NOTHING\n      RETURNING *\n    `;\n\n    if (!row) {\n      // Check if it already exists\n      const existing = await this.findByBookmarkId(bookmarkId);\n      if (existing) {\n        return existing;\n      }\n      throw new Error(`Failed to create pending web content for bookmark ${bookmarkId}`);\n    }\n\n    return row;\n  }\n\n  /**\n   * Marks web content as processing\n   */\n  async markAsProcessing(bookmarkId: number): Promise<void> {\n    await this.db.exec`\n      UPDATE web_contents\n      SET\n        status = 'processing',\n        processing_started_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Updates web content with extracted data from FireCrawl\n   */\n  async updateContent(\n    bookmarkId: number,\n    data: {\n      raw_markdown: string;\n      raw_html: string;\n      page_title: string;\n      page_description: string;\n      language: string;\n      word_count: number;\n      char_count: number;\n      estimated_reading_minutes: number;\n      metadata: Record<string, any>;\n    }\n  ): Promise<void> {\n    await this.db.exec`\n      UPDATE web_contents\n      SET\n        raw_markdown = ${data.raw_markdown},\n        raw_html = ${data.raw_html},\n        page_title = ${data.page_title},\n        page_description = ${data.page_description},\n        language = ${data.language},\n        word_count = ${data.word_count},\n        char_count = ${data.char_count},\n        estimated_reading_minutes = ${data.estimated_reading_minutes},\n        metadata = ${data.metadata}\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Updates the summary field (from OpenAI)\n   */\n  async updateSummary(bookmarkId: number, summary: string): Promise<void> {\n    await this.db.exec`\n      UPDATE web_contents\n      SET summary = ${summary}\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Marks web content as completed\n   */\n  async markAsCompleted(bookmarkId: number): Promise<void> {\n    await this.db.exec`\n      UPDATE web_contents\n      SET\n        status = 'completed',\n        processing_completed_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Marks web content as failed with error message\n   */\n  async markAsFailed(bookmarkId: number, errorMessage: string): Promise<void> {\n    await this.db.exec`\n      UPDATE web_contents\n      SET\n        status = 'failed',\n        error_message = ${errorMessage},\n        processing_completed_at = NOW()\n      WHERE bookmark_id = ${bookmarkId}\n    `;\n  }\n\n  /**\n   * Finds web content by bookmark ID\n   */\n  async findByBookmarkId(bookmarkId: number): Promise<WebContent | null> {\n    return await this.db.queryRow<WebContent>`\n      SELECT * FROM web_contents\n      WHERE bookmark_id = ${bookmarkId}\n    ` || null;\n  }\n\n  /**\n   * Finds web content by ID\n   */\n  async findById(id: number): Promise<WebContent | null> {\n    return await this.db.queryRow<WebContent>`\n      SELECT * FROM web_contents\n      WHERE id = ${id}\n    ` || null;\n  }\n\n  /**\n   * Lists all web content with pagination\n   */\n  async list(params: {\n    limit: number;\n    offset: number;\n    status?: ContentStatus;\n  }): Promise<WebContent[]> {\n    const { limit, offset, status } = params;\n\n    const query = status\n      ? this.db.query<WebContent>`\n          SELECT * FROM web_contents\n          WHERE status = ${status}\n          ORDER BY created_at DESC\n          LIMIT ${limit} OFFSET ${offset}\n        `\n      : this.db.query<WebContent>`\n          SELECT * FROM web_contents\n          ORDER BY created_at DESC\n          LIMIT ${limit} OFFSET ${offset}\n        `;\n\n    const items: WebContent[] = [];\n    for await (const item of query) {\n      items.push(item);\n    }\n\n    return items;\n  }\n}\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport log from \"encore.dev/log\";\nimport { contentExtractedTopic, ContentExtractedEvent } from \"../events/content-extracted.events\";\nimport { OpenAIService } from \"../services/openai.service\";\nimport { WebContentRepository } from \"../repositories/web-content.repository\";\nimport { getContentType } from \"../config/firecrawl.config\";\nimport { BookmarkSource } from \"../types\";\nimport { db } from \"../db\";\nimport { secret } from \"encore.dev/config\";\n\n// Initialize dependencies\nconst openaiApiKey = secret(\"OpenAIAPIKey\");\nconst openaiService = new OpenAIService(openaiApiKey());\nconst webContentRepo = new WebContentRepository(db);\n\n/**\n * Content Summary Processor\n *\n * Responsibility:\n * - Receives content-extracted events\n * - Classifies content type (short_post, article, long_form)\n * - Generates AI summary using OpenAI\n * - Stores summary in web_contents table\n * - Marks content as completed\n *\n * Content Type Classification:\n * - short_post (<500 words): Brief summary, max 150 tokens\n * - article (500-2000 words): Standard summary, max 300 tokens\n * - long_form (>2000 words): Comprehensive summary, max 500 tokens\n *\n * Exported for testing purposes\n */\nexport async function handleContentSummary(\n  event: ContentExtractedEvent\n): Promise<void> {\n  const { bookmarkId, content, wordCount, source } = event;\n\n  try {\n    log.info(\"Starting content summarization\", {\n      bookmarkId,\n      wordCount,\n      source,\n    });\n\n    // Classify content type for appropriate summarization\n    const contentType = getContentType(wordCount);\n\n    log.info(\"Content type classified\", {\n      bookmarkId,\n      contentType,\n      wordCount,\n    });\n\n    // Select appropriate max tokens based on content type\n    const maxTokens = (() => {\n      switch (contentType) {\n        case 'short_post': return 150;\n        case 'article': return 300;\n        case 'long_form': return 500;\n      }\n    })();\n\n    // Generate summary using OpenAI with content-type-aware prompts\n    const summary = await openaiService.generateSummary(\n      content,\n      source as BookmarkSource,\n      { maxTokens, contentType }\n    );\n\n    log.info(\"Summary generated successfully\", {\n      bookmarkId,\n      summaryLength: summary.length,\n      contentType,\n    });\n\n    // Store summary\n    await webContentRepo.updateSummary(bookmarkId, summary);\n\n    // Mark as completed\n    await webContentRepo.markAsCompleted(bookmarkId);\n\n    log.info(\"Content summarization completed\", {\n      bookmarkId,\n      summaryLength: summary.length,\n    });\n\n  } catch (error) {\n    const errorMessage = error instanceof Error ? error.message : String(error);\n\n    log.error(error, \"Content summarization failed\", {\n      bookmarkId,\n      wordCount,\n      source,\n      errorMessage,\n    });\n\n    // Mark as failed\n    await webContentRepo.markAsFailed(\n      bookmarkId,\n      `Summarization failed: ${errorMessage}`\n    );\n  }\n}\n\n/**\n * Subscription to content-extracted topic\n * Processes all successfully extracted content for AI summarization\n */\nexport const contentSummarySubscription = new Subscription(\n  contentExtractedTopic,\n  \"content-summary-processor\",\n  {\n    handler: handleContentSummary,\n  }\n);\n", "import OpenAI from \"openai\";\nimport log from \"encore.dev/log\";\nimport { OPENAI_CONFIG } from \"../config/transcription.config\";\nimport { DAILY_DIGEST_CONFIG } from \"../config/daily-digest.config\";\nimport { BookmarkSource } from \"../types/domain.types\";\nimport { ContentType } from \"../types/web-content.types\";\nimport { SUMMARY_PROMPTS } from \"../config/prompts.config\";\n\n/**\n * Service for OpenAI summarization using Responses API\n */\nexport class OpenAIService {\n  private readonly client: OpenAI;\n\n  constructor(apiKey: string) {\n    this.client = new OpenAI({ apiKey });\n  }\n\n  /**\n   * Generates a summary using OpenAI Responses API\n   * Supports both legacy transcript summarization and new content-type-aware summarization\n   *\n   * @param content - The text to summarize (transcript, article, etc.)\n   * @param sourceOrInstructions - Either a BookmarkSource or custom instructions string (for backward compatibility)\n   * @param options - Configuration options (for content-type-aware summarization)\n   * @returns Summary text\n   * @throws Error if summarization fails\n   */\n  async generateSummary(\n    content: string,\n    sourceOrInstructions?: BookmarkSource | string,\n    options?: {\n      maxTokens?: number;\n      contentType?: ContentType;\n    }\n  ): Promise<string> {\n    // Backward compatibility: if sourceOrInstructions is a string or undefined, treat as custom instructions\n    const isLegacyMode = typeof sourceOrInstructions === \"string\" || sourceOrInstructions === undefined;\n\n    const instructions = isLegacyMode\n      ? (typeof sourceOrInstructions === \"string\" ? sourceOrInstructions : OPENAI_CONFIG.instructions)\n      : this.getInstructionsForSource(\n          sourceOrInstructions,\n          options?.contentType || \"article\"\n        );\n\n    const maxTokens = options?.maxTokens || OPENAI_CONFIG.maxOutputTokens;\n\n    log.info(\"Generating summary with OpenAI Responses API\", {\n      contentLength: content.length,\n      isLegacyMode,\n      source: isLegacyMode ? \"custom\" : sourceOrInstructions,\n      contentType: options?.contentType,\n      maxTokens,\n    });\n\n    try {\n      const response = await this.client.responses.create({\n        model: OPENAI_CONFIG.model,\n        instructions,\n        input: `Please provide a concise summary:\\n\\n${content}`,\n        temperature: OPENAI_CONFIG.temperature,\n        max_output_tokens: maxTokens,\n      });\n\n      const summary = response.output_text || \"No summary available\";\n\n      log.info(\"Summary generated successfully\", {\n        contentLength: content.length,\n        summaryLength: summary.length,\n        source: isLegacyMode ? \"custom\" : sourceOrInstructions,\n        contentType: options?.contentType,\n      });\n\n      return summary;\n    } catch (error) {\n      log.error(error, \"Failed to generate summary with OpenAI\", {\n        contentLength: content.length,\n        source: isLegacyMode ? \"custom\" : sourceOrInstructions,\n        errorMessage: error instanceof Error ? error.message : String(error),\n      });\n      throw new Error(\n        `OpenAI API error: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  }\n\n  /**\n   * Gets appropriate instructions based on source and content type\n   * Combines source-specific prompts with content-type-specific guidance\n   */\n  private getInstructionsForSource(\n    source: BookmarkSource,\n    contentType: ContentType\n  ): string {\n    // Get base instructions from source-specific prompts\n    let instructions = SUMMARY_PROMPTS[source] || SUMMARY_PROMPTS[BookmarkSource.OTHER];\n\n    // Add content-type-specific instructions\n    switch (contentType) {\n      case \"short_post\":\n        instructions += \"\\n\\nThis is a short post or social media content. Provide a very brief 1-2 sentence summary capturing the main point.\";\n        break;\n      case \"article\":\n        instructions += \"\\n\\nThis is a standard article. Provide a clear 2-3 paragraph summary highlighting the key points and takeaways.\";\n        break;\n      case \"long_form\":\n        instructions += \"\\n\\nThis is long-form content. Provide a comprehensive summary with main sections, key arguments, and conclusions.\";\n        break;\n    }\n\n    return instructions;\n  }\n\n  /**\n   * Generates a daily digest from prompt and content\n   * Used for Tier 1 (simple concatenation) and Tier 2 (reduce phase)\n   * @param prompt - The formatted prompt with instructions\n   * @param content - The content to process (summaries or intermediate text)\n   * @param maxTokens - Custom max output tokens (optional, defaults to config)\n   * @returns Digest text\n   * @throws Error if generation fails\n   */\n  async generateDigest(prompt: string, content: string, maxTokens?: number): Promise<string> {\n    const tokensToUse = maxTokens ?? DAILY_DIGEST_CONFIG.maxOutputTokens;\n\n    log.info(\"Generating daily digest with OpenAI Responses API\", {\n      promptLength: prompt.length,\n      contentLength: content.length,\n      maxTokens: tokensToUse,\n    });\n\n    try {\n      const response = await this.client.responses.create({\n        model: DAILY_DIGEST_CONFIG.openaiModel,\n        instructions: prompt,\n        input: content,\n        temperature: DAILY_DIGEST_CONFIG.temperature,\n        max_output_tokens: tokensToUse,\n      });\n\n      const digest = response.output_text || \"No digest generated\";\n\n      log.info(\"Daily digest generated successfully\", {\n        digestLength: digest.length,\n      });\n\n      return digest;\n    } catch (error) {\n      log.error(error, \"Failed to generate daily digest with OpenAI\");\n      throw new Error(\n        `OpenAI API error: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  }\n}\n", "import { Subscription } from \"encore.dev/pubsub\";\nimport { secret } from \"encore.dev/config\";\nimport log from \"encore.dev/log\";\nimport { db } from \"../db\";\nimport { audioTranscribedTopic } from \"../events/audio-transcribed.events\";\nimport { AudioTranscribedEvent } from \"../types\";\nimport { OpenAIService } from \"../services/openai.service\";\nimport { TranscriptionRepository } from \"../repositories/transcription.repository\";\nimport { SUMMARY_PROMPTS, DEFAULT_SUMMARY_PROMPT } from \"../config/prompts.config\";\nimport { BookmarkSource } from \"../types/domain.types\";\n\n// Secrets\nconst openaiApiKey = secret(\"OpenAIAPIKey\");\n\n// Initialize services\nconst openaiService = new OpenAIService(openaiApiKey());\nconst transcriptionRepo = new TranscriptionRepository(db);\n\n/**\n * Summary Generation Processor\n * Generates OpenAI summary with source-specific prompts\n * Independent: Uses source metadata to select appropriate prompt\n *\n * Exported for testing purposes\n */\nexport async function handleSummaryGeneration(event: AudioTranscribedEvent) {\n  const { bookmarkId, transcript, source } = event;\n\n  try {\n    // Select source-specific prompt\n    const prompt = SUMMARY_PROMPTS[source as BookmarkSource] || DEFAULT_SUMMARY_PROMPT;\n\n    log.info(\"Starting summary generation\", {\n      bookmarkId,\n      source,\n      transcriptLength: transcript.length,\n      usingPrompt: source,\n    });\n\n    // Generate summary using OpenAI with source-specific prompt\n    const summary = await openaiService.generateSummary(transcript, prompt);\n\n    log.info(\"Summary generated\", {\n      bookmarkId,\n      source,\n      summaryLength: summary.length,\n    });\n\n    // Store summary and mark as completed\n    await transcriptionRepo.updateSummary(bookmarkId, summary);\n\n    log.info(\"Summary generation completed, transcription marked as completed\", {\n      bookmarkId,\n      source,\n    });\n  } catch (error) {\n    const errorMessage =\n      error instanceof Error ? error.message : String(error);\n\n    log.error(error, \"Summary generation failed\", {\n      bookmarkId,\n      source,\n      errorMessage,\n    });\n\n    // Mark as failed\n    await transcriptionRepo.markAsFailed(\n      bookmarkId,\n      `Summary generation failed: ${errorMessage}`\n    );\n  }\n}\n\n// Subscription to audio-transcribed topic\nexport const summaryGenerationSubscription = new Subscription(\n  audioTranscribedTopic,\n  \"summary-generation-processor\",\n  {\n    handler: handleSummaryGeneration,\n  }\n);\n", "import { Service } from \"encore.dev/service\";\n\n// Import processors to register subscriptions (decoupled event-driven pipeline)\nimport \"./processors/bookmark-classification.processor\"; // Classify bookmark URL and update source\nimport \"./processors/audio-download.processor\"; // Unified audio download (YouTube, Podcast, etc.)\nimport \"./processors/audio-transcription.processor\"; // Deepgram transcription (source-agnostic)\nimport \"./processors/summary-generation.processor\"; // OpenAI summary (source-aware prompts)\nimport \"./processors/content-extraction.processor\"; // FireCrawl web content extraction (textual sources)\nimport \"./processors/content-summary.processor\"; // Web content AI summarization (OpenAI)\n\n// Import cron jobs to register scheduled tasks\nimport \"./cron/daily-digest.cron\"; // Generate daily digest at 9 PM GMT\n\nexport default new Service(\"bookmarks\");\n", "import { CronJob } from \"encore.dev/cron\";\nimport { generateYesterdaysDigest } from \"../api\";\n\n/**\n * Daily Digest Cron Job\n *\n * Runs daily at 9 PM GMT to generate per-user digests of all bookmarks\n * from the previous day.\n *\n * Schedule: 9 PM GMT every day (21:00 UTC)\n * Endpoint: generateYesterdaysDigest API (no parameters required)\n *\n * This cron job will:\n * 1. Fetch all active user IDs from the users service\n * 2. For each user:\n *    a. Fetch all completed transcriptions from yesterday\n *    b. Calculate metadata (bookmark count, sources breakdown, total duration)\n *    c. Generate a unified summary using map-reduce OpenAI strategy\n *    d. Store the completed digest in the database\n * 3. Log comprehensive results (success rate, errors, etc.)\n *\n * Note: If one user's digest fails, the cron continues processing other users\n */\nconst _ = new CronJob(\"daily-digest-generator\", {\n  title: \"Generate Daily Digest\",\n  schedule: \"0 21 * * *\", // 9 PM GMT every day\n  endpoint: generateYesterdaysDigest,\n});\n", "import { Service } from \"encore.dev/service\";\n\n// Import endpoints to register them with Encore\nimport \"./api\";\nimport \"./auth\";\nimport \"./webhooks\";\n\n/**\n * Users service handles authentication and user management\n *\n * Provides:\n * - User signup and login\n * - JWT-based authentication\n * - Auth handler for protected endpoints\n * - User profile management\n * - Supabase auth webhooks\n */\nexport default new Service(\"users\");\n"],
  "mappings": ";;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACsFO,SAAS,mBAAmB,eAIjC;AACA,QAAM,MAAM,iBAAiB,oBAAI,KAAK;AAGtC,QAAM,aAAa,IAAI,KAAK,GAAG;AAC/B,aAAW,QAAQ,WAAW,QAAQ,IAAI,CAAC;AAC3C,aAAW,SAAS,GAAG,GAAG,GAAG,CAAC;AAG9B,QAAM,YAAY,IAAI,KAAK,UAAU;AAGrC,QAAM,UAAU,IAAI,KAAK,UAAU;AACnC,UAAQ,SAAS,IAAI,IAAI,IAAI,GAAG;AAEhC,SAAO,EAAE,WAAW,SAAS,WAAW;AAC1C;AAOO,SAAS,iBAAiB,MAAoB;AACnD,SAAO,KAAK,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AACxC;AAOO,SAAS,gBAAgB,YAA0B;AACxD,QAAM,OAAO,IAAI,KAAK,UAAU;AAChC,OAAK,SAAS,GAAG,GAAG,GAAG,CAAC;AACxB,SAAO;AACT;AA9HA,IAIa;AAJb;AAAA;AAAA;AAIO,IAAM,sBAAsB;AAAA;AAAA;AAAA;AAAA;AAAA,MAMjC,UAAU;AAAA;AAAA,MAGV,cAAc;AAAA;AAAA,MAGd,eAAe;AAAA;AAAA;AAAA;AAAA;AAAA,MAOf,YAAY;AAAA;AAAA,MAGZ,gBAAgB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAOhB,aAAa;AAAA;AAAA,MAGb,iBAAiB;AAAA;AAAA,MAGjB,aAAa;AAAA;AAAA;AAAA;AAAA;AAAA,MAOb,mBAAmB;AAAA;AAAA;AAAA;AAAA;AAAA,MAOnB,gBAAgB;AAAA;AAAA,MAGhB,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAmBhB;AAAA;AAAA;;;ACgFO,SAAS,iBAAiB,QAAgC;AAC/D,QAAM,cAA8C;AAAA,IAClD,wBAAuB,GAAG;AAAA,IAC1B,wBAAuB,GAAG;AAAA,IAC1B,sBAAsB,GAAG;AAAA,IACzB,wBAAuB,GAAG;AAAA,IAC1B,0BAAwB,GAAG;AAAA,IAC3B,kBAAoB,GAAG;AAAA,IACvB,gBAAmB,GAAG;AAAA,IACtB,oBAAqB,GAAG;AAAA,EAC1B;AAEA,SAAO,YAAY,MAAM,KAAK;AAChC;AAxKA,IAMa,iBAgEA,wBAUA,uBA+BA,wBAoBA;AAnIb;AAAA;AAAA;AAAA;AAMO,IAAM,kBAAkD;AAAA,MAC7D,wBAAuB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAO1B,wBAAuB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQ1B,sBAAsB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQzB,wBAAuB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQ1B,0BAAwB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQ3B,kBAAoB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQvB,gBAAmB,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQtB,oBAAqB,GAAG;AAAA;AAAA;AAAA,IAG1B;AAKO,IAAM,yBAAyB,mCAAoC;AAUnE,IAAM,wBAAwB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA+B9B,IAAM,yBAAyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAoB/B,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACzHjC,SAAS,mBAAmB,MAAsB;AACvD,MAAI,CAAC;AAAM,WAAO;AAElB,SAAO,KAAK,KAAK,KAAK,SAAS,CAAC;AAClC;AAwDO,SAAS,eACd,WACA,mBACY;AACZ,MAAI,UAAU,WAAW;AAAG,WAAO,CAAC;AAEpC,QAAM,UAAsB,CAAC;AAC7B,MAAI,eAAyB,CAAC;AAC9B,MAAI,qBAAqB;AAEzB,aAAW,WAAW,WAAW;AAC/B,UAAM,gBAAgB,mBAAmB,OAAO;AAGhD,QACE,qBAAqB,gBAAgB,qBACrC,aAAa,SAAS,GACtB;AACA,cAAQ,KAAK,YAAY;AACzB,qBAAe,CAAC;AAChB,2BAAqB;AAAA,IACvB;AAEA,iBAAa,KAAK,OAAO;AACzB,0BAAsB;AAAA,EACxB;AAGA,MAAI,aAAa,SAAS,GAAG;AAC3B,YAAQ,KAAK,YAAY;AAAA,EAC3B;AAEA,SAAO;AACT;AAvGA;AAAA;AAAA;AAAA;AAAA;;;ACAA;AAAA;AAAA;AAAA;AAAA,OAAOA,UAAS;AAChB,SAAS,kBAAkB;AAgD3B,SAAS,QAAQ,OAAe,UAA0B;AACxD,QAAM,OAAO,OAAO,KAAK,EAAE,YAAY,KAAK,SAAS,KAAK,EAAE,YAAY;AACxE,QAAM,OAAO,KACV,QAAQ,iBAAiB,EAAE,EAC3B,QAAQ,QAAQ,GAAG,EACnB,QAAQ,OAAO,GAAG,EAClB,QAAQ,UAAU,EAAE;AACvB,SAAO,QAAQ;AACjB;AAEA,SAAS,cAAc,MAAsC;AAC3D,MAAI,CAAC;AAAM,WAAO,CAAC;AACnB,SAAO,MAAM;AAAA,IACX,IAAI;AAAA,MACF,KACG;AAAA,QAAQ,CAAC,QACR,IACG,MAAM,QAAQ,EACd,IAAI,CAAC,MAAM,EAAE,KAAK,EAAE,YAAY,CAAC,EACjC,OAAO,OAAO;AAAA,MACnB;AAAA,IACJ;AAAA,EACF;AACF;AAEA,SAAS,YAAY,GAAW,GAAmB;AACjD,QAAM,WAAW,CAAC,UAChB,MACG,YAAY,EACZ,MAAM,YAAY,EAClB,OAAO,CAAC,UAAU,MAAM,SAAS,CAAC;AAEvC,QAAM,OAAO,IAAI,IAAI,SAAS,CAAC,CAAC;AAChC,QAAM,OAAO,IAAI,IAAI,SAAS,CAAC,CAAC;AAChC,MAAI,CAAC,KAAK,QAAQ,CAAC,KAAK;AAAM,WAAO;AACrC,MAAI,eAAe;AACnB,OAAK,QAAQ,CAAC,UAAU;AACtB,QAAI,KAAK,IAAI,KAAK;AAAG,sBAAgB;AAAA,EACvC,CAAC;AACD,SAAO,eAAe,KAAK,IAAI,KAAK,MAAM,KAAK,IAAI;AACrD;AAEA,SAAS,mBAAmB,MAAmB;AAC7C,QAAM,UAAU,KAAK,KAAK;AAC1B,MAAI;AACF,WAAO,KAAK,MAAM,OAAO;AAAA,EAC3B,QAAQ;AAEN,UAAM,QAAQ,QAAQ,MAAM,+BAA+B;AAC3D,QAAI,OAAO;AACT,aAAO,mBAAmB,MAAM,CAAC,CAAC;AAAA,IACpC;AAEA,UAAM,eAAe,QAAQ,QAAQ,GAAG;AACxC,UAAM,aAAa,QAAQ,QAAQ,GAAG;AACtC,UAAM,QAAQ,iBAAiB,KAAK,eAAe;AACnD,QAAI,UAAU,IAAI;AAChB,YAAM,UAAU,QAAQ,MAAM,KAAK;AACnC,UAAI;AACF,eAAO,KAAK,MAAM,OAAO;AAAA,MAC3B,QAAQ;AAAA,MAER;AAAA,IACF;AACA,UAAM,IAAI,MAAM,gDAAgD;AAAA,EAClE;AACF;AAMA,SAAS,+BACP,OACA,YACQ;AACR,SAAO,MAAM,IAAI,CAAC,MAAM,QAAQ;AAC9B,UAAM,aAAa,iBAAiB,KAAK,MAAM;AAC/C,UAAM,cAAc,KAAK,iBAAiB,UAAU,aAAa;AAEjE,UAAM,QAAQ,KAAK,SAAS;AAC5B,UAAM,YAAY,KAAK,aACnB,IAAI,KAAK,KAAK,UAAU,EAAE,YAAY,IACtC;AACJ,UAAM,aAAa,aAAa,MAAM;AAEtC,UAAM,cACJ,KAAK,iBAAiB,WAAW,KAAK,WAClC,GAAG,KAAK,IAAI,GAAG,KAAK,MAAM,KAAK,WAAW,EAAE,CAAC,CAAC,iBAC9C;AAEN,UAAM,aACJ,KAAK,iBAAiB,aAAa,KAAK,kBACpC,GAAG,KAAK,eAAe,cACvB;AAEN,UAAM,eAAe,KAAK,YACtB,SAAS,KAAK,SAAS,KACvB;AAEJ,UAAM,YAAY;AAAA,MAChB;AAAA,MACA;AAAA,MACA;AAAA,IACF,EAAE,OAAO,OAAO;AAEhB,UAAM,eAAe,UAAU,SAC3B,SAAS,UAAU,KAAK,KAAK,CAAC,KAC9B;AAEJ,WAAO,SAAS,UAAU;AAAA,QACtB,WAAW;AAAA,SACV,KAAK;AAAA,UACJ,UAAU;AAAA,YACR,SAAS;AAAA,EACnB,eAAe,eAAe,OAAO,EAAE;AAAA,EACvC,KAAK,OAAO;AAAA;AAAA,EAEZ,CAAC,EAAE,KAAK,MAAM;AAChB;AAxKA,IA8Ka;AA9Kb;AAAA;AAAA;AAKA;AACA;AAMA;AAkKO,IAAM,yBAAN,MAA6B;AAAA,MACjB;AAAA,MAEjB,YAAYC,eAAsB;AAChC,aAAK,MAAM,IAAI,WAAW;AAAA,UACxB,OAAO,oBAAoB;AAAA,UAC3B,aAAa,oBAAoB;AAAA,UACjC,WAAW,oBAAoB;AAAA,UAC/B,QAAQA;AAAA,QACV,CAAC;AAAA,MACH;AAAA,MAEQ,aAAa,OAAkC;AACrD,cAAM,WAA2B,CAAC;AAClC,cAAM,UAAU,oBAAI,IAA0B;AAE9C,mBAAW,QAAQ,OAAO;AACxB,gBAAM,iBAAiB,KAAK,YAAY,QAAQ,KAAK,YAAY,SAAS;AAC1E,cAAI,UAAU,QAAQ,IAAI,cAAc;AAExC,cAAI,CAAC,SAAS;AACZ,sBAAU,KAAK,mBAAmB,gBAAgB,MAAM,QAAQ;AAChE,gBAAI,CAAC,SAAS;AACZ,wBAAU;AAAA,gBACR,MAAM;AAAA,gBACN,OAAO,CAAC;AAAA,gBACR,MAAM,oBAAI,IAAY;AAAA,cACxB;AACA,uBAAS,KAAK,OAAO;AAAA,YACvB;AACA,oBAAQ,IAAI,gBAAgB,OAAO;AAAA,UACrC;AAEA,kBAAQ,MAAM,KAAK,IAAI;AACvB,eAAK,KAAK,QAAQ,CAAC,QAAQ,QAAS,KAAK,IAAI,GAAG,CAAC;AAAA,QACnD;AAEA,eAAO;AAAA,MACT;AAAA,MAEQ,mBACN,MACA,MACA,UAC0B;AAC1B,cAAM,WAAW,IAAI,IAAI,KAAK,IAAI;AAGlC,cAAM,cAAc,SAAS,KAAK,CAAC,YAAY,QAAQ,SAAS,IAAI;AACpE,YAAI,aAAa;AACf,iBAAO;AAAA,QACT;AAEA,YAAI;AAEJ,mBAAW,WAAW,UAAU;AAC9B,gBAAM,cAAc,QAAQ;AAC5B,cAAI,UAAU;AACd,mBAAS,QAAQ,CAAC,QAAQ;AACxB,gBAAI,YAAY,IAAI,GAAG;AAAG,yBAAW;AAAA,UACvC,CAAC;AAED,gBAAM,WACJ,SAAS,QAAQ,YAAY,OACzB,UAAU,KAAK,IAAI,SAAS,MAAM,YAAY,IAAI,IAClD;AAEN,gBAAM,aAAa;AAAA,YACjB,KAAK;AAAA,YACL,QAAQ,MAAM,CAAC,GAAG,cAAc;AAAA,UAClC;AAEA,gBAAM,gBAAgB,KAAK,IAAI,UAAU,UAAU;AAEnD,cAAI,iBAAiB,KAAK;AACxB,gBAAI,CAAC,aAAa,gBAAgB,UAAU,OAAO;AACjD,0BAAY,EAAE,SAAS,OAAO,cAAc;AAAA,YAC9C;AAAA,UACF;AAAA,QACF;AAEA,eAAO,WAAW;AAAA,MACpB;AAAA,MAEA,MAAc,kBACZ,UAC2B;AAC3B,cAAM,YAA8B,CAAC;AAErC,mBAAW,CAAC,OAAO,OAAO,KAAK,SAAS,QAAQ,GAAG;AACjD,gBAAM,kBAAkB,MAAM;AAAA,YAC5B,IAAI,IAAI,QAAQ,MAAM,IAAI,CAAC,SAAS,KAAK,UAAU,EAAE,OAAO,OAAO,CAAC;AAAA,UACtE;AAEA,gBAAM,eAAe,QAAQ,MAC1B,IAAI,CAAC,SAAS;AACb,kBAAM,QAAQ,KAAK,SAAS,IAAI,CAAC,SAAS,KAAK,IAAI,EAAE,EAAE,KAAK,IAAI;AAChE,kBAAM,OAAO,KAAK,KAAK,KAAK,IAAI,KAAK;AACrC,mBAAO,QAAQ,KAAK,UAAU;AAAA,WAC7B,KAAK,UAAU;AAAA,aACb,KAAK,kBAAkB;AAAA;AAAA,EAElC,QAAQ,QAAQ,kBAAkB;AAAA,aACvB,KAAK,qBAAqB;AAAA,aAC1B,KAAK,OAAO;AAAA,UACf,IAAI;AAAA,YACF,KAAK,WAAW;AAAA,UACpB,CAAC,EACA,KAAK,MAAM;AAEd,gBAAM,SAAS,uBAAuB;AAAA,YACpC;AAAA,YACA,QAAQ;AAAA,UACV,EACG,QAAQ,sBAAsB,gBAAgB,KAAK,KAAK,KAAK,QAAQ,IAAI,EACzE,QAAQ,kBAAkB,MAAM,KAAK,QAAQ,IAAI,EAAE,KAAK,IAAI,KAAK,SAAS,EAC1E,QAAQ,mBAAmB,YAAY;AAE1C,gBAAM,WAAW,MAAM,KAAK,IAAI,OAAO,MAAM;AAC7C,gBAAM,UAAU,mBAAmB,SAAS,QAAQ,SAAS,CAAC;AAE9D,cAAI,CAAC,SAAS,iBAAiB,CAAC,SAAS,qBAAqB;AAC5D,kBAAM,IAAI;AAAA,cACR,2CAA2C,QAAQ,IAAI;AAAA,YACzD;AAAA,UACF;AAEA,gBAAM,eAAe,MAAM,QAAQ,QAAQ,aAAa,IACpD,QAAQ,cAAc,IAAI,CAAC,MAAW,EAAE,SAAS,CAAC,IAClD,CAAC;AAEL,oBAAU,KAAK;AAAA,YACb,MAAM,QAAQ;AAAA,YACd,OAAO,QAAQ,cAAc,SAAS;AAAA,YACtC,WAAW,QAAQ,oBAAoB,SAAS;AAAA,YAChD;AAAA,YACA,iBAAiB,QAAQ,mBAAmB,IAAI,SAAS;AAAA,YACzD,MAAM,MAAM,KAAK,QAAQ,IAAI;AAAA,UAC/B,CAAC;AAAA,QACH;AAEA,eAAO;AAAA,MACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQA,MAAM,eACJ,cACA,SACiB;AACjB,cAAM,aAAa,aAAa,OAAO,OAAK,EAAE,iBAAiB,OAAO,EAAE;AACxE,cAAM,eAAe,aAAa,OAAO,OAAK,EAAE,iBAAiB,SAAS,EAAE;AAC5E,cAAM,aAAa,aAAa;AAEhC,QAAAD,KAAI,KAAK,yCAAyC;AAAA,UAChD,kBAAkB;AAAA,UAClB;AAAA,UACA;AAAA,QACF,CAAC;AAED,YAAI;AAEF,gBAAM,YAAY,aAAa,IAAI,CAAC,SAAS,KAAK,WAAW,sBAAsB;AAEnF,UAAAA,KAAI,KAAK,qCAAqC;AAAA,YAC5C,cAAc,UAAU;AAAA,YACxB,qBAAqB,UAAU;AAAA,cAC7B,CAAC,KAAK,MAAM,MAAM,mBAAmB,CAAC;AAAA,cACtC;AAAA,YACF;AAAA,UACF,CAAC;AAGD,gBAAM,UAAU;AAAA,YACd;AAAA,YACA,oBAAoB;AAAA,UACtB;AAEA,UAAAA,KAAI,KAAK,iCAAiC;AAAA,YACxC,YAAY,QAAQ;AAAA,YACpB,cAAc,KAAK;AAAA,cACjB,QAAQ,OAAO,CAAC,KAAK,MAAM,MAAM,EAAE,QAAQ,CAAC,IAAI,QAAQ;AAAA,YAC1D;AAAA,UACF,CAAC;AAGD,gBAAM,WAAW,MAAM,KAAK,SAAS,SAAS,YAAY;AAE1D,UAAAA,KAAI,KAAK,uBAAuB;AAAA,YAC9B,WAAW,SAAS;AAAA,UACtB,CAAC;AAGD,gBAAM,WAAW,KAAK,aAAa,QAAQ;AAE3C,UAAAA,KAAI,KAAK,wBAAwB;AAAA,YAC/B,cAAc,SAAS;AAAA,YACvB,cAAc,SAAS,IAAI,CAAC,MAAM,EAAE,IAAI;AAAA,UAC1C,CAAC;AAGD,gBAAM,mBAAmB,MAAM,KAAK,kBAAkB,QAAQ;AAE9D,UAAAA,KAAI,KAAK,mCAAmC;AAAA,YAC1C,qBAAqB,iBAAiB;AAAA,UACxC,CAAC;AAGD,gBAAM,cAAc,MAAM,KAAK,YAAY,kBAAkB;AAAA,YAC3D,YAAY,SAAS;AAAA,YACrB,YAAY,SAAS,cAAc;AAAA,YACnC,YAAY,SAAS,cAAc;AAAA,YACnC,cAAc,SAAS,gBAAgB;AAAA,UACzC,CAAC;AAED,UAAAA,KAAI,KAAK,0CAA0C;AAAA,YACjD,mBAAmB,YAAY;AAAA,UACjC,CAAC;AAED,iBAAO;AAAA,QACT,SAAS,OAAO;AACd,UAAAA,KAAI,MAAM,OAAO,qCAAqC;AACtD,gBAAM,IAAI;AAAA,YACR,wCAAwC,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,UAChG;AAAA,QACF;AAAA,MACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAQA,MAAc,SACZ,SACA,cACoB;AACpB,QAAAA,KAAI,KAAK,sBAAsB,EAAE,YAAY,QAAQ,OAAO,CAAC;AAE7D,cAAM,QAAmB,CAAC;AAC1B,YAAI,eAAe;AAEnB,iBAAS,aAAa,GAAG,aAAa,QAAQ,QAAQ,cAAc;AAClE,gBAAM,QAAQ,QAAQ,UAAU;AAChC,gBAAM,oBAAoB,aAAa;AAAA,YACrC;AAAA,YACA,eAAe,MAAM;AAAA,UACvB;AAEA,UAAAA,KAAI,KAAK,oBAAoB;AAAA,YAC3B;AAAA,YACA,WAAW,MAAM;AAAA,YACjB,YAAY;AAAA,UACd,CAAC;AAED,gBAAM,iBAAiB;AAAA,YACrB;AAAA,YACA;AAAA,UACF;AAEA,gBAAM,SAAS,sBAAsB;AAAA,YACnC;AAAA,YACA;AAAA,UACF;AAEA,gBAAM,WAAW,MAAM,KAAK,IAAI,OAAO,MAAM;AAC7C,gBAAM,UAAU,mBAAmB,SAAS,QAAQ,SAAS,CAAC;AAE9D,cAAI,CAAC,MAAM,QAAQ,OAAO,GAAG;AAC3B,kBAAM,IAAI,MAAM,qCAAqC;AAAA,UACvD;AAEA,cAAI,QAAQ,WAAW,MAAM,QAAQ;AACnC,YAAAA,KAAI,KAAK,wDAAwD;AAAA,cAC/D,UAAU,MAAM;AAAA,cAChB,UAAU,QAAQ;AAAA,YACpB,CAAC;AAAA,UACH;AAEA,kBAAQ,QAAQ,CAAC,SAAc,QAAgB;AAC7C,kBAAM,aACJ,OAAO,SAAS,gBAAgB,WAC5B,QAAQ,cACR,eAAe,MAAM;AAE3B,kBAAM,WAAW;AAAA,cACf,SAAS,aAAa;AAAA,cACtB,SAAS,eAAe;AAAA,YAC1B;AAEA,kBAAM,OAAO,cAAc,SAAS,IAAI;AAExC,kBAAM,KAAK;AAAA,cACT;AAAA,cACA,UAAU;AAAA,cACV,cAAc,SAAS,aAAa,IAAI,SAAS;AAAA,cACjD,aAAa,SAAS,eAAe,IAAI,SAAS;AAAA,cAClD,qBAAqB,SAAS,wBAAwB,IAAI,SAAS;AAAA,cACnE,UACE,MAAM,QAAQ,SAAS,SAAS,KAAK,QAAQ,UAAU,SACnD,QAAQ,UAAU,IAAI,CAAC,MAAW,EAAE,SAAS,CAAC,IAC9C,CAAC;AAAA,cACP,wBACE,SAAS,2BAA2B,IACpC,SAAS;AAAA,cACX,UAAU,SAAS,WAAW,IAAI,SAAS;AAAA,cAC3C;AAAA,cACA,cAAc,SAAS,gBAAgB,IAAI,SAAS;AAAA,YACtD,CAAC;AAAA,UACH,CAAC;AAED,0BAAgB,MAAM;AAAA,QACxB;AAEA,eAAO;AAAA,MACT;AAAA,MAEA,MAAc,YACZ,kBACA,SACiB;AACjB,QAAAA,KAAI,KAAK,yBAAyB;AAAA,UAChC,cAAc,iBAAiB;AAAA,QACjC,CAAC;AAED,YAAI,iBAAiB,WAAW,GAAG;AACjC,gBAAM,IAAI,MAAM,8CAA8C;AAAA,QAChE;AAEA,cAAM,gBAAgB,iBACnB,IAAI,CAAC,SAAS,QAAQ;AACrB,gBAAM,YAAY,QAAQ,aACvB,IAAI,CAAC,MAAM,KAAK,CAAC,EAAE,EACnB,KAAK,IAAI;AACZ,iBAAO,WAAW,MAAM,CAAC,KAAK,QAAQ,IAAI;AAAA,SACzC,QAAQ,KAAK;AAAA,aACT,QAAQ,SAAS;AAAA;AAAA,EAE5B,aAAa,GAAG;AAAA,UACR,QAAQ,cAAc;AAAA,QACxB,QAAQ,KAAK,KAAK,IAAI,KAAK,SAAS;AAAA,QACtC,CAAC,EACA,KAAK,MAAM;AAEd,cAAM,eAAe,yBAClB,QAAQ,oBAAoB,aAAa,EACzC,QAAQ,iBAAiB,QAAQ,cAAc,OAAO,EACtD;AAAA,UACC;AAAA,UACA,OAAO,QAAQ,cAAc,iBAAiB,MAAM;AAAA,QACtD,EACC,QAAQ,iBAAiB,OAAO,QAAQ,cAAc,CAAC,CAAC,EACxD,QAAQ,mBAAmB,OAAO,QAAQ,gBAAgB,CAAC,CAAC;AAE/D,cAAM,SAAS,MAAM,KAAK,IAAI,OAAO,YAAY;AACjD,eAAO,OAAO,QAAQ,SAAS;AAAA,MACjC;AAAA,IACF;AAAA;AAAA;;;ACxhBA;AAAA;AAAA;AAAA,oBAAAE;AAAA,EAAA;AAAA,4BAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAAS,SAAS,UAAU,WAAW,mBAAmB;AAM1D,eAAsBD,YAAW,QAAQ,MAAM;AAC3C,MAAI,OAA6E;AAC7E,WAAO,eAAe,WAAW,QAAQ,IAAI;AAAA,EACjD;AAEA,SAAO,QAAQ,aAAa,cAAc,QAAQ,IAAI;AAC1D;AACA,eAAsBC,oBAAmB,QAAQ,MAAM;AACnD,MAAI,OAA6E;AAC7E,WAAO,eAAe,mBAAmB,QAAQ,IAAI;AAAA,EACzD;AAEA,SAAO,QAAQ,aAAa,sBAAsB,QAAQ,IAAI;AAClE;AACA,eAAsB,OAAO,QAAQ,MAAM;AACvC,MAAI,OAA6E;AAC7E,WAAO,eAAe,OAAO,QAAQ,IAAI;AAAA,EAC7C;AAEA,SAAO,QAAQ,aAAa,UAAU,QAAQ,IAAI;AACtD;AACA,eAAsB,IAAI,QAAQ,MAAM;AACpC,MAAI,OAA6E;AAC7E,WAAO,eAAe,IAAI,QAAQ,IAAI;AAAA,EAC1C;AAEA,SAAO,QAAQ,aAAa,OAAO,QAAQ,IAAI;AACnD;AACA,eAAsB,KAAK,QAAQ,MAAM;AACrC,MAAI,OAA6E;AAC7E,WAAO,eAAe,KAAK,QAAQ,IAAI;AAAA,EAC3C;AAEA,SAAO,QAAQ,aAAa,QAAQ,QAAQ,IAAI;AACpD;AACA,eAAsB,OAAO,QAAQ,MAAM;AACvC,MAAI,OAA6E;AAC7E,WAAO,eAAe,OAAO,QAAQ,IAAI;AAAA,EAC7C;AAEA,SAAO,QAAQ,aAAa,UAAU,QAAQ,IAAI;AACtD;AACA,eAAsB,OAAO,QAAQ,MAAM;AACvC,MAAI,OAA6E;AAC7E,WAAO,eAAe,OAAO,QAAQ,IAAI;AAAA,EAC7C;AAEA,SAAO,QAAQ,aAAa,UAAU,QAAQ,IAAI;AACtD;AACA,eAAsB,WAAW,QAAQ,MAAM;AAC3C,MAAI,OAA6E;AAC7E,WAAO,eAAe,WAAW,QAAQ,IAAI;AAAA,EACjD;AAEA,SAAO,QAAQ,aAAa,cAAc,QAAQ,IAAI;AAC1D;AACA,eAAsB,oBAAoB,QAAQ,MAAM;AACpD,MAAI,OAA6E;AAC7E,WAAO,eAAe,oBAAoB,QAAQ,IAAI;AAAA,EAC1D;AAEA,SAAO,QAAQ,aAAa,uBAAuB,QAAQ,IAAI;AACnE;AACA,eAAsB,eAAe,QAAQ,MAAM;AAC/C,MAAI,OAA6E;AAC7E,WAAO,eAAe,eAAe,QAAQ,IAAI;AAAA,EACrD;AAEA,SAAO,QAAQ,aAAa,kBAAkB,QAAQ,IAAI;AAC9D;AACA,eAAsB,iBAAiB,QAAQ,MAAM;AACjD,MAAI,OAA6E;AAC7E,WAAO,eAAe,iBAAiB,QAAQ,IAAI;AAAA,EACvD;AAEA,SAAO,QAAQ,aAAa,oBAAoB,QAAQ,IAAI;AAChE;AACA,eAAsB,yBAAyB,MAAM;AACjD,QAAM,SAAS;AACf,MAAI,OAA6E;AAC7E,WAAO,eAAe,yBAAyB,QAAQ,IAAI;AAAA,EAC/D;AAEA,SAAO,QAAQ,aAAa,4BAA4B,QAAQ,IAAI;AACxE;AA1FA,IAEM;AAFN;AAAA;AAAA;AAEA,IAAM,iBAAiB,QACjB,MAAa,OACb;AAAA;AAAA;;;ACJN,IAAAC,qBAAA;AAAA,SAAAA,oBAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAAS,WAAAC,UAAS,YAAAC,WAAU,aAAAC,YAAW,eAAAC,oBAAmB;AAM1D,eAAsB,GAAG,MAAM;AAC3B,QAAM,SAAS;AACf,MAAI,OAA6E;AAC7E,WAAOC,gBAAe,GAAG,QAAQ,IAAI;AAAA,EACzC;AAEA,SAAOJ,SAAQ,SAAS,MAAM,QAAQ,IAAI;AAC9C;AACA,eAAsB,cAAc,QAAQ,MAAM;AAC9C,MAAI,OAA6E;AAC7E,WAAOI,gBAAe,cAAc,QAAQ,IAAI;AAAA,EACpD;AAEA,SAAOJ,SAAQ,SAAS,iBAAiB,QAAQ,IAAI;AACzD;AACA,eAAsB,WAAW,MAAM;AACnC,QAAM,SAAS;AACf,MAAI,OAA6E;AAC7E,WAAOI,gBAAe,WAAW,QAAQ,IAAI;AAAA,EACjD;AAEA,SAAOJ,SAAQ,SAAS,cAAc,QAAQ,IAAI;AACtD;AACA,eAAsB,YAAY,QAAQ,MAAM;AAC5C,MAAI,OAA6E;AAC7E,WAAOI,gBAAe,YAAY,QAAQ,IAAI;AAAA,EAClD;AAEA,SAAOJ,SAAQ,SAAS,eAAe,QAAQ,IAAI;AACvD;AAnCA,IAEMI;AAFN,IAAAC,kBAAA;AAAA;AAAA;AAEA,IAAMD,kBAAiB,QACjB,MAAa,OACb;AAAA;AAAA;;;ACJN;AAAA;AAAA;AAAA,eAAAE;AAAA;AAAA;AAAA;AAAA;AAAA;AACA,UAAAC;AAAA;AAAA;;;ACDA,SAAS,kBAAkB,kBAAkB,WAAyB;;;ACAtE,SAAiB,SAAS,gBAAgB;AAC1C,SAAS,mBAAmB;AAC5B,SAAS,WAAW,oBAAoB,iBAAiB;AACzD,OAAO,SAAS;AAChB,SAAS,eAAe;;;ACJxB,SAAS,cAAc;AAoBvB,IAAM,cAAc,OAAO,aAAa;AAGxC,IAAM,kBAAkB,OAAO,iBAAiB;AAGhD,IAAM,yBAAyB,OAAO,wBAAwB;AAMvD,IAAM,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA,EAK7B,KAAK,MAAM,YAAY;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOvB,SAAS,MAAM,gBAAgB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQ/B,gBAAgB,MAAM,uBAAuB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAO7C,cAAc,MAAM,GAAG,YAAY,CAAC;AAAA;AAAA;AAAA;AAAA;AAAA,EAMpC,cAAc,MAAM,GAAG,YAAY,CAAC;AACtC;;;ADpCA,IAAM,gBAAgB;AAAA,EACpB,IAAI,IAAI,gBAAgB,aAAa,CAAC;AACxC;AAgBO,IAAM,OAAO;AAAA,EAClB,OAAO,WAAW;AAChB,QAAI;AAEF,UAAI,CAAC,OAAO,iBAAiB,CAAC,OAAO,cAAc,WAAW,SAAS,GAAG;AACxE,cAAM,IAAI;AAAA,UACR;AAAA,QACF;AAAA,MACF;AAEA,YAAM,QAAQ,OAAO,cAAc,UAAU,CAAC;AAE9C,UAAI,CAAC,OAAO;AACV,cAAM,IAAI,MAAM,2CAA2C;AAAA,MAC7D;AAMA,YAAM,MAAM,QAAQ,EAAE;AACtB,YAAM,SAAS,IAAI,SAAS,UAAU,QAAQ,IAAI,qBAAqB;AAEvE,UAAI;AAEJ,UAAI,QAAQ;AAGV,YAAI,KAAK,mDAAmD;AAE5D,cAAM,UAAU,UAAU,KAAK;AAC/B,0BAAkB;AAAA,MACpB,OAAO;AAML,cAAM,EAAE,QAAQ,IAAI,MAAM,UAAU,OAAO,eAAe;AAAA,UACxD,QAAQ,gBAAgB,aAAa;AAAA;AAAA,QACvC,CAAC;AAED,0BAAkB;AAAA,MACpB;AAGA,YAAM,WAAqB;AAAA,QACzB,QAAQ,gBAAgB;AAAA;AAAA,QACxB,OAAO,gBAAgB,SAAS;AAAA,MAClC;AAEA,UAAI,KAAK,mCAAmC;AAAA,QAC1C,QAAQ,SAAS;AAAA,QACjB,OAAO,SAAS;AAAA,QAChB,UAAU;AAAA,MACZ,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,UAAI,KAAK,yBAAyB;AAAA,QAChC,OAAO,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,MAC9D,CAAC;AAGD,YAAM,SAAS;AAAA,QACb,iBAAiB,QAAQ,MAAM,UAAU;AAAA,MAC3C;AAAA,IACF;AAAA,EACF;AACF;AAMO,IAAM,UAAU,IAAI,QAAQ;AAAA,EACjC,aAAa;AACf,CAAC;;;AE7HD,SAAS,WAAW;AACpB,OAAOC,UAAS;;;ACDhB,SAAS,mBAAmB;AAGrB,IAAM,KAAK,IAAI,YAAY,aAAa;AAAA,EAC7C,YAAY;AACd,CAAC;;;ACCM,IAAM,qBAAN,MAAyB;AAAA,EAC9B,YAA6BC,KAAiB;AAAjB,cAAAA;AAAA,EAAkB;AAAA;AAAA;AAAA;AAAA,EAK/C,MAAM,OAAO,MAOS;AACpB,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA,UAGpB,KAAK,OAAO;AAAA,UACZ,KAAK,GAAG;AAAA,UACR,KAAK,KAAK;AAAA,UACV,KAAK,MAAM;AAAA,UACX,KAAK,WAAW;AAAA,UAChB,KAAK,QAAQ;AAAA;AAAA;AAAA;AAKnB,QAAI,CAAC,KAAK;AACR,YAAM,IAAI,MAAM,2BAA2B;AAAA,IAC7C;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,SAAS,IAAY,QAA0C;AACnE,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA,2CACa,EAAE,kBAAkB,MAAM;AAAA;AAEjE,WAAO,OAAO;AAAA,EAChB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,KAAK,QAK2C;AACpD,UAAM,EAAE,QAAQ,OAAO,QAAQ,OAAO,IAAI;AAE1C,QAAI;AACJ,QAAI;AAEJ,QAAI,QAAQ;AACV,uBAAiB,KAAK,GAAG;AAAA;AAAA,0BAEL,MAAM,iBAAiB,MAAM;AAAA;AAAA,gBAEvC,KAAK,WAAW,MAAM;AAAA;AAGhC,mBAAa,KAAK,GAAG;AAAA;AAAA,0BAED,MAAM,iBAAiB,MAAM;AAAA;AAAA,IAEnD,OAAO;AACL,uBAAiB,KAAK,GAAG;AAAA;AAAA,0BAEL,MAAM;AAAA;AAAA,gBAEhB,KAAK,WAAW,MAAM;AAAA;AAGhC,mBAAa,KAAK,GAAG;AAAA;AAAA,0BAED,MAAM;AAAA;AAAA,IAE5B;AAGA,UAAM,YAAwB,CAAC;AAC/B,qBAAiB,YAAY,gBAAgB;AAC3C,gBAAU,KAAK,QAAQ;AAAA,IACzB;AAGA,UAAM,cAAc,MAAM;AAC1B,UAAM,QAAQ,aAAa,SAAS;AAEpC,WAAO,EAAE,WAAW,MAAM;AAAA,EAC5B;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,OACJ,IACA,QACA,MAMmB;AAEnB,UAAM,WAAW,MAAM,KAAK,SAAS,IAAI,MAAM;AAC/C,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,oBAAoB,EAAE,uBAAuB,MAAM,EAAE;AAAA,IACvE;AAGA,UAAM,WAAW,KAAK,QAAQ,SAAY,KAAK,MAAM,SAAS;AAC9D,UAAM,aAAa,KAAK,UAAU,SAAY,KAAK,QAAQ,SAAS;AACpE,UAAM,cAAc,KAAK,WAAW,SAAY,KAAK,SAAS,SAAS;AACvE,UAAM,gBAAgB,KAAK,aAAa,SAAY,KAAK,WAAW,SAAS;AAG7E,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA,gBAGd,QAAQ;AAAA,kBACN,UAAU;AAAA,mBACT,WAAW;AAAA,qBACT,aAAa;AAAA,mBACf,EAAE,kBAAkB,MAAM;AAAA;AAAA;AAIzC,QAAI,CAAC,KAAK;AACR,YAAM,IAAI,MAAM,2BAA2B;AAAA,IAC7C;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,aAAa,IAAY,QAAuC;AAEpE,UAAM,SAAS,MAAM,KAAK,GAAG;AAAA;AAAA,qBAEZ,MAAM;AAAA,mBACR,EAAE;AAAA;AAAA;AAIjB,QAAI,CAAC,QAAQ;AACX,YAAM,IAAI,MAAM,oBAAoB,EAAE,YAAY;AAAA,IACpD;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,OAAO,IAAY,QAA+B;AAEtD,UAAM,WAAW,MAAM,KAAK,SAAS,IAAI,MAAM;AAC/C,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,oBAAoB,EAAE,uBAAuB,MAAM,EAAE;AAAA,IACvE;AAEA,UAAM,KAAK,GAAG;AAAA,yCACuB,EAAE,kBAAkB,MAAM;AAAA;AAAA,EAEjE;AACF;;;ACpLA,SAAS,aAAa;AAQf,IAAM,uBAAuB,IAAI;AAAA,EACtC;AAAA,EACA;AAAA,IACE,mBAAmB;AAAA,EACrB;AACF;;;ACVA;;;ACWO,IAAM,wBAAN,MAA4B;AAAA,EACjC,YAA6BC,KAAiB;AAAjB,cAAAA;AAAA,EAAkB;AAAA;AAAA;AAAA;AAAA;AAAA,EAM/C,MAAM,OAAO,MAOY;AACvB,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAWpB,KAAK,UAAU;AAAA,UACf,KAAK,MAAM;AAAA;AAAA,UAEX,KAAK,aAAa;AAAA,UAClB,KAAK,gBAAgB;AAAA,UACrB,KAAK,cAAc;AAAA,UACnB,KAAK,YAAY;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQvB,QAAI,CAAC,KAAK;AACR,YAAM,IAAI,MAAM,+BAA+B;AAAA,IACjD;AAEA,WAAO;AAAA,MACL,GAAG;AAAA,MACH,mBAAmB,KAAK;AAAA,MACxB,qBAAqB;AAAA,IACvB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,WAAW,YAAkB,QAA8C;AAC/E,UAAM,cAAc,WAAW,SAAY,SAAS;AACpD,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BAMF,UAAU;AAAA,2CACK,WAAW;AAAA;AAElD,WAAO,MAAM;AAAA,MACX,GAAG;AAAA,MACH,mBAAmB;AAAA,MACnB,qBAAqB;AAAA,IACvB,IAAmB;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,gBACJ,WACA,SACA,QACwB;AACxB,UAAM,cAAc,WAAW,SAAY,SAAS;AACpD,UAAM,QAAQ,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6BAMG,SAAS;AAAA,6BACT,OAAO;AAAA,2CACO,WAAW;AAAA;AAAA;AAIlD,UAAM,UAAyB,CAAC;AAChC,qBAAiB,UAAU,OAAO;AAChC,cAAQ,KAAK;AAAA,QACX,GAAG;AAAA,QACH,mBAAmB;AAAA,QACnB,qBAAqB;AAAA,MACvB,CAAgB;AAAA,IAClB;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,KAAK,QAI4C;AACrD,UAAM,EAAE,OAAO,QAAQ,OAAO,IAAI;AAClC,UAAM,cAAc,WAAW,SAAY,SAAS;AAEpD,UAAM,eAAe,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,2CAMU,WAAW;AAAA;AAAA,cAExC,KAAK,WAAW,MAAM;AAAA;AAGhC,UAAM,aAAa,KAAK,GAAG;AAAA;AAAA,2CAEY,WAAW;AAAA;AAIlD,UAAM,UAAyB,CAAC;AAChC,qBAAiB,UAAU,cAAc;AACvC,cAAQ,KAAK;AAAA,QACX,GAAG;AAAA,QACH,mBAAmB;AAAA,QACnB,qBAAqB;AAAA,MACvB,CAAgB;AAAA,IAClB;AAGA,UAAM,cAAc,MAAM;AAC1B,UAAM,QAAQ,aAAa,SAAS;AAEpC,WAAO,EAAE,SAAS,MAAM;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,YAAkB,QAAmC;AACvE,UAAM,cAAc,WAAW,SAAY,SAAS;AACpD,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA,8BAGA,UAAU;AAAA,6CACK,WAAW;AAAA;AAAA;AAGpD,WAAO,KAAK,UAAU;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aACJ,IACA,QACA,cACe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,mBAGC,MAAM;AAAA,0BACC,gBAAgB,IAAI;AAAA,oCACV,0CAAqC,mCAAiC,oBAAI,KAAK,IAAI,IAAI;AAAA,mBACxG,EAAE;AAAA;AAAA,EAEnB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,iBAAiB,IAA2B;AAChD,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA,mBAKC,EAAE;AAAA;AAAA,EAEnB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,gBACJ,IACA,SACA,eACA,UACe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA,2BAIS,OAAO;AAAA,2BACP,aAAa;AAAA,gCACR,QAAQ;AAAA;AAAA,mBAErB,EAAE;AAAA;AAAA,EAEnB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,IAAY,cAAqC;AAClE,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA,0BAIQ,YAAY;AAAA;AAAA,mBAEnB,EAAE;AAAA;AAAA,EAEnB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cACJ,IACA,SACA,UACe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,2BAGS,OAAO;AAAA,gCACF,QAAQ;AAAA,mBACrB,EAAE;AAAA;AAAA,EAEnB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,OAAO,IAA2B;AACtC,UAAM,WAAW,MAAM,KAAK,GAAG;AAAA,gDACa,EAAE;AAAA;AAG9C,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,wBAAwB,EAAE,YAAY;AAAA,IACxD;AAEA,UAAM,KAAK,GAAG;AAAA,6CAC2B,EAAE;AAAA;AAAA,EAE7C;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAM,kCACJ,WACA,SACA,QACiC;AACjC,UAAM,cAAc,WAAW,SAAY,SAAS;AAEpD,UAAM,QAAQ,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8BAcI,SAAS;AAAA,8BACT,OAAO;AAAA,6CACQ,WAAW;AAAA;AAAA;AAIpD,UAAM,iBAAyC,CAAC;AAChD,qBAAiB,iBAAiB,OAAO;AACvC,qBAAe,KAAK,aAAa;AAAA,IACnC;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,8BACJ,WACA,SACA,QAC8B;AAC9B,UAAM,cAAc,WAAW,SAAY,SAAS;AAEpD,UAAM,QAAQ,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+BAcK,SAAS;AAAA,+BACT,OAAO;AAAA,6CACO,WAAW;AAAA;AAAA;AAIpD,UAAM,QAA6B,CAAC;AACpC,qBAAiB,QAAQ,OAAO;AAC9B,YAAM,KAAK,IAAI;AAAA,IACjB;AAEA,WAAO;AAAA,EACT;AACF;;;AC5WA,OAAOC,UAAS;AAYhB;AAUO,IAAM,qBAAN,MAAyB;AAAA,EAC9B,YAA6B,YAAmC;AAAnC;AAAA,EAAoC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOjE,MAAM,oBAAoB,SAAwD;AAChF,UAAM,EAAE,MAAM,QAAQ,gBAAgB,IAAI;AAC1C,UAAM,gBAAgB,iBAAiB,IAAI;AAE3C,IAAAC,KAAI,KAAK,oCAAoC;AAAA,MAC3C,YAAY;AAAA,MACZ,QAAQ,UAAU;AAAA,MAClB;AAAA,IACF,CAAC;AAED,QAAI;AAEF,YAAM,iBAAiB,MAAM,KAAK,oBAAoB,MAAM,MAAM;AAElE,UAAI,kBAAkB,CAAC,iBAAiB;AACtC,YAAI,eAAe,wCAAmC;AACpD,UAAAA,KAAI,KAAK,8DAA8D;AAAA,YACrE,UAAU,eAAe;AAAA,YACzB,YAAY;AAAA,UACd,CAAC;AACD,iBAAO;AAAA,QACT;AAEA,YAAI,eAAe,0CAAoC;AACrD,UAAAA,KAAI,KAAK,2DAA2D;AAAA,YAClE,UAAU,eAAe;AAAA,YACzB,YAAY;AAAA,UACd,CAAC;AACD,iBAAO;AAAA,QACT;AAGA,QAAAA,KAAI,KAAK,yDAAyD;AAAA,UAChE,UAAU,eAAe;AAAA,UACzB,QAAQ,eAAe;AAAA,QACzB,CAAC;AAAA,MACH;AAGA,YAAM,EAAE,WAAW,QAAQ,IAAI,KAAK,mBAAmB,IAAI;AAE3D,MAAAA,KAAI,KAAK,oCAAoC;AAAA,QAC3C,YAAY;AAAA,QACZ,WAAW,UAAU,YAAY;AAAA,QACjC,SAAS,QAAQ,YAAY;AAAA,MAC/B,CAAC;AAGD,YAAM,eAAe,MAAM,KAAK;AAAA,QAC9B;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,YAAM,iBAAiB,aAAa,OAAO,OAAK,EAAE,iBAAiB,OAAO,EAAE;AAC5E,YAAM,mBAAmB,aAAa,OAAO,OAAK,EAAE,iBAAiB,SAAS,EAAE;AAEhF,MAAAA,KAAI,KAAK,8BAA8B;AAAA,QACrC,YAAY;AAAA,QACZ,YAAY,aAAa;AAAA,QACzB,YAAY;AAAA,QACZ,cAAc;AAAA,MAChB,CAAC;AAGD,YAAM,gBAAgB,aAAa;AACnC,YAAM,mBAAmB,KAAK,0BAA0B,YAAY;AACpE,YAAM,gBAAgB,KAAK,uBAAuB,YAAY;AAE9D,MAAAA,KAAI,KAAK,8BAA8B;AAAA,QACrC,YAAY;AAAA,QACZ;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAGD,UAAI;AAEJ,UAAI,gBAAgB;AAElB,cAAM,KAAK,WAAW,iBAAiB,eAAe,EAAE;AACxD,iBAAS;AAAA,MACX,OAAO;AAEL,iBAAS,MAAM,KAAK,WAAW,OAAO;AAAA,UACpC,YAAY;AAAA,UACZ,QAAQ,UAAU;AAAA,UAClB;AAAA,UACA;AAAA,UACA,gBAAgB;AAAA,UAChB,cAAc;AAAA,QAChB,CAAC;AAAA,MACH;AAEA,MAAAA,KAAI,KAAK,iCAAiC;AAAA,QACxC,UAAU,OAAO;AAAA,QACjB,YAAY;AAAA,QACZ,QAAQ;AAAA,MACV,CAAC;AAGD,YAAM,YAAY,KAAK,IAAI;AAC3B,YAAM,gBAAgB,MAAM,KAAK,uBAAuB,cAAc;AAAA,QACpE,YAAY;AAAA,QACZ,YAAY;AAAA,QACZ,YAAY;AAAA,QACZ,cAAc;AAAA,MAChB,CAAC;AACD,YAAM,uBAAuB,KAAK,IAAI,IAAI;AAG1C,YAAM,qBAAyC;AAAA,QAC7C,WAAW,oBAAoB;AAAA,QAC/B,uBAAuB;AAAA,QACvB;AAAA,MACF;AAGA,YAAM,KAAK,WAAW;AAAA,QACpB,OAAO;AAAA,QACP;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAEA,MAAAA,KAAI,KAAK,qCAAqC;AAAA,QAC5C,UAAU,OAAO;AAAA,QACjB,YAAY;AAAA,QACZ;AAAA,QACA;AAAA,MACF,CAAC;AAGD,YAAM,kBAAkB,MAAM,KAAK,WAAW,WAAW,MAAM,MAAM;AACrE,UAAI,CAAC,iBAAiB;AACpB,cAAM,IAAI,MAAM,qCAAqC;AAAA,MACvD;AAEA,aAAO;AAAA,IACT,SAAS,OAAO;AACd,YAAM,eACJ,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAEvD,MAAAA,KAAI,MAAM,OAAO,kCAAkC;AAAA,QACjD,YAAY;AAAA,QACZ,QAAQ,UAAU;AAAA,QAClB;AAAA,MACF,CAAC;AAED,YAAM,IAAI,MAAM,oCAAoC,YAAY,EAAE;AAAA,IACpE;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAc,oBACZ,MACA,QAC6B;AAC7B,WAAO,MAAM,KAAK,WAAW,WAAW,MAAM,MAAM;AAAA,EACtD;AAAA;AAAA;AAAA;AAAA,EAKQ,mBAAmB,MAAgD;AAEzE,UAAM,aAAa,IAAI,KAAK,IAAI;AAChC,eAAW,SAAS,GAAG,GAAG,GAAG,CAAC;AAG9B,UAAM,YAAY,IAAI,KAAK,UAAU;AAGrC,UAAM,UAAU,IAAI,KAAK,UAAU;AACnC,YAAQ,SAAS,IAAI,IAAI,IAAI,GAAG;AAEhC,WAAO,EAAE,WAAW,QAAQ;AAAA,EAC9B;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAc,oBACZ,WACA,SACA,QAC8B;AAE9B,UAAM,iBAAiB,MAAM,KAAK,WAAW;AAAA,MAC3C;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,UAAM,aAAa,MAAM,KAAK,WAAW;AAAA,MACvC;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,UAAM,qBAA0C,eAAe,IAAI,QAAM;AAAA,MACvE,aAAa,EAAE;AAAA,MACf,cAAc;AAAA,MACd,SAAS,EAAE,WAAW,EAAE,oBAAoB;AAAA,MAC5C,QAAQ,EAAE;AAAA,MACV,OAAO,EAAE,kBAAkB;AAAA,MAC3B,UAAU,EAAE,YAAY;AAAA,MACxB,WAAW,EAAE,aAAa;AAAA,MAC1B,YAAY,EAAE;AAAA,IAChB,EAAE;AAGF,UAAM,WAAW,CAAC,GAAG,oBAAoB,GAAG,UAAU;AAEtD,IAAAA,KAAI,KAAK,8BAA8B;AAAA,MACrC,YAAY,mBAAmB;AAAA,MAC/B,cAAc,WAAW;AAAA,MACzB,YAAY,SAAS;AAAA,MACrB,WAAW,UAAU,YAAY;AAAA,MACjC,SAAS,QAAQ,YAAY;AAAA,IAC/B,CAAC;AAED,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,0BACN,OACkB;AAClB,UAAM,YAA8B,CAAC;AAErC,eAAW,QAAQ,OAAO;AACxB,YAAM,SAAS,KAAK;AACpB,gBAAU,MAAM,KAAK,UAAU,MAAM,KAAK,KAAK;AAAA,IACjD;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,uBAAuB,OAAoC;AACjE,WAAO,MAAM,OAAO,CAAC,OAAO,SAAS;AAEnC,UAAI,KAAK,iBAAiB,WAAW,KAAK,UAAU;AAClD,eAAO,QAAQ,KAAK;AAAA,MACtB;AACA,aAAO;AAAA,IACT,GAAG,CAAC;AAAA,EACN;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAaA,MAAc,uBACZ,cACA,SAMwB;AAExB,QAAI,aAAa,WAAW,GAAG;AAC7B,MAAAA,KAAI,KAAK,+BAA+B;AACxC,aAAO;AAAA,IACT;AAEA,IAAAA,KAAI,KAAK,8CAA8C;AAAA,MACrD,YAAY,QAAQ;AAAA,MACpB,kBAAkB,QAAQ;AAAA,MAC1B,YAAY,QAAQ;AAAA,MACpB,cAAc,QAAQ;AAAA,IACxB,CAAC;AAED,QAAI;AAEF,YAAM,EAAE,QAAAC,QAAO,IAAI,MAAM,OAAO,mBAAmB;AACnD,YAAMC,gBAAeD,QAAO,cAAc;AAG1C,YAAM,EAAE,wBAAAE,wBAAuB,IAAI,MAAM;AACzC,YAAM,mBAAmB,IAAIA,wBAAuBD,cAAa,CAAC;AAElE,YAAM,SAAS,MAAM,iBAAiB,eAAe,cAAc,OAAO;AAE1E,MAAAF,KAAI,KAAK,0CAA0C;AAAA,QACjD,YAAY,QAAQ;AAAA,QACpB,kBAAkB,QAAQ;AAAA,QAC1B,YAAY,QAAQ;AAAA,QACpB,cAAc,QAAQ;AAAA,QACtB,cAAc,OAAO;AAAA,MACvB,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,sCAAsC;AAAA,QACrD,YAAY,QAAQ;AAAA,QACpB,kBAAkB,QAAQ;AAAA,QAC1B,cAAc,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,MACrE,CAAC;AAGD,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,gBAAgB,MAAY,QAA8C;AAC9E,WAAO,MAAM,KAAK,WAAW,WAAW,MAAM,MAAM;AAAA,EACtD;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,YAAY,QAIqC;AACrD,WAAO,MAAM,KAAK,WAAW,KAAK,MAAM;AAAA,EAC1C;AACF;;;AN9WA,IAAM,eAAe,IAAI,mBAAmB,EAAE;AAC9C,IAAM,kBAAkB,IAAI,sBAAsB,EAAE;AACpD,IAAM,qBAAqB,IAAI,mBAAmB,eAAe;AAoB1D,IAAM,aAAa;AAAA,EACxB,EAAE,QAAQ,MAAM,QAAQ,QAAQ,MAAM,mBAAmB,MAAM,MAAM;AAAA,EACrE,OAAO,QAAwE;AAC7E,IAAAI,KAAI,KAAK,wCAAwC;AAAA,MAC/C,KAAK,IAAI;AAAA,MACT,QAAQ,IAAI;AAAA,IACd,CAAC;AAGD,UAAM,SAAS,IAAI,UAAU;AAC7B,UAAM,SAAS,IAAI;AAGnB,UAAM,WAAW,MAAM,aAAa,OAAO;AAAA,MACzC,SAAS;AAAA,MACT,KAAK,IAAI;AAAA,MACT,OAAO,IAAI,SAAS;AAAA,MACpB;AAAA,MACA,aAAa,oBAAI,KAAK;AAAA,MACtB,UAAU;AAAA,IACZ,CAAC;AAED,IAAAA,KAAI,KAAK,4CAA4C;AAAA,MACnD,YAAY,SAAS;AAAA,MACrB,KAAK,SAAS;AAAA,MACd,QAAQ,SAAS;AAAA,IACnB,CAAC;AAGD,UAAM,qBAAqB,QAAQ;AAAA,MACjC,YAAY,SAAS;AAAA,MACrB,KAAK,SAAS;AAAA,MACd,QAAQ,SAAS;AAAA,MACjB,OAAO,SAAS,SAAS;AAAA,IAC3B,CAAC;AAED,WAAO;AAAA,MACL,YAAY,SAAS;AAAA,MACrB,KAAK,SAAS;AAAA,MACd,QAAQ,SAAS;AAAA,MACjB,SAAS;AAAA,IACX;AAAA,EACF;AACF;AAgBO,IAAM,qBAAqB;AAAA,EAChC,EAAE,QAAQ,MAAM,QAAQ,QAAQ,MAAM,0BAA0B,MAAM,MAAM;AAAA,EAC5E,OAAO,QAAwE;AAC7E,IAAAA,KAAI,KAAK,8CAA8C;AAAA,MACrD,MAAM,IAAI;AAAA,MACV,QAAQ,IAAI;AAAA,IACd,CAAC;AAGD,UAAM,SAAS,IAAI,UAAU;AAG7B,UAAM,aAAa,IAAI,OAAO,IAAI,KAAK,IAAI,IAAI,IAAI,oBAAI,KAAK;AAE5D,IAAAA,KAAI,KAAK,sCAAsC;AAAA,MAC7C,YAAY,WAAW,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAAA,MACjD;AAAA,IACF,CAAC;AAED,QAAI;AACF,YAAM,SAAS,MAAM,mBAAmB,oBAAoB;AAAA,QAC1D,MAAM;AAAA,QACN;AAAA,QACA,iBAAiB;AAAA,MACnB,CAAC;AAED,MAAAA,KAAI,KAAK,gCAAgC;AAAA,QACvC,UAAU,OAAO;AAAA,QACjB,YAAY,OAAO;AAAA,QACnB,eAAe,OAAO;AAAA,MACxB,CAAC;AAED,aAAO;AAAA,QACL;AAAA,QACA,SAAS,OAAO,iBACZ,wCACA;AAAA,MACN;AAAA,IACF,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,yCAAyC;AAAA,QACxD,YAAY,WAAW,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAAA,QACjD;AAAA,MACF,CAAC;AAED,YAAM;AAAA,IACR;AAAA,EACF;AACF;;;AOhJA,SAAS,OAAAC,MAAK,YAAAC,iBAAgB;;;ACA9B,SAAS,eAAe,oBAAoB;AAKrC,SAAS,cAA+B;AAC3C,SAAO,aAAa;AACxB;;;ADLA,OAAOC,UAAS;;;AEST,IAAM,0BAAN,MAA8B;AAAA,EACnC,YAA6BC,KAAiB;AAAjB,cAAAA;AAAA,EAAkB;AAAA;AAAA;AAAA;AAAA,EAK/C,MAAM,cAAc,YAAmC;AACrD,UAAM,KAAK,GAAG;AAAA;AAAA,gBAEF,UAAU;AAAA;AAAA,EAExB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,iBAAiB,YAAmC;AACxD,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,4BAGU,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,YAAoB,cAAqC;AAC1E,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA,0BAIQ,YAAY;AAAA;AAAA,4BAEV,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,iBAAiB,YAAmD;AACxE,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BAMF,UAAU;AAAA;AAElC,WAAO,MAAM,EAAE,GAAG,KAAK,mBAAmB,KAAK,IAAqB;AAAA,EACtE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,wBACJ,YACA,MASe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,uBAGK,KAAK,UAAU;AAAA,6BACT,KAAK,eAAe;AAAA,sBAC3B,KAAK,SAAS;AAAA,4BACR,KAAK,cAAc;AAAA,8BACjB,KAAK,gBAAgB;AAAA,qBAC9B,KAAK,QAAQ;AAAA,uBACX,KAAK,UAAU;AAAA;AAAA;AAAA,4BAGV,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,YAAoB,SAAgC;AACtE,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,oBAGE,OAAO;AAAA;AAAA;AAAA,4BAGC,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,MAAM,8BACJ,YACA,MAIe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,uBAGK,KAAK,UAAU;AAAA,uBACf,KAAK,UAAU;AAAA;AAAA;AAAA,4BAGV,UAAU;AAAA;AAAA,EAEpC;AACF;;;AF7GA;AAGA,IAAMC,gBAAe,IAAI,mBAAmB,EAAE;AAC9C,IAAM,oBAAoB,IAAI,wBAAwB,EAAE;AACxD,IAAMC,mBAAkB,IAAI,sBAAsB,EAAE;AAGpD,IAAMC,sBAAqB,IAAI,mBAAmBD,gBAAe;AAMjE,SAAS,uBAAuB,GAAwC;AACtE,SAAO;AAAA,IACL,YAAY,EAAE;AAAA,IACd,kBAAkB,EAAE;AAAA,IACpB,SAAS,EAAE;AAAA,IACX,WAAW,EAAE;AAAA,IACb,iBAAiB,EAAE;AAAA,IACnB,UAAU,EAAE;AAAA,IACZ,YAAY,EAAE;AAAA,IACd,QAAQ,EAAE;AAAA,IACV,eAAe,EAAE;AAAA,IACjB,YAAY,EAAE;AAAA,IACd,YAAY,EAAE;AAAA,EAChB;AACF;AAGO,IAAME,UAASC;AAAA,EACpB,EAAE,QAAQ,MAAM,QAAQ,QAAQ,MAAM,cAAc,MAAM,KAAK;AAAA,EAC/D,OAAO,QAA0D;AAE/D,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAGpB,QAAI,CAAC,IAAI,OAAO,CAAC,IAAI,aAAa;AAChC,YAAMC,UAAS,gBAAgB,kCAAkC;AAAA,IACnE;AAGA,UAAM,SAAS,IAAI;AAGnB,UAAM,WAAW,MAAMN,cAAa,OAAO;AAAA,MACzC,SAAS;AAAA,MACT,KAAK,IAAI;AAAA,MACT,OAAO,IAAI,SAAS;AAAA,MACpB;AAAA,MACA,aAAa,IAAI;AAAA,MACjB,UAAU,IAAI,YAAY;AAAA,IAC5B,CAAC;AAED,IAAAO,KAAI,KAAK,yDAAyD;AAAA,MAChE,YAAY,SAAS;AAAA,MACrB,KAAK,SAAS;AAAA,MACd,QAAQ,SAAS;AAAA,IACnB,CAAC;AAID,QAAI;AACF,YAAM,YAAY,MAAM,qBAAqB,QAAQ;AAAA,QACnD,YAAY,SAAS;AAAA,QACrB,KAAK,SAAS;AAAA,QACd,QAAQ,SAAS;AAAA,QACjB,OAAO,SAAS,SAAS;AAAA,MAC3B,CAAC;AAED,MAAAA,KAAI,KAAK,iDAAiD;AAAA,QACxD,YAAY,SAAS;AAAA,QACrB;AAAA,MACF,CAAC;AAAA,IACH,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,4CAA4C;AAAA,QAC3D,YAAY,SAAS;AAAA,MACvB,CAAC;AAAA,IAEH;AAEA,WAAO,EAAE,SAAS;AAAA,EACpB;AACF;AAGO,IAAMC,OAAMJ;AAAA,EACjB,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,kBAAkB,MAAM,KAAK;AAAA,EAClE,OAAO,QAAuD;AAE5D,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAEpB,UAAM,WAAW,MAAML,cAAa,SAAS,IAAI,IAAI,MAAM;AAE3D,QAAI,CAAC,UAAU;AACb,YAAMM,UAAS,SAAS,oBAAoB,IAAI,EAAE,YAAY;AAAA,IAChE;AAEA,WAAO,EAAE,SAAS;AAAA,EACpB;AACF;AAGO,IAAMG,QAAOL;AAAA,EAClB,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,cAAc,MAAM,KAAK;AAAA,EAC9D,OAAO,QAA8D;AAEnE,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAEpB,UAAM,QAAQ,IAAI,SAAS;AAC3B,UAAM,SAAS,IAAI,UAAU;AAE7B,UAAM,EAAE,WAAW,MAAM,IAAI,MAAML,cAAa,KAAK;AAAA,MACnD;AAAA,MACA;AAAA,MACA;AAAA,MACA,QAAQ,IAAI;AAAA,IACd,CAAC;AAED,WAAO,EAAE,WAAW,MAAM;AAAA,EAC5B;AACF;AAGO,IAAMU,UAASN;AAAA,EACpB,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,kBAAkB,MAAM,KAAK;AAAA,EAClE,OAAO,QAA0D;AAE/D,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAGpB,QACE,IAAI,QAAQ,UACZ,IAAI,UAAU,UACd,IAAI,WAAW,UACf,IAAI,aAAa,QACjB;AACA,YAAMC,UAAS,gBAAgB,qBAAqB;AAAA,IACtD;AAEA,UAAM,WAAW,MAAMN,cAAa,OAAO,IAAI,IAAI,QAAQ;AAAA,MACzD,KAAK,IAAI;AAAA,MACT,OAAO,IAAI;AAAA,MACX,QAAQ,IAAI;AAAA,MACZ,UAAU,IAAI;AAAA,IAChB,CAAC;AAED,WAAO,EAAE,SAAS;AAAA,EACpB;AACF;AAGO,IAAMW,UAASP;AAAA,EACpB,EAAE,QAAQ,MAAM,QAAQ,UAAU,MAAM,kBAAkB,MAAM,KAAK;AAAA,EACrE,OAAO,QAAgE;AAErE,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAEpB,UAAML,cAAa,OAAO,IAAI,IAAI,MAAM;AACxC,WAAO,EAAE,SAAS,KAAK;AAAA,EACzB;AACF;AAGO,IAAMY,cAAaR;AAAA,EACxB,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,0BAA0B,MAAM,KAAK;AAAA,EAC1E,OAAO,QAAqE;AAE1E,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAGpB,UAAM,WAAW,MAAML,cAAa,SAAS,IAAI,IAAI,MAAM;AAE3D,QAAI,CAAC,UAAU;AACb,YAAMM,UAAS,SAAS,oBAAoB,IAAI,EAAE,YAAY;AAAA,IAChE;AAGA,UAAM,gBAAgB,MAAM,kBAAkB,iBAAiB,IAAI,EAAE;AAErE,IAAAC,KAAI,KAAK,4BAA4B;AAAA,MACnC,YAAY,IAAI;AAAA,MAChB,QAAQ,SAAS;AAAA,MACjB,kBAAkB,CAAC,CAAC;AAAA,IACtB,CAAC;AAED,WAAO;AAAA,MACL;AAAA,MACA,eAAe,gBAAgB,uBAAuB,aAAa,IAAI;AAAA,IACzE;AAAA,EACF;AACF;AAQO,IAAMM,uBAAsBT;AAAA,EACjC,EAAE,QAAQ,MAAM,QAAQ,QAAQ,MAAM,qBAAqB,MAAM,KAAK;AAAA,EACtE,OAAO,QAA2E;AAEhF,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAIpB,QAAI;AAEJ,QAAI,KAAK,MAAM;AACb,UAAI;AACF,qBAAa,gBAAgB,IAAI,IAAI;AAAA,MACvC,SAAS,OAAO;AACd,cAAMC,UAAS,gBAAgB,qCAAqC;AAAA,MACtE;AAAA,IACF,OAAO;AAEL,YAAM,EAAE,YAAY,UAAU,IAAI,mBAAmB;AACrD,mBAAa;AAAA,IACf;AAEA,IAAAC,KAAI,KAAK,2BAA2B;AAAA,MAClC,YAAY,WAAW,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAAA,MACjD;AAAA,IACF,CAAC;AAED,QAAI;AACF,YAAM,SAAS,MAAML,oBAAmB,oBAAoB;AAAA,QAC1D,MAAM;AAAA,QACN;AAAA,QACA,iBAAiB;AAAA,MACnB,CAAC;AAED,aAAO;AAAA,QACL;AAAA,QACA,SAAS,OAAO,iBACZ,wCACA;AAAA,MACN;AAAA,IACF,SAAS,OAAO;AACd,MAAAK,KAAI,MAAM,OAAO,mCAAmC;AAAA,QAClD,YAAY,WAAW,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAAA,MACnD,CAAC;AAED,YAAMD,UAAS;AAAA,QACb,oCACE,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CACvD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAGO,IAAMQ,kBAAiBV;AAAA,EAC5B,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,kBAAkB,MAAM,KAAK;AAAA,EAClE,OAAO,QAAgE;AAErE,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAEpB,QAAI;AAEJ,QAAI;AACF,mBAAa,gBAAgB,IAAI,IAAI;AAAA,IACvC,SAAS,OAAO;AACd,YAAMC,UAAS,gBAAgB,qCAAqC;AAAA,IACtE;AAEA,IAAAC,KAAI,KAAK,yBAAyB;AAAA,MAChC,YAAY,IAAI;AAAA,MAChB;AAAA,IACF,CAAC;AAED,UAAM,SAAS,MAAML,oBAAmB;AAAA,MACtC;AAAA,MACA;AAAA,IACF;AAEA,QAAI,CAAC,QAAQ;AACX,MAAAK,KAAI,KAAK,0BAA0B;AAAA,QACjC,YAAY,IAAI;AAAA,QAChB;AAAA,MACF,CAAC;AAAA,IACH;AAEA,WAAO,EAAE,OAAO;AAAA,EAClB;AACF;AAGO,IAAMQ,oBAAmBX;AAAA,EAC9B,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,YAAY,MAAM,KAAK;AAAA,EAC5D,OAAO,QAAoE;AAEzE,UAAMC,QAAO,YAAY;AACzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AACA,UAAM,SAASD,MAAK;AAEpB,UAAM,QAAQ,IAAI,SAAS;AAC3B,UAAM,SAAS,IAAI,UAAU;AAE7B,IAAAE,KAAI,KAAK,yBAAyB;AAAA,MAChC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,UAAM,EAAE,SAAS,MAAM,IAAI,MAAML,oBAAmB,YAAY;AAAA,MAC9D;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,WAAO,EAAE,SAAS,MAAM;AAAA,EAC1B;AACF;AAIO,IAAMc,4BAA2BZ;AAAA,EACtC,EAAE,QAAQ,OAAO,QAAQ,QAAQ,MAAM,8BAA8B;AAAA,EACrE,YAAkD;AAEhD,UAAM,EAAE,WAAW,IAAI,mBAAmB;AAC1C,UAAM,gBAAgB,WAAW,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAE3D,IAAAG,KAAI,KAAK,mEAAmE;AAAA,MAC1E,YAAY;AAAA,IACd,CAAC;AAED,QAAI;AAEF,YAAM,EAAE,MAAM,IAAI,MAAM;AAGxB,YAAM,EAAE,QAAQ,IAAI,MAAM,MAAM,WAAW;AAE3C,MAAAA,KAAI,KAAK,uCAAuC;AAAA,QAC9C,YAAY;AAAA,QACZ,WAAW,QAAQ;AAAA,MACrB,CAAC;AAGD,YAAM,UAAU;AAAA,QACd,OAAO,QAAQ;AAAA,QACf,YAAY;AAAA,QACZ,QAAQ;AAAA,QACR,QAAQ,CAAC;AAAA,MACX;AAGA,iBAAW,UAAU,SAAS;AAC5B,YAAI;AACF,gBAAML,oBAAmB,oBAAoB;AAAA,YAC3C,MAAM;AAAA,YACN;AAAA,YACA,iBAAiB;AAAA,UACnB,CAAC;AAED,kBAAQ;AAER,UAAAK,KAAI,KAAK,6BAA6B;AAAA,YACpC;AAAA,YACA,YAAY;AAAA,UACd,CAAC;AAAA,QACH,SAAS,OAAO;AACd,kBAAQ;AACR,gBAAM,eACJ,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAEvD,kBAAQ,OAAO,KAAK,EAAE,QAAQ,OAAO,aAAa,CAAC;AAEnD,UAAAA,KAAI,MAAM,OAAO,sCAAsC;AAAA,YACrD;AAAA,YACA,YAAY;AAAA,YACZ;AAAA,UACF,CAAC;AAAA,QAGH;AAAA,MACF;AAGA,MAAAA,KAAI,KAAK,uDAAuD;AAAA,QAC9D,YAAY;AAAA,QACZ,OAAO,QAAQ;AAAA,QACf,YAAY,QAAQ;AAAA,QACpB,QAAQ,QAAQ;AAAA,QAChB,aAAa,IAAK,QAAQ,aAAa,QAAQ,QAAS,KAAK,QAAQ,CAAC,CAAC;AAAA,MACzE,CAAC;AAGD,UAAI,QAAQ,WAAW,QAAQ,SAAS,QAAQ,QAAQ,GAAG;AACzD,cAAM,IAAI;AAAA,UACR,OAAO,QAAQ,KAAK;AAAA,QACtB;AAAA,MACF;AAIA,YAAM,cAAc,QAAQ,CAAC;AAC7B,YAAM,SAAS,cACX,MAAML,oBAAmB,gBAAgB,YAAY,WAAW,IAChE;AAEJ,aAAO;AAAA,QACL,QAAQ,UAAW,CAAC;AAAA;AAAA,QACpB,SAAS,iCAAiC,QAAQ,UAAU,IAAI,QAAQ,KAAK;AAAA,MAC/E;AAAA,IACF,SAAS,OAAO;AACd,MAAAK,KAAI,MAAM,OAAO,kDAAkD;AAAA,QACjE,YAAY;AAAA,MACd,CAAC;AAED,YAAMD,UAAS;AAAA,QACb,0CACE,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CACvD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;;;AGteA,SAAS,OAAAW,MAAK,YAAAC,iBAAgB;AAE9B,OAAOC,UAAS;;;ACFhB,SAAS,eAAAC,oBAAmB;AAMrB,IAAMC,MAAK,IAAID,aAAY,SAAS;AAAA,EACzC,YAAY;AACd,CAAC;;;ACDM,IAAM,iBAAN,MAAqB;AAAA,EAC1B,YAA6BE,KAAiB;AAAjB,cAAAA;AAAA,EAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAS/C,MAAM,OAAO,MAIK;AAChB,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA,gBAEd,KAAK,EAAE,KAAK,KAAK,KAAK,KAAK,KAAK,QAAQ,IAAI;AAAA;AAAA;AAIxD,QAAI,CAAC,KAAK;AACR,YAAM,IAAI,MAAM,uBAAuB;AAAA,IACzC;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,SAAS,IAAkC;AAC/C,WACG,MAAM,KAAK,GAAG;AAAA,uCACkB,EAAE;AAAA,SAC/B;AAAA,EAER;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,YAAY,OAAqC;AACrD,WACG,MAAM,KAAK,GAAG;AAAA,0CACqB,KAAK;AAAA,SACrC;AAAA,EAER;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,MAAM,iBAAiB,gBAA8C;AACnE,WAAO,KAAK,SAAS,cAAc;AAAA,EACrC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,OAAiC;AACnD,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA,0DAC4B,KAAK;AAAA;AAE3D,WAAO,MAAM,IAAI,QAAQ,IAAI;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,IAA8B;AAC7C,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA,uDACyB,EAAE;AAAA;AAErD,WAAO,MAAM,IAAI,QAAQ,IAAI;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,OACJ,IACA,MAIe;AAEf,UAAM,UAAoB,CAAC;AAE3B,QAAI,KAAK,UAAU,QAAW;AAC5B,cAAQ,KAAK,OAAO;AAAA,IACtB;AACA,QAAI,KAAK,SAAS,QAAW;AAC3B,cAAQ,KAAK,MAAM;AAAA,IACrB;AAEA,QAAI,QAAQ,WAAW,GAAG;AACxB,YAAM,IAAI,MAAM,qBAAqB;AAAA,IACvC;AAGA,QAAI;AACJ,QAAI,KAAK,UAAU,UAAa,KAAK,SAAS,QAAW;AACvD,cAAQ,KAAK,GAAG;AAAA;AAAA,sBAEA,KAAK,KAAK,YAAY,KAAK,IAAI;AAAA,qBAChC,EAAE;AAAA;AAAA;AAAA,IAGnB,WAAW,KAAK,UAAU,QAAW;AACnC,cAAQ,KAAK,GAAG;AAAA;AAAA,sBAEA,KAAK,KAAK;AAAA,qBACX,EAAE;AAAA;AAAA;AAAA,IAGnB,OAAO;AACL,cAAQ,KAAK,GAAG;AAAA;AAAA,qBAED,KAAK,IAAI;AAAA,qBACT,EAAE;AAAA;AAAA;AAAA,IAGnB;AAEA,UAAM,MAAM,MAAM;AAElB,QAAI,CAAC,KAAK;AACR,YAAM,IAAI,MAAM,gBAAgB,EAAE,YAAY;AAAA,IAChD;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,IAA2B;AACtC,UAAM,KAAK,GAAG;AAAA,qCACmB,EAAE;AAAA;AAAA,EAErC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,iBAAoC;AACxC,UAAM,QAAQ,KAAK,GAAG;AAAA;AAAA;AAItB,UAAM,UAAoB,CAAC;AAC3B,qBAAiB,OAAO,OAAO;AAC7B,cAAQ,KAAK,IAAI,EAAE;AAAA,IACrB;AAEA,WAAO;AAAA,EACT;AACF;;;AFpKA,IAAM,WAAW,IAAI,eAAeC,GAAE;AAyC/B,IAAMC,MAAKC;AAAA,EAChB,EAAE,QAAQ,MAAM,QAAQ,OAAO,MAAM,aAAa,MAAM,KAAK;AAAA,EAC7D,YAAiC;AAG/B,UAAMC,QAAO,YAAY;AAEzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AAEA,QAAI;AAEF,YAAM,OAAO,MAAM,SAAS,SAASD,MAAK,MAAM;AAEhD,UAAI,CAAC,MAAM;AACT,QAAAE,KAAI,KAAK,oCAAoC;AAAA,UAC3C,QAAQF,MAAK;AAAA,UACb,OAAOA,MAAK;AAAA,QACd,CAAC;AACD,cAAM,IAAI;AAAA,UACR,QAAQA,MAAK,MAAM;AAAA,QACrB;AAAA,MACF;AAEA,MAAAE,KAAI,KAAK,kCAAkC;AAAA,QACzC,QAAQ,KAAK;AAAA,MACf,CAAC;AAED,YAAM,WAAW;AAAA,QACf,IAAI,KAAK;AAAA;AAAA,QACT,OAAO,KAAK;AAAA,QACZ,MAAM,KAAK;AAAA,QACX,sBAAsB,KAAK;AAAA,QAC3B,YAAY,KAAK;AAAA,QACjB,YAAY,KAAK;AAAA,MACnB;AAEA,aAAO,EAAE,MAAM,SAAS;AAAA,IAC1B,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,gCAAgC;AAAA,QAC/C,QAAQF,MAAK;AAAA,MACf,CAAC;AAED,YAAMC,UAAS,SAAS,kCAAkC;AAAA,IAC5D;AAAA,EACF;AACF;AAcO,IAAME,iBAAgBJ;AAAA,EAC3B,EAAE,QAAQ,MAAM,QAAQ,SAAS,MAAM,aAAa,MAAM,KAAK;AAAA,EAC/D,OAAO,QAA8D;AAEnE,UAAMC,QAAO,YAAY;AAEzB,QAAI,CAACA,OAAM;AACT,YAAMC,UAAS,gBAAgB,yBAAyB;AAAA,IAC1D;AAEA,QAAI;AAEF,UAAI,IAAI,SAAS,QAAW;AAC1B,cAAMA,UAAS;AAAA,UACb;AAAA,QACF;AAAA,MACF;AAGA,YAAM,cAAc,MAAM,SAAS,OAAOD,MAAK,QAAQ;AAAA,QACrD,MAAM,IAAI;AAAA,MACZ,CAAC;AAED,MAAAE,KAAI,KAAK,wBAAwB;AAAA,QAC/B,QAAQ,YAAY;AAAA,QACpB,eAAe,OAAO,KAAK,GAAG;AAAA,MAChC,CAAC;AAED,YAAM,WAAW;AAAA,QACf,IAAI,YAAY;AAAA,QAChB,OAAO,YAAY;AAAA,QACnB,MAAM,YAAY;AAAA,QAClB,sBAAsB,YAAY;AAAA,QAClC,YAAY,YAAY;AAAA,QACxB,YAAY,YAAY;AAAA,MAC1B;AAEA,aAAO,EAAE,MAAM,SAAS;AAAA,IAC1B,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,iCAAiC;AAAA,QAChD,QAAQF,MAAK;AAAA,MACf,CAAC;AAED,UAAI,iBAAiBC,WAAU;AAC7B,cAAM;AAAA,MACR;AAEA,YAAMA,UAAS,SAAS,+BAA+B;AAAA,IACzD;AAAA,EACF;AACF;AAeO,IAAMG,cAAaL;AAAA,EACxB,EAAE,QAAQ,OAAO,QAAQ,OAAO,MAAM,cAAc,MAAM,MAAM;AAAA,EAChE,YAAyC;AACvC,QAAI;AACF,YAAM,UAAU,MAAM,SAAS,eAAe;AAE9C,MAAAG,KAAI,KAAK,4CAA4C;AAAA,QACnD,OAAO,QAAQ;AAAA,MACjB,CAAC;AAED,aAAO,EAAE,QAAQ;AAAA,IACnB,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,0BAA0B;AAC3C,YAAMD,UAAS,SAAS,0BAA0B;AAAA,IACpD;AAAA,EACF;AACF;;;AG5MA,SAAS,OAAAI,YAAqB;AAC9B,OAAOC,UAAS;AAwBhB,IAAMC,YAAW,IAAI,eAAeC,GAAE;AAmB/B,IAAMC,eAAcC;AAAA,EACzB;AAAA,IACE,QAAQ;AAAA,IACR,QAAQ;AAAA,IACR,MAAM;AAAA,IACN,MAAM;AAAA;AAAA,EACR;AAAA,EACA,OAAO,YAAkF;AACvF,QAAI;AACF,MAAAC,KAAI,KAAK,qCAAqC;AAAA,QAC5C,QAAQ,QAAQ;AAAA,QAChB,YAAY,QAAQ;AAAA,MACtB,CAAC;AAGD,YAAM,SAAS,QAAQ;AAGvB,YAAM,QAAQ,QAAQ,OAAO;AAC7B,YAAM,OAAO,QAAQ,OAAO,eAAe;AAG3C,YAAM,eAAe,MAAMJ,UAAS,SAAS,MAAM;AAEnD,UAAI,cAAc;AAChB,QAAAI,KAAI,KAAK,yCAAyC;AAAA,UAChD;AAAA,UACA;AAAA,QACF,CAAC;AAGD,eAAO;AAAA,UACL,QAAQ;AAAA,YACN,GAAG,QAAQ;AAAA,YACX,iBAAiB;AAAA,UACnB;AAAA,QACF;AAAA,MACF;AAGA,YAAM,OAAO,MAAMJ,UAAS,OAAO;AAAA,QACjC,IAAI;AAAA,QACJ;AAAA,QACA;AAAA,MACF,CAAC;AAED,MAAAI,KAAI,KAAK,kCAAkC;AAAA,QACzC,QAAQ,KAAK;AAAA,QACb,OAAO,KAAK;AAAA,MACd,CAAC;AAID,aAAO;AAAA,QACL,QAAQ;AAAA,UACN,GAAG,QAAQ;AAAA,UACX,iBAAiB;AAAA,QACnB;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,MAAAA,KAAI,MAAM,OAAO,8CAA8C;AAAA,QAC7D,QAAQ,QAAQ;AAAA,QAChB,OAAO,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,MAC9D,CAAC;AAID,aAAO,EAAE,QAAQ,QAAQ,OAAO;AAAA,IAClC;AAAA,EACF;AACF;;;AClHA,SAAS,oBAAoB;AAC7B,SAAS,UAAAC,eAAc;AACvB,OAAOC,WAAS;;;ACFhB,SAAS,SAAAC,cAAa;AAQf,IAAM,gCAAgC,IAAIA;AAAA,EAC/C;AAAA,EACA;AAAA,IACE,mBAAmB;AAAA,EACrB;AACF;;;ACbA,SAAS,SAAAC,cAAa;AAQf,IAAM,uBAAuB,IAAIA;AAAA,EACtC;AAAA,EACA;AAAA,IACE,mBAAmB;AAAA,EACrB;AACF;;;ACbA,SAAS,SAAAC,cAAa;AAQf,IAAM,wBAAwB,IAAIA;AAAA,EACvC;AAAA,EACA;AAAA,IACE,mBAAmB;AAAA,EACrB;AACF;;;ACbA,SAAS,aAAa;AACtB,OAAO,QAAQ;AACf,OAAOC,UAAS;;;ACFhB,SAAS,cAAc;AAOhB,IAAM,mBAAmB,IAAI,OAAO,eAAe;AAAA,EACxD,WAAW;AACb,CAAC;;;ACgBM,SAAS,gBAAgB,KAA6B;AAE3D,QAAM,aAAa,IAAI,MAAM,iCAAiC;AAC9D,MAAI,YAAY;AACd,WAAO;AAAA,MACL,UAAU;AAAA,MACV,QAAQ,WAAW,CAAC;AAAA,IACtB;AAAA,EACF;AAGA,QAAM,cAAc,IAAI,MAAM,yCAAyC;AACvE,MAAI,aAAa;AACf,QAAI;AACF,YAAM,UAAU,OAAO,KAAK,YAAY,CAAC,GAAG,QAAQ,EAAE,SAAS,OAAO;AAGtE,UAAI,IAAI,OAAO;AAEf,aAAO,EAAE,UAAU,UAAU,QAAQ;AAAA,IACvC,SAAS,OAAO;AACd,YAAM,IAAI;AAAA,QACR,kEAAkE,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,MAC1H;AAAA,IACF;AAAA,EACF;AAGA,MAAI,IAAI,SAAS,MAAM,KAAK,IAAI,SAAS,OAAO,KAAK,IAAI,SAAS,MAAM,GAAG;AACzE,WAAO,EAAE,UAAU,OAAO,SAAS,IAAI;AAAA,EACzC;AAEA,SAAO,EAAE,UAAU,UAAU;AAC/B;AAQA,eAAsB,mBAAmB,QAAiC;AACxE,QAAM,WAAW,MAAM;AAAA,IACrB,sCAAsC,MAAM;AAAA,EAC9C;AAEA,MAAI,CAAC,SAAS,IAAI;AAChB,UAAM,IAAI;AAAA,MACR,qBAAqB,SAAS,MAAM,IAAI,SAAS,UAAU;AAAA,IAC7D;AAAA,EACF;AAEA,QAAM,OAAQ,MAAM,SAAS,KAAK;AAElC,MAAI,CAAC,KAAK,WAAW,KAAK,QAAQ,WAAW,GAAG;AAC9C,UAAM,IAAI;AAAA,MACR,2CAA2C,MAAM;AAAA,IACnD;AAAA,EACF;AAEA,QAAM,UAAU,KAAK,QAAQ,CAAC,GAAG;AACjC,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI,MAAM,2CAA2C;AAAA,EAC7D;AAEA,SAAO;AACT;;;AFtFA,OAAO,kBAAkB;AACzB,OAAO,SAAS;AAChB,OAAO,eAAe;AAGtB,IAAM,gBAAgB,MAAM,OAAO;AACnC,IAAM,mBAAmB;AACzB,IAAM,gBAAgB;AACtB,IAAM,wBAAwB;AAK9B,eAAe,iBAAiB,KAAa,YAAY,eAAkC;AACzF,QAAM,aAAa,IAAI,gBAAgB;AACvC,QAAM,UAAU,WAAW,MAAM,WAAW,MAAM,GAAG,SAAS;AAE9D,MAAI;AACF,UAAM,WAAW,MAAM,MAAM,KAAK,EAAE,QAAQ,WAAW,OAAO,CAAC;AAC/D,iBAAa,OAAO;AACpB,WAAO;AAAA,EACT,SAAS,OAAY;AACnB,iBAAa,OAAO;AACpB,QAAI,MAAM,SAAS,cAAc;AAC/B,YAAM,IAAI,MAAM,yBAAyB,SAAS,OAAO,GAAG,EAAE;AAAA,IAChE;AACA,UAAM;AAAA,EACR;AACF;AAMO,IAAM,2BAAN,MAA+B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASpC,MAAM,kBACJ,YACA,YACiB;AACjB,IAAAC,KAAI,KAAK,8BAA8B,EAAE,YAAY,WAAW,CAAC;AAGjE,UAAM,aAAa,MAAM,KAAK,cAAc,UAAU;AACtD,IAAAA,KAAI,KAAK,yBAAyB,EAAE,YAAY,WAAW,CAAC;AAG5D,UAAM,eAAe,MAAM,KAAK,gBAAgB,UAAU;AAC1D,IAAAA,KAAI,KAAK,2BAA2B,EAAE,cAAc,WAAW,CAAC;AAGhE,UAAM,WAAW,MAAM,KAAK,oBAAoB,YAAY,YAAY;AACxE,IAAAA,KAAI,KAAK,2BAA2B,EAAE,UAAU,WAAW,CAAC;AAG5D,WAAO,MAAM,KAAK,qBAAqB,UAAU,UAAU;AAAA,EAC7D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,MAAc,cAAc,YAAqC;AAC/D,UAAM,UAAU,gBAAgB,UAAU;AAE1C,YAAQ,QAAQ,UAAU;AAAA,MACxB,KAAK;AACH,eAAO,QAAQ;AAAA,MAEjB,KAAK;AACH,eAAO,MAAM,mBAAmB,QAAQ,MAAO;AAAA,MAEjD,KAAK;AACH,eAAO,QAAQ;AAAA,MAEjB;AACE,cAAM,IAAI,MAAM,mCAAmC,UAAU,EAAE;AAAA,IACnE;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,MAAc,gBAAgB,YAAqC;AAEjE,QACE,WAAW,SAAS,MAAM,KAC1B,WAAW,SAAS,QAAQ,KAC5B,WAAW,SAAS,OAAO,GAC3B;AACA,aAAO;AAAA,IACT;AAEA,QAAI;AACF,YAAM,EAAE,QAAQ,OAAO,SAAS,IAAI,MAAM,IAAI;AAAA,QAC5C,KAAK;AAAA,QACL,SAAS;AAAA;AAAA,QACT,cAAc;AAAA,UACZ,SAAS;AAAA,YACP,cAAc;AAAA,UAChB;AAAA,QACF;AAAA,MACF,CAAC;AAED,UAAI,UAAU;AACZ,cAAM,IAAI,MAAM,8BAA8B,OAAO,SAAS,eAAe,EAAE;AAAA,MACjF;AAEA,YAAM,QAAQ,OAAO,WAAW,OAAO,gBAAgB;AAEvD,UAAI,CAAC,OAAO;AACV,cAAM,IAAI,MAAM,sCAAsC;AAAA,MACxD;AAGA,aAAO,KAAK,kBAAkB,KAAK;AAAA,IACrC,SAAS,OAAY;AACnB,YAAM,WAAW,MAAM,SAAS,MAAM,WAAW,OAAO,KAAK;AAC7D,MAAAA,KAAI,KAAK,0EAA0E;AAAA,QACjF;AAAA,QACA;AAAA,MACF,CAAC;AAID,aAAO;AAAA,IACT;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,kBAAkB,OAAuB;AAC/C,WAAO,MACJ,QAAQ,4CAA4C,EAAE,EACtD,KAAK;AAAA,EACV;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAc,oBACZ,YACA,cACiB;AAEjB,UAAM,WAAW,MAAM,iBAAiB,UAAU;AAClD,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,6BAA6B,SAAS,MAAM,IAAI,SAAS,UAAU,EAAE;AAAA,IACvF;AAEA,UAAM,UAAU,MAAM,SAAS,KAAK;AAGpC,UAAM,UAAU,MAAM,IAAI;AAAA,MACxB,CAAC,SAAS,WAAW;AACnB,qBAAa,SAAS,CAAC,KAAK,SAAS;AACnC,cAAI;AAAK,mBAAO,GAAG;AAAA;AACd,oBAAQ,IAAI;AAAA,QACnB,CAAC;AAAA,MACH;AAAA,IACF;AAEA,QAAI,CAAC,QAAQ,YAAY,QAAQ,SAAS,WAAW,GAAG;AACtD,YAAM,IAAI,MAAM,+BAA+B;AAAA,IACjD;AAGA,QAAI,CAAC,cAAc;AACjB,MAAAA,KAAI,KAAK,sCAAsC,EAAE,WAAW,CAAC;AAC7D,YAAM,gBAAgB,QAAQ,SAAS,CAAC;AAExC,UAAI,CAAC,cAAc,WAAW,KAAK;AACjC,cAAM,IAAI,MAAM,iCAAiC;AAAA,MACnD;AAEA,MAAAA,KAAI,KAAK,2BAA2B;AAAA,QAClC,OAAO,cAAc;AAAA,QACrB,UAAU,cAAc,UAAU;AAAA,MACpC,CAAC;AAED,aAAO,cAAc,UAAU;AAAA,IACjC;AAGA,UAAM,gBAAgB,QAAQ,SAAS,IAAI,CAAC,OAAO,GAAG,KAAK;AAC3D,UAAM,UAAU,UAAU,GAAG,cAAc,eAAe;AAAA,MACxD,WAAW;AAAA,MACX,OAAO;AAAA;AAAA,IACT,CAAC;AAED,QAAI,QAAQ,WAAW,KAAK,QAAQ,CAAC,EAAE,QAAQ,uBAAuB;AACpE,MAAAA,KAAI,KAAK,+BAA+B;AAAA,QACtC,aAAa;AAAA,QACb,iBAAiB,cAAc,MAAM,GAAG,CAAC;AAAA,QACzC,WAAW,QAAQ,CAAC,GAAG;AAAA,MACzB,CAAC;AACD,YAAM,IAAI;AAAA,QACR,YAAY,YAAY;AAAA,MAC1B;AAAA,IACF;AAGA,UAAM,iBAAiB,QAAQ,CAAC,EAAE,SAAS,cAAc,QAAQ,QAAQ,CAAC,EAAE,MAAM,IAAI;AACtF,UAAM,iBAAiB,QAAQ,SAAS,cAAc;AAEtD,QAAI,CAAC,eAAe,WAAW,KAAK;AAClC,YAAM,IAAI,MAAM,kCAAkC;AAAA,IACpD;AAEA,IAAAA,KAAI,KAAK,4BAA4B;AAAA,MACnC,aAAa;AAAA,MACb,cAAc,eAAe;AAAA,MAC7B,OAAO,QAAQ,CAAC,EAAE;AAAA,MAClB,UAAU,eAAe,UAAU;AAAA,MACnC,oBAAoB,QAAQ,MAAM,GAAG,CAAC,EAAE,IAAI,QAAM;AAAA,QAChD,OAAO,EAAE;AAAA,QACT,OAAO,EAAE;AAAA,MACX,EAAE;AAAA,IACJ,CAAC;AAED,WAAO,eAAe,UAAU;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,MAAc,qBACZ,UACA,YACiB;AACjB,UAAM,WAAW,gBAAgB,UAAU;AAC3C,UAAM,YAAY,SAAS,UAAU;AAGrC,QAAI;AACJ,QAAI;AACF,kBAAY,IAAI,IAAI,QAAQ;AAAA,IAC9B,QAAQ;AACN,YAAM,IAAI,MAAM,6BAA6B,QAAQ,EAAE;AAAA,IACzD;AAGA,QAAI,CAAC,CAAC,SAAS,QAAQ,EAAE,SAAS,UAAU,QAAQ,GAAG;AACrD,YAAM,IAAI,MAAM,yBAAyB,UAAU,QAAQ,yBAAyB;AAAA,IACtF;AAEA,IAAAA,KAAI,KAAK,6BAA6B;AAAA,MACpC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI;AAEF,YAAM,KAAK,iBAAiB,UAAU,QAAQ;AAG9C,UAAI,CAAC,GAAG,WAAW,QAAQ,GAAG;AAC5B,cAAM,IAAI,MAAM,2BAA2B;AAAA,MAC7C;AAEA,YAAM,WAAW,GAAG,SAAS,QAAQ,EAAE;AAGvC,UAAI,aAAa,GAAG;AAClB,WAAG,WAAW,QAAQ;AACtB,cAAM,IAAI,MAAM,0BAA0B;AAAA,MAC5C;AAEA,UAAI,WAAW,eAAe;AAC5B,WAAG,WAAW,QAAQ;AACtB,cAAM,IAAI;AAAA,UACR,0BAA0B,WAAW,OAAO,MAAM,QAAQ,CAAC,CAAC,WAAW,gBAAgB,OAAO,IAAI;AAAA,QACpG;AAAA,MACF;AAEA,MAAAA,KAAI,KAAK,iDAAiD;AAAA,QACxD;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAGD,YAAM,cAAc,GAAG,aAAa,QAAQ;AAC5C,YAAM,iBAAiB,OAAO,WAAW,aAAa;AAAA,QACpD,aAAa;AAAA,MACf,CAAC;AAED,MAAAA,KAAI,KAAK,4BAA4B;AAAA,QACnC;AAAA,QACA;AAAA,QACA,MAAM;AAAA,MACR,CAAC;AAGD,SAAG,WAAW,QAAQ;AAEtB,aAAO;AAAA,IACT,SAAS,OAAO;AAEd,UAAI;AACF,YAAI,GAAG,WAAW,QAAQ,GAAG;AAC3B,aAAG,WAAW,QAAQ;AAAA,QACxB;AAAA,MACF,SAAS,cAAc;AACrB,QAAAA,KAAI,KAAK,cAAc,gCAAgC,EAAE,SAAS,CAAC;AAAA,MACrE;AAEA,MAAAA,KAAI,MAAM,OAAO,+CAA+C;AAAA,QAC9D;AAAA,QACA;AAAA,MACF,CAAC;AACD,YAAM,IAAI;AAAA,QACR,qCAAqC,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,MAC7F;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOQ,iBAAiB,KAAa,YAAmC;AACvE,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AAEtC,YAAM,OAAO,MAAM,QAAQ;AAAA,QACzB;AAAA;AAAA,QACA;AAAA;AAAA,QACA;AAAA,QAAc,OAAO,mBAAmB,GAAI;AAAA;AAAA,QAC5C;AAAA,QAAkB,OAAO,aAAa;AAAA;AAAA,QACtC;AAAA,QAAM;AAAA;AAAA,QACN;AAAA;AAAA,MACF,CAAC;AAED,UAAI,SAAS;AAEb,WAAK,OAAO,GAAG,QAAQ,CAAC,SAAS;AAC/B,kBAAU,KAAK,SAAS;AAAA,MAC1B,CAAC;AAED,WAAK,GAAG,SAAS,CAAC,UAAU;AAC1B,eAAO,IAAI,MAAM,qBAAqB,MAAM,OAAO,EAAE,CAAC;AAAA,MACxD,CAAC;AAED,WAAK,GAAG,SAAS,CAAC,SAAS;AACzB,YAAI,SAAS,GAAG;AACd,iBAAO,IAAI,MAAM,yBAAyB,IAAI,KAAK,MAAM,EAAE,CAAC;AAAA,QAC9D,OAAO;AACL,cAAI,QAAQ;AACV,YAAAA,KAAI,MAAM,sBAAsB,EAAE,OAAO,CAAC;AAAA,UAC5C;AACA,kBAAQ;AAAA,QACV;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAAA,EACH;AACF;;;AGnYA,SAAS,iBAAiB;AAC1B,SAAS,QAAQ,oBAAoB;AACrC,OAAOC,SAAQ;AACf,OAAOC,UAAS;;;ACHhB,OAAO,QAAQ;AACf,OAAO,UAAU;AAMV,IAAM,kBAAkB;AAAA,EAC7B,OAAO;AAAA,EACP,aAAa;AAAA,EACb,YAAY;AAAA,EACZ,WAAW;AAAA,EACX,SAAS;AAAA,EACT,WAAW;AAAA,EACX,WAAW;AAAA,EACX,SAAS;AAAA,EACT,QAAQ;AAAA,EACR,UAAU;AACZ;AAMO,IAAM,gBAAgB;AAAA,EAC3B,OAAO;AAAA,EACP,aAAa;AAAA,EACb,iBAAiB;AAAA,EACjB,cACE;AACJ;AAMO,IAAM,iBAAiB;AAAA,EAC5B,aAAa;AAAA,EACb,cAAc;AAAA;AAAA,EACd,aAAa,CAAC,YAAoB,KAAK,KAAK,GAAG,OAAO,GAAG,GAAG,OAAO,MAAM;AAC3E;AAMO,IAAM,uBAAuB;AAAA,EAClC;AAAA,EACA;AAAA,EACA;AACF;AAMO,IAAM,gBAAgB;AAAA,EAC3B,OAAO;AAAA;AAAA,EACP,gBAAgB;AAAA;AAAA,EAChB,SAAS;AAAA;AAAA,EACT,SAAS;AAAA;AACX;;;ACtDO,SAAS,sBAAsB,KAA4B;AAChE,aAAW,WAAW,sBAAsB;AAC1C,UAAM,QAAQ,IAAI,MAAM,OAAO;AAC/B,QAAI,SAAS,MAAM,CAAC,GAAG;AACrB,aAAO,MAAM,CAAC;AAAA,IAChB;AAAA,EACF;AACA,SAAO;AACT;AAOO,SAAS,gBAAgB,SAAyB;AACvD,SAAO,mCAAmC,OAAO;AACnD;;;AFhBA,IAAM,OAAO,UAAU,YAAY;AAMnC,SAAS,gBAAwB;AAC/B,QAAM,QAAQ;AAAA,IACZ;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,EACF;AAEA,aAAWC,SAAQ,OAAO;AACxB,QAAI;AACF,UAAIA,MAAK,WAAW,GAAG,KAAKC,IAAG,WAAWD,KAAI,GAAG;AAC/C,eAAOA;AAAA,MACT;AAAA,IACF,QAAQ;AAAA,IAER;AAAA,EACF;AAGA,SAAO;AACT;AAEA,IAAM,cAAc,cAAc;AAGlCE,KAAI,KAAK,kCAAkC;AAAA,EACzC,WAAW;AAAA,EACX,YAAY,YAAY,WAAW,GAAG;AACxC,CAAC;AAKM,IAAM,2BAAN,MAA+B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQpC,MAAM,kBACJ,SACA,YACiB;AACjB,UAAM,aAAa,gBAAgB,OAAO;AAC1C,UAAM,WAAW,eAAe,YAAY,OAAO;AACnD,UAAM,YAAY,SAAS,UAAU,IAAI,OAAO;AAEhD,IAAAA,KAAI,KAAK,yCAAyC;AAAA,MAChD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI;AAEF,YAAM,EAAE,QAAQ,OAAO,IAAI,MAAM;AAAA,QAC/B,GAAG,WAAW,sBAAsB,eAAe,WAAW,oBAAoB,eAAe,YAAY,QAAQ,QAAQ,MAAM,UAAU;AAAA,MAC/I;AAEA,UAAI,UAAU,CAAC,OAAO,SAAS,wBAAwB,GAAG;AACxD,QAAAA,KAAI,KAAK,wBAAwB,EAAE,OAAO,CAAC;AAAA,MAC7C;AAEA,YAAM,WAAWD,IAAG,SAAS,QAAQ,EAAE;AACvC,MAAAC,KAAI,KAAK,iDAAiD;AAAA,QACxD;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAGD,YAAM,cAAcD,IAAG,aAAa,QAAQ;AAC5C,YAAM,iBAAiB,OAAO,WAAW,aAAa;AAAA,QACpD,aAAa;AAAA,MACf,CAAC;AAED,MAAAC,KAAI,KAAK,4BAA4B;AAAA,QACnC;AAAA,QACA;AAAA,QACA;AAAA,QACA,MAAM,YAAY;AAAA,MACpB,CAAC;AAGD,MAAAD,IAAG,WAAW,QAAQ;AAEtB,aAAO;AAAA,IACT,SAAS,OAAO;AAEd,UAAI;AACF,YAAIA,IAAG,WAAW,QAAQ,GAAG;AAC3B,UAAAA,IAAG,WAAW,QAAQ;AAAA,QACxB;AAAA,MACF,SAAS,cAAc;AACrB,QAAAC,KAAI,KAAK,cAAc,gCAAgC,EAAE,SAAS,CAAC;AAAA,MACrE;AAEA,MAAAA,KAAI,MAAM,OAAO,+CAA+C;AAAA,QAC9D;AAAA,QACA;AAAA,MACF,CAAC;AACD,YAAM,IAAI;AAAA,QACR,qCAAqC,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,MAC7F;AAAA,IACF;AAAA,EACF;AACF;;;AG7HA,SAAS,0BAA0B;AACnC,OAAOC,WAAS;AAsBT,IAAM,gBAAN,MAAoB;AAAA,EACR;AAAA,EAEjB,YAAY,QAAgB;AAE1B,QAAI,CAAC,UAAU,OAAO,KAAK,MAAM,IAAI;AACnC,YAAM,IAAI,MAAM,sCAAsC;AAAA,IACxD;AAGA,UAAM,YAAY,OAAO,SAAS,IAC9B,GAAG,OAAO,UAAU,GAAG,CAAC,CAAC,MAAM,OAAO,UAAU,OAAO,SAAS,CAAC,CAAC,KAClE;AAEJ,IAAAC,MAAI,KAAK,+BAA+B;AAAA,MACtC,cAAc,OAAO;AAAA,MACrB,eAAe;AAAA,MACf,YAAY;AAAA,IACd,CAAC;AAED,SAAK,SAAS,IAAI,mBAAmB,MAAM;AAAA,EAC7C;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,uBACJ,UACA,SACmC;AACnC,UAAM,YAAY,KAAK,IAAI;AAE3B,QAAI;AACF,MAAAA,MAAI,KAAK,iCAAiC;AAAA,QACxC;AAAA,QACA;AAAA,QACA,OAAO,cAAc;AAAA,MACvB,CAAC;AAED,YAAM,QAAQ,KAAK,OAAO,mBAAmB;AAAA,QAC3C,OAAO,cAAc;AAAA,MACvB,CAAC;AAGD,YAAM,SAAS;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAcf,YAAM,SAAS,MAAM,QAAQ,KAAK;AAAA,QAChC,MAAM,gBAAgB;AAAA,UACpB;AAAA,UACA;AAAA,YACE,UAAU;AAAA,cACR,UAAU;AAAA;AAAA,cACV,SAAS;AAAA,YACX;AAAA,UACF;AAAA,QACF,CAAC;AAAA,QACD,KAAK,cAAc,cAAc,OAAO;AAAA,MAC1C,CAAC;AAGD,UAAI,WAAW,WAAW;AACxB,cAAM,IAAI,6BAA6B;AAAA,MACzC;AAEA,YAAM,WAAW,OAAO;AACxB,YAAM,aAAa,SAAS,KAAK,EAAE,KAAK;AAGxC,UAAI,CAAC,cAAc,WAAW,SAAS,IAAI;AACzC,cAAM,IAAI,MAAM,sCAAsC;AAAA,MACxD;AAEA,YAAM,iBAAiB,KAAK,IAAI,IAAI;AAEpC,MAAAA,MAAI,KAAK,mCAAmC;AAAA,QAC1C;AAAA,QACA,kBAAkB,WAAW;AAAA,QAC7B;AAAA,QACA,YAAY,WAAW,MAAM,KAAK,EAAE;AAAA,MACtC,CAAC;AAED,aAAO;AAAA,QACL;AAAA,QACA,YAAY;AAAA;AAAA,QACZ;AAAA,QACA,QAAQ;AAAA,MACV;AAAA,IACF,SAAS,OAAO;AACd,YAAM,iBAAiB,KAAK,IAAI,IAAI;AACpC,YAAM,eAAe,KAAK,WAAW,KAAK;AAC1C,YAAM,YAAY,KAAK,cAAc,KAAK;AAG1C,YAAM,eAAwC;AAAA,QAC5C;AAAA,QACA;AAAA,QACA,OAAO,cAAc;AAAA,QACrB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAGA,UAAI,iBAAiB,OAAO;AAC1B,qBAAa,YAAY,MAAM;AAC/B,qBAAa,aAAa,MAAM;AAGhC,cAAM,WAAW;AACjB,YAAI,SAAS,UAAU;AACrB,uBAAa,iBAAiB,SAAS,SAAS;AAChD,uBAAa,qBAAqB,SAAS,SAAS;AACpD,uBAAa,eAAe,KAAK,UAAU,SAAS,SAAS,QAAQ,CAAC,CAAC;AAAA,QACzE;AACA,YAAI,SAAS,SAAS;AACpB,uBAAa,mBAAmB,SAAS;AAAA,QAC3C;AACA,YAAI,SAAS,MAAM;AACjB,uBAAa,YAAY,SAAS;AAAA,QACpC;AAAA,MACF;AAEA,MAAAA,MAAI,MAAM,oDAAoD,YAAY;AAE1E,aAAO;AAAA,QACL,YAAY;AAAA,QACZ,YAAY;AAAA,QACZ;AAAA,QACA,QAAQ;AAAA,QACR,OAAO;AAAA,MACT;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,cAAc,IAAgC;AACpD,WAAO,IAAI,QAAQ,CAAC,YAAY;AAC9B,iBAAW,MAAM,QAAQ,SAAS,GAAG,EAAE;AAAA,IACzC,CAAC;AAAA,EACH;AAAA;AAAA;AAAA;AAAA,EAKQ,WAAW,OAAwB;AACzC,QAAI,iBAAiB,OAAO;AAC1B,aAAO,MAAM;AAAA,IACf;AACA,WAAO,OAAO,KAAK;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKQ,cAAc,OAAiC;AACrD,UAAM,WAAW,OAAO,KAAK,EAAE,YAAY;AAE3C,QAAI,SAAS,SAAS,SAAS,KAAK,SAAS,SAAS,aAAa,GAAG;AACpE;AAAA,IACF;AACA,QAAI,SAAS,SAAS,YAAY,KAAK,SAAS,SAAS,OAAO,GAAG;AACjE;AAAA,IACF;AACA,QAAI,SAAS,SAAS,SAAS,GAAG;AAChC;AAAA,IACF;AACA,QAAI,SAAS,SAAS,SAAS,KAAK,SAAS,SAAS,KAAK,GAAG;AAC5D;AAAA,IACF;AACA,QAAI,SAAS,SAAS,UAAU,KAAK,SAAS,SAAS,UAAU,GAAG;AAClE;AAAA,IACF;AACA,QAAI,SAAS,SAAS,SAAS,KAAK,SAAS,SAAS,YAAY,GAAG;AACnE;AAAA,IACF;AAEA;AAAA,EACF;AACF;;;AV5MA;AAGA,IAAM,eAAeC,QAAO,cAAc;AAG1C,IAAM,oBAAoB,IAAI,yBAAyB;AACvD,IAAM,oBAAoB,IAAI,yBAAyB;AACvD,IAAM,gBAAgB,IAAI,cAAc,aAAa,CAAC;AACtD,IAAMC,qBAAoB,IAAI,wBAAwB,EAAE;AAYxD,eAAsB,oBAAoB,OAAsC;AAC9E,QAAM,EAAE,YAAY,QAAQ,KAAK,MAAM,IAAI;AAC3C,MAAI,iBAAgC;AAEpC,MAAI;AACF,IAAAC,MAAI,KAAK,2BAA2B,EAAE,YAAY,QAAQ,IAAI,CAAC;AAG/D,QAAI,sCAAqC,oCAAmC;AAC1E,MAAAA,MAAI,KAAK,sDAAsD;AAAA,QAC7D;AAAA,QACA;AAAA,MACF,CAAC;AACD;AAAA,IACF;AAGA,UAAM,WAAW,MAAMD,mBAAkB,iBAAiB,UAAU;AACpE,QAAI,YAAY,SAAS,WAAW,WAAW;AAC7C,MAAAC,MAAI,KAAK,6DAA6D;AAAA,QACpE;AAAA,QACA,eAAe,SAAS;AAAA,MAC1B,CAAC;AACD;AAAA,IACF;AAGA,QAAI,CAAC,UAAU;AACb,YAAMD,mBAAkB,cAAc,UAAU;AAChD,MAAAC,MAAI,KAAK,wCAAwC,EAAE,WAAW,CAAC;AAAA,IACjE;AAGA,UAAMD,mBAAkB,iBAAiB,UAAU;AAGnD,QAAI,WAAmC,CAAC;AAExC,QAAI,oCAAmC;AAErC,YAAM,UAAU,sBAAsB,GAAG;AACzC,UAAI,CAAC,SAAS;AACZ,cAAM,IAAI,MAAM,iDAAiD;AAAA,MACnE;AAEA,MAAAC,MAAI,KAAK,mCAAmC,EAAE,YAAY,QAAQ,CAAC;AACnE,YAAM,WAAW,gBAAgB,OAAO;AACxC,YAAM,eAAe,MAAM,cAAc,uBAAuB,UAAU,OAAO;AAEjF,UAAI,aAAa,OAAO;AAEtB,QAAAA,MAAI,KAAK,yDAAyD;AAAA,UAChE;AAAA,UACA;AAAA,UACA,aAAa,aAAa;AAAA,QAC5B,CAAC;AAGD,yBAAiB,MAAM,kBAAkB;AAAA,UACvC;AAAA,UACA;AAAA,QACF;AAEA,QAAAA,MAAI,KAAK,kDAAkD;AAAA,UACzD;AAAA,UACA;AAAA,UACA;AAAA,QACF,CAAC;AAGD,cAAM,qBAAqB,QAAQ;AAAA,UACjC;AAAA,UACA;AAAA,UACA;AAAA,UACA,UAAU,EAAE,SAAS,eAAe,aAAa,MAAM;AAAA,QACzD,CAAC;AAED,QAAAA,MAAI,KAAK,0CAA0C;AAAA,UACjD;AAAA,UACA;AAAA,QACF,CAAC;AAED;AAAA,MACF;AAGA,MAAAA,MAAI,KAAK,mCAAmC;AAAA,QAC1C;AAAA,QACA;AAAA,QACA,gBAAgB,aAAa;AAAA,QAC7B,kBAAkB,aAAa,WAAW;AAAA,MAC5C,CAAC;AAGD,YAAMD,mBAAkB,8BAA8B,YAAY;AAAA,QAChE,YAAY,aAAa;AAAA,QACzB,YAAY,aAAa;AAAA,MAC3B,CAAC;AAGD,YAAM,sBAAsB,QAAQ;AAAA,QAClC;AAAA,QACA,YAAY,aAAa;AAAA,QACzB;AAAA,MACF,CAAC;AAED,MAAAC,MAAI,KAAK,qCAAqC,EAAE,WAAW,CAAC;AAC5D;AAAA,IACF,WAAW,oCAAmC;AAE5C,MAAAA,MAAI,KAAK,6BAA6B,EAAE,YAAY,IAAI,CAAC;AACzD,uBAAiB,MAAM,kBAAkB,kBAAkB,KAAK,UAAU;AAC1E,iBAAW,EAAE,YAAY,IAAI;AAAA,IAC/B,OAAO;AACL,YAAM,IAAI,MAAM,6BAA6B,MAAM,EAAE;AAAA,IACvD;AAEA,IAAAA,MAAI,KAAK,4BAA4B;AAAA,MACnC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,YAAY,MAAM,qBAAqB,QAAQ;AAAA,MACnD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,IAAAA,MAAI,KAAK,oCAAoC;AAAA,MAC3C;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,UAAM,eACJ,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAEvD,IAAAA,MAAI,MAAM,OAAO,yBAAyB;AAAA,MACxC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,QAAI,gBAAgB;AAClB,UAAI;AACF,cAAM,iBAAiB,OAAO,cAAc;AAC5C,QAAAA,MAAI,KAAK,0CAA0C;AAAA,UACjD;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH,SAAS,cAAc;AACrB,QAAAA,MAAI,KAAK,cAAc,oCAAoC;AAAA,UACzD;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF;AAGA,UAAMD,mBAAkB;AAAA,MACtB;AAAA,MACA,0BAA0B,YAAY;AAAA,IACxC;AAAA,EACF;AACF;AAMO,IAAM,4BAA4B,IAAI;AAAA,EAC3C;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AWvNA,SAAS,gBAAAE,qBAAoB;AAC7B,SAAS,UAAAC,eAAc;AACvB,OAAOC,WAAS;;;ACFhB,SAAS,oBAAoB;AAC7B,OAAOC,WAAS;AAOT,IAAM,kBAAN,MAAsB;AAAA,EAC3B,YAA6B,QAAgB;AAAhB;AAAA,EAAiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAS9C,MAAM,WACJ,aACA,UAC2B;AAC3B,UAAM,WAAW,aAAa,KAAK,MAAM;AAEzC,IAAAC,MAAI;AAAA,MACF;AAAA,MACA,EAAE,UAAU,YAAY,YAAY,OAAO;AAAA,IAC7C;AAEA,QAAI;AAEF,YAAM,EAAE,QAAQ,MAAM,IAAI,MAAM,SAAS,OAAO,YAAY;AAAA,QAC1D;AAAA,QACA;AAAA,MACF;AAEA,UAAI,OAAO;AACT,cAAM,IAAI,MAAM,uBAAuB,MAAM,OAAO,EAAE;AAAA,MACxD;AAGA,YAAM,WAAW;AAEjB,MAAAA,MAAI,KAAK,oCAAoC;AAAA,QAC3C,UAAU,SAAS,SAAS;AAAA,QAC5B,UAAU,SAAS,SAAS;AAAA,QAC5B,cAAc,CAAC,CAAC,SAAS,QAAQ;AAAA,QACjC,YAAY,CAAC,CAAC,SAAS,QAAQ;AAAA,QAC/B,YAAY,CAAC,CAAC,SAAS,QAAQ;AAAA,QAC/B,WAAW,CAAC,CAAC,SAAS,QAAQ;AAAA,MAChC,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,MAAAA,MAAI,MAAM,OAAO,sCAAsC,EAAE,SAAS,CAAC;AACnE,YAAM;AAAA,IACR;AAAA,EACF;AACF;;;ACtCO,SAAS,oBACd,UAC4B;AAE5B,QAAM,aACJ,SAAS,QAAQ,SAAS,CAAC,GAAG,aAAa,CAAC,GAAG,cAAc;AAE/D,MAAI,CAAC,YAAY;AACf,UAAM,IAAI,MAAM,sCAAsC;AAAA,EACxD;AAGA,QAAM,aACJ,SAAS,QAAQ,SAAS,CAAC,GAAG,aAAa,CAAC,GAAG,cAAc;AAC/D,QAAM,WAAW,SAAS,SAAS;AAGnC,QAAM,YAAY,SAAS,QAAQ,YAAY,SAAS,aAAa;AACrE,QAAM,iBACJ,SAAS,QAAQ,YAAY,SAAS,mBAAmB;AAC3D,QAAM,kBAAkB,SAAS,QAAQ,SAAS,SAAS;AAE3D,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;;;AFrCA,IAAM,iBAAiBC,QAAO,gBAAgB;AAG9C,IAAM,kBAAkB,IAAI,gBAAgB,eAAe,CAAC;AAC5D,IAAMC,qBAAoB,IAAI,wBAAwB,EAAE;AASxD,eAAsB,yBAAyB,OAA6B;AAC1E,QAAM,EAAE,YAAY,gBAAgB,QAAQ,SAAS,IAAI;AAEzD,MAAI;AACF,IAAAC,MAAI,KAAK,gCAAgC;AAAA,MACvC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,cAAc,MAAM,iBAAiB,SAAS,cAAc;AAClE,IAAAA,MAAI,KAAK,gCAAgC;AAAA,MACvC;AAAA,MACA;AAAA,MACA,YAAY,YAAY;AAAA,IAC1B,CAAC;AAGD,UAAM,mBAAmB,MAAM,gBAAgB;AAAA,MAC7C;AAAA,MACA;AAAA,IACF;AAGA,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,IAAI,oBAAoB,gBAAgB;AAExC,IAAAA,MAAI,KAAK,2BAA2B;AAAA,MAClC;AAAA,MACA,kBAAkB,WAAW;AAAA,MAC7B;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,YAAY,CAAC,CAAC;AAAA,MACd,YAAY,CAAC,CAAC,iBAAiB,QAAQ;AAAA,MACvC,WAAW,CAAC,CAAC,iBAAiB,QAAQ;AAAA,IACxC,CAAC;AAID,UAAMD,mBAAkB,wBAAwB,YAAY;AAAA,MAC1D;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,IAAAC,MAAI,KAAK,yCAAyC,EAAE,WAAW,CAAC;AAGhE,UAAM,iBAAiB,OAAO,cAAc;AAC5C,IAAAA,MAAI,KAAK,6BAA6B,EAAE,YAAY,eAAe,CAAC;AAGpE,UAAM,YAAY,MAAM,sBAAsB,QAAQ;AAAA,MACpD;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,IAAAA,MAAI,KAAK,kDAAkD;AAAA,MACzD;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,UAAM,eACJ,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAEvD,IAAAA,MAAI,MAAM,OAAO,8BAA8B;AAAA,MAC7C;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAMD,mBAAkB;AAAA,MACtB;AAAA,MACA,yBAAyB,YAAY;AAAA,IACvC;AAGA,QAAI;AACF,YAAM,iBAAiB,OAAO,cAAc;AAC5C,MAAAC,MAAI,KAAK,2CAA2C;AAAA,QAClD;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACH,SAAS,cAAc;AACrB,MAAAA,MAAI,KAAK,cAAc,sCAAsC;AAAA,QAC3D;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACH;AAAA,EACF;AACF;AAGO,IAAM,iCAAiC,IAAIC;AAAA,EAChD;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AG9IA,SAAS,gBAAAC,qBAAoB;AAC7B,OAAOC,WAAS;;;ACDhB;AAYO,SAAS,oBAAoB,KAA6B;AAE/D,QAAM,iBAAiB,sBAAsB,GAAG;AAChD,MAAI,gBAAgB;AAClB;AAAA,EACF;AAGA,QAAM,cAAc,gBAAgB,GAAG;AACvC,MAAI,YAAY,aAAa,WAAW;AACtC;AAAA,EACF;AAGA,MAAI,IAAI,SAAS,eAAe,KAAK,IAAI,SAAS,UAAU,GAAG;AAC7D;AAAA,EACF;AAGA,MAAI,IAAI,SAAS,cAAc,KAAK,IAAI,SAAS,QAAQ,GAAG;AAC1D;AAAA,EACF;AAGA,MAAI,IAAI,SAAS,eAAe,GAAG;AACjC;AAAA,EACF;AAIA,MACE,IAAI,SAAS,aAAa,KAC1B,IAAI,SAAS,eAAe,KAC5B,IAAI,SAAS,gBAAgB,KAC7B,IAAI,SAAS,eAAe,KAC5B,IAAI,SAAS,WAAW,KACxB,IAAI,SAAS,QAAQ,KACrB,IAAI,SAAS,WAAW,GACxB;AACA;AAAA,EACF;AAGA;AACF;;;ADjDA;AAEA,IAAMC,gBAAe,IAAI,mBAAmB,EAAE;AAS9C,eAAsB,6BAA6B,OAKjC;AAChB,QAAM,EAAE,YAAY,KAAK,QAAQ,MAAM,IAAI;AAE3C,EAAAC,MAAI,KAAK,wCAAwC;AAAA,IAC/C;AAAA,IACA;AAAA,IACA,eAAe;AAAA,EACjB,CAAC;AAED,MAAI,cAAc;AAGlB,MAAI,4BAA+B;AACjC,UAAM,iBAAiB,oBAAoB,GAAG;AAE9C,IAAAA,MAAI,KAAK,gCAAgC;AAAA,MACvC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,QAAI,oCAAuC;AACzC,UAAI;AACF,cAAMD,cAAa,aAAa,YAAY,cAAc;AAC1D,QAAAC,MAAI,KAAK,2BAA2B;AAAA,UAClC;AAAA,UACA,WAAW;AAAA,UACX,WAAW;AAAA,QACb,CAAC;AACD,sBAAc;AAAA,MAChB,SAAS,OAAO;AACd,QAAAA,MAAI,MAAM,OAAO,oCAAoC;AAAA,UACnD;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MAEH;AAAA,IACF;AAAA,EACF,OAAO;AACL,IAAAA,MAAI,KAAK,iDAAiD;AAAA,MACxD;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAIA,MAAI;AACF,UAAM,YAAY,MAAM,8BAA8B,QAAQ;AAAA,MAC5D;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,MACA;AAAA,IACF,CAAC;AAED,IAAAA,MAAI,KAAK,8CAA8C;AAAA,MACrD;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,IAAAA,MAAI,MAAM,OAAO,sDAAsD;AAAA,MACrE;AAAA,MACA,QAAQ;AAAA,IACV,CAAC;AACD,UAAM;AAAA,EACR;AACF;AAMO,IAAM,qCAAqC,IAAIC;AAAA,EACpD;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AEvGA,SAAS,gBAAAC,qBAAoB;AAC7B,OAAOC,WAAS;AAChB,SAAS,UAAAC,eAAc;;;ACFvB,SAAS,SAAAC,cAAa;AAiBf,IAAM,wBAAwB,IAAIA;AAAA,EACvC;AAAA,EACA;AAAA,IACE,mBAAmB;AAAA,EACrB;AACF;;;ACtBA,OAAOC,WAAS;;;ACMT,IAAM,mBAAmB;AAAA;AAAA,EAE9B,SAAS;AAAA;AAAA,EAGT,SAAS;AAAA;AAAA,EAGT,SAAS,CAAC,YAAY,MAAM;AAAA;AAAA,EAG5B,iBAAiB;AAAA;AAAA,EAGjB,iBAAiB;AAAA;AAAA,EAGjB,SAAS;AAAA,IACP,aAAa;AAAA,IACb,aAAa;AAAA;AAAA,IACb,YAAY;AAAA;AAAA,IACZ,oBAAoB;AAAA,EACtB;AACF;AAMA,IAAM,0BAA0B;AAAA,EAC9B,YAAY;AAAA;AAAA,EACZ,SAAS;AAAA;AAAA,EACT,WAAW;AAAA;AACb;AAMA,IAAM,oBAAoB;AAMnB,SAAS,eAAe,WAAgC;AAC7D,MAAI,YAAY,wBAAwB,YAAY;AAClD,WAAO;AAAA,EACT;AACA,MAAI,YAAY,wBAAwB,SAAS;AAC/C,WAAO;AAAA,EACT;AACA,SAAO;AACT;AAKO,SAAS,qBAAqB,WAA2B;AAC9D,SAAO,KAAK,KAAK,YAAY,iBAAiB;AAChD;;;AD1DO,IAAM,mBAAN,MAAuB;AAAA,EAC5B,YAA6B,QAAgB;AAAhB;AAAA,EAAiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAS9C,MAAM,OAAO,KAA+C;AAC1D,UAAM,YAAY,KAAK,IAAI;AAE3B,IAAAC,MAAI,KAAK,6BAA6B,EAAE,IAAI,CAAC;AAE7C,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,eAAe,GAAG;AAC9C,YAAM,aAAa,KAAK,IAAI,IAAI;AAEhC,MAAAA,MAAI,KAAK,+BAA+B;AAAA,QACtC;AAAA,QACA;AAAA,QACA,eAAe,SAAS,KAAK,UAAU,UAAU;AAAA,QACjD,aAAa,CAAC,CAAC,SAAS,KAAK;AAAA,MAC/B,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,YAAM,aAAa,KAAK,IAAI,IAAI;AAChC,YAAM,eAAe,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAE1E,MAAAA,MAAI,MAAM,OAAO,2BAA2B;AAAA,QAC1C;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAED,YAAM,IAAI,MAAM,wBAAwB,GAAG,KAAK,YAAY,EAAE;AAAA,IAChE;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAc,eACZ,KACA,UAAU,GACwB;AAClC,QAAI;AAEF,YAAM,aAAa,IAAI,gBAAgB;AACvC,YAAM,YAAY;AAAA,QAChB,MAAM,WAAW,MAAM;AAAA,QACvB,iBAAiB;AAAA,MACnB;AAGA,YAAM,WAAW,MAAM,MAAM,GAAG,iBAAiB,OAAO,WAAW;AAAA,QACjE,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,iBAAiB,UAAU,KAAK,MAAM;AAAA,UACtC,gBAAgB;AAAA,QAClB;AAAA,QACA,MAAM,KAAK,UAAU;AAAA,UACnB;AAAA,UACA,SAAS,iBAAiB;AAAA,UAC1B,iBAAiB,iBAAiB;AAAA,QACpC,CAAC;AAAA,QACD,QAAQ,WAAW;AAAA,MACrB,CAAC;AAED,mBAAa,SAAS;AAGtB,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,YAAY,MAAM,SAAS,KAAK;AAGtC,YAAI,SAAS,WAAW,KAAK;AAC3B,gBAAM,aAAa,SAAS,QAAQ,IAAI,aAAa;AACrD,gBAAM,IAAI;AAAA,YACR,8BAA8B,cAAc,SAAS;AAAA,UACvD;AAAA,QACF;AAEA,cAAM,IAAI;AAAA,UACR,wBAAwB,SAAS,MAAM,MAAM,SAAS;AAAA,QACxD;AAAA,MACF;AAGA,YAAM,OAAQ,MAAM,SAAS,KAAK;AAElC,UAAI,CAAC,KAAK,SAAS;AACjB,cAAM,IAAI,MAAM,0CAA0C;AAAA,MAC5D;AAEA,aAAO;AAAA,IAET,SAAS,OAAO;AAEd,UAAI,UAAU,iBAAiB,QAAQ,aAAa;AAClD,cAAM,QAAQ,KAAK,sBAAsB,OAAO;AAEhD,QAAAA,MAAI,KAAK,8BAA8B;AAAA,UACrC;AAAA,UACA;AAAA,UACA,aAAa,UAAU;AAAA,UACvB,SAAS;AAAA,UACT,cAAc,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,QACrE,CAAC;AAGD,cAAM,IAAI,QAAQ,aAAW,WAAW,SAAS,KAAK,CAAC;AAGvD,eAAO,KAAK,eAAe,KAAK,UAAU,CAAC;AAAA,MAC7C;AAGA,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,sBAAsB,SAAyB;AACrD,UAAM,EAAE,aAAa,WAAW,IAAI,iBAAiB;AAGrD,UAAM,mBAAmB,cAAc,KAAK,IAAI,GAAG,UAAU,CAAC;AAG9D,UAAM,SAAS,mBAAmB,KAAK,OAAO,IAAI;AAGlD,WAAO,KAAK,IAAI,mBAAmB,QAAQ,UAAU;AAAA,EACvD;AACF;;;AE5IO,IAAM,uBAAN,MAA2B;AAAA,EAChC,YAA6BC,KAAiB;AAAjB,cAAAA;AAAA,EAAkB;AAAA;AAAA;AAAA;AAAA;AAAA,EAM/C,MAAM,cAAc,YAAyC;AAC3D,UAAM,MAAM,MAAM,KAAK,GAAG;AAAA;AAAA,gBAEd,UAAU;AAAA;AAAA;AAAA;AAKtB,QAAI,CAAC,KAAK;AAER,YAAM,WAAW,MAAM,KAAK,iBAAiB,UAAU;AACvD,UAAI,UAAU;AACZ,eAAO;AAAA,MACT;AACA,YAAM,IAAI,MAAM,qDAAqD,UAAU,EAAE;AAAA,IACnF;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,iBAAiB,YAAmC;AACxD,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA,4BAKU,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cACJ,YACA,MAWe;AACf,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA,yBAGO,KAAK,YAAY;AAAA,qBACrB,KAAK,QAAQ;AAAA,uBACX,KAAK,UAAU;AAAA,6BACT,KAAK,gBAAgB;AAAA,qBAC7B,KAAK,QAAQ;AAAA,uBACX,KAAK,UAAU;AAAA,uBACf,KAAK,UAAU;AAAA,sCACA,KAAK,yBAAyB;AAAA,qBAC/C,KAAK,QAAQ;AAAA,4BACN,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,YAAoB,SAAgC;AACtE,UAAM,KAAK,GAAG;AAAA;AAAA,sBAEI,OAAO;AAAA,4BACD,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,gBAAgB,YAAmC;AACvD,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA;AAAA,4BAKU,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,YAAoB,cAAqC;AAC1E,UAAM,KAAK,GAAG;AAAA;AAAA;AAAA;AAAA,0BAIQ,YAAY;AAAA;AAAA,4BAEV,UAAU;AAAA;AAAA,EAEpC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,iBAAiB,YAAgD;AACrE,WAAO,MAAM,KAAK,GAAG;AAAA;AAAA,4BAEG,UAAU;AAAA,SAC7B;AAAA,EACP;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,SAAS,IAAwC;AACrD,WAAO,MAAM,KAAK,GAAG;AAAA;AAAA,mBAEN,EAAE;AAAA,SACZ;AAAA,EACP;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,KAAK,QAIe;AACxB,UAAM,EAAE,OAAO,QAAQ,OAAO,IAAI;AAElC,UAAM,QAAQ,SACV,KAAK,GAAG;AAAA;AAAA,2BAEW,MAAM;AAAA;AAAA,kBAEf,KAAK,WAAW,MAAM;AAAA,YAEhC,KAAK,GAAG;AAAA;AAAA;AAAA,kBAGE,KAAK,WAAW,MAAM;AAAA;AAGpC,UAAM,QAAsB,CAAC;AAC7B,qBAAiB,QAAQ,OAAO;AAC9B,YAAM,KAAK,IAAI;AAAA,IACjB;AAEA,WAAO;AAAA,EACT;AACF;;;AJ5JA,IAAM,kBAAkBC,QAAO,iBAAiB;AAChD,IAAM,mBAAmB,IAAI,iBAAiB,gBAAgB,CAAC;AAC/D,IAAM,iBAAiB,IAAI,qBAAqB,EAAE;AAMlD,IAAM,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAMxB;AAiBA,eAAsB,wBACpB,OACe;AACf,QAAM,EAAE,YAAY,QAAQ,KAAK,MAAM,IAAI;AAG3C,MAAI,CAAC,gBAAgB,SAAS,MAAwB,GAAG;AACvD,IAAAC,MAAI,KAAK,sDAAsD;AAAA,MAC7D;AAAA,MACA;AAAA,IACF,CAAC;AACD;AAAA,EACF;AAEA,MAAI;AACF,IAAAA,MAAI,KAAK,+BAA+B;AAAA,MACtC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,WAAW,MAAM,eAAe,iBAAiB,UAAU;AAEjE,QAAI,YAAY,SAAS,oCAAkC;AACzD,MAAAA,MAAI,KAAK,sDAAsD;AAAA,QAC7D;AAAA,QACA,eAAe,SAAS;AAAA,MAC1B,CAAC;AACD;AAAA,IACF;AAGA,QAAI,CAAC,UAAU;AACb,YAAM,eAAe,cAAc,UAAU;AAC7C,MAAAA,MAAI,KAAK,sCAAsC,EAAE,WAAW,CAAC;AAAA,IAC/D;AAGA,UAAM,eAAe,iBAAiB,UAAU;AAGhD,UAAM,UAAU,MAAM,iBAAiB,OAAO,GAAG;AAEjD,QAAI,CAAC,QAAQ,WAAW,CAAC,QAAQ,KAAK,UAAU;AAC9C,YAAM,IAAI,MAAM,8DAA8D;AAAA,IAChF;AAGA,UAAM,WAAW,QAAQ,KAAK;AAC9B,UAAM,YAAY,SAAS,MAAM,KAAK,EAAE,OAAO,OAAK,EAAE,SAAS,CAAC,EAAE;AAClE,UAAM,YAAY,SAAS;AAC3B,UAAM,0BAA0B,qBAAqB,SAAS;AAG9D,UAAM,WAAW,QAAQ,KAAK,YAAY,CAAC;AAE3C,IAAAA,MAAI,KAAK,yCAAyC;AAAA,MAChD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,aAAa,CAAC,CAAC,QAAQ,KAAK;AAAA,IAC9B,CAAC;AAGD,UAAM,eAAe,cAAc,YAAY;AAAA,MAC7C,cAAc;AAAA,MACd,UAAU,QAAQ,KAAK,QAAQ;AAAA;AAAA,MAE/B,YAAY,SAAS,SAAS,SAAS;AAAA,MACvC,kBAAkB,SAAS,eAAe;AAAA,MAC1C,UAAU,SAAS,YAAY;AAAA,MAC/B,YAAY;AAAA,MACZ,YAAY;AAAA,MACZ,2BAA2B;AAAA;AAAA,MAE3B;AAAA,IACF,CAAC;AAED,IAAAA,MAAI,KAAK,6CAA6C;AAAA,MACpD;AAAA,MACA;AAAA,MACA,WAAW,SAAS,SAAS,SAAS;AAAA,IACxC,CAAC;AAGD,UAAM,YAAY,MAAM,sBAAsB,QAAQ;AAAA,MACpD;AAAA,MACA,SAAS;AAAA,MACT;AAAA,MACA;AAAA,IACF,CAAC;AAED,IAAAA,MAAI,KAAK,qCAAqC;AAAA,MAC5C;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EAEH,SAAS,OAAO;AACd,UAAM,eAAe,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAE1E,IAAAA,MAAI,MAAM,OAAO,6BAA6B;AAAA,MAC5C;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,eAAe;AAAA,MACnB;AAAA,MACA,sBAAsB,YAAY;AAAA,IACpC;AAAA,EACF;AACF;AAMO,IAAM,gCAAgC,IAAIC;AAAA,EAC/C;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AK1KA,SAAS,gBAAAC,qBAAoB;AAC7B,OAAOC,WAAS;;;ACDhB,OAAO,YAAY;AACnB,OAAOC,WAAS;AAEhB;AACA;AAEA;AAKO,IAAM,gBAAN,MAAoB;AAAA,EACR;AAAA,EAEjB,YAAY,QAAgB;AAC1B,SAAK,SAAS,IAAI,OAAO,EAAE,OAAO,CAAC;AAAA,EACrC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAYA,MAAM,gBACJ,SACA,sBACA,SAIiB;AAEjB,UAAM,eAAe,OAAO,yBAAyB,YAAY,yBAAyB;AAE1F,UAAM,eAAe,eAChB,OAAO,yBAAyB,WAAW,uBAAuB,cAAc,eACjF,KAAK;AAAA,MACH;AAAA,MACA,SAAS,eAAe;AAAA,IAC1B;AAEJ,UAAM,YAAY,SAAS,aAAa,cAAc;AAEtD,IAAAC,MAAI,KAAK,gDAAgD;AAAA,MACvD,eAAe,QAAQ;AAAA,MACvB;AAAA,MACA,QAAQ,eAAe,WAAW;AAAA,MAClC,aAAa,SAAS;AAAA,MACtB;AAAA,IACF,CAAC;AAED,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,OAAO,UAAU,OAAO;AAAA,QAClD,OAAO,cAAc;AAAA,QACrB;AAAA,QACA,OAAO;AAAA;AAAA,EAAwC,OAAO;AAAA,QACtD,aAAa,cAAc;AAAA,QAC3B,mBAAmB;AAAA,MACrB,CAAC;AAED,YAAM,UAAU,SAAS,eAAe;AAExC,MAAAA,MAAI,KAAK,kCAAkC;AAAA,QACzC,eAAe,QAAQ;AAAA,QACvB,eAAe,QAAQ;AAAA,QACvB,QAAQ,eAAe,WAAW;AAAA,QAClC,aAAa,SAAS;AAAA,MACxB,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,MAAAA,MAAI,MAAM,OAAO,0CAA0C;AAAA,QACzD,eAAe,QAAQ;AAAA,QACvB,QAAQ,eAAe,WAAW;AAAA,QAClC,cAAc,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,MACrE,CAAC;AACD,YAAM,IAAI;AAAA,QACR,qBAAqB,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,MAC7E;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,yBACN,QACA,aACQ;AAER,QAAI,eAAe,gBAAgB,MAAM,KAAK,mCAAoC;AAGlF,YAAQ,aAAa;AAAA,MACnB,KAAK;AACH,wBAAgB;AAChB;AAAA,MACF,KAAK;AACH,wBAAgB;AAChB;AAAA,MACF,KAAK;AACH,wBAAgB;AAChB;AAAA,IACJ;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAM,eAAe,QAAgB,SAAiB,WAAqC;AACzF,UAAM,cAAc,aAAa,oBAAoB;AAErD,IAAAA,MAAI,KAAK,qDAAqD;AAAA,MAC5D,cAAc,OAAO;AAAA,MACrB,eAAe,QAAQ;AAAA,MACvB,WAAW;AAAA,IACb,CAAC;AAED,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,OAAO,UAAU,OAAO;AAAA,QAClD,OAAO,oBAAoB;AAAA,QAC3B,cAAc;AAAA,QACd,OAAO;AAAA,QACP,aAAa,oBAAoB;AAAA,QACjC,mBAAmB;AAAA,MACrB,CAAC;AAED,YAAM,SAAS,SAAS,eAAe;AAEvC,MAAAA,MAAI,KAAK,uCAAuC;AAAA,QAC9C,cAAc,OAAO;AAAA,MACvB,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,MAAAA,MAAI,MAAM,OAAO,6CAA6C;AAC9D,YAAM,IAAI;AAAA,QACR,qBAAqB,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK,CAAC;AAAA,MAC7E;AAAA,IACF;AAAA,EACF;AACF;;;ADnJA,SAAS,UAAAC,eAAc;AAGvB,IAAM,eAAeA,QAAO,cAAc;AAC1C,IAAM,gBAAgB,IAAI,cAAc,aAAa,CAAC;AACtD,IAAMC,kBAAiB,IAAI,qBAAqB,EAAE;AAmBlD,eAAsB,qBACpB,OACe;AACf,QAAM,EAAE,YAAY,SAAS,WAAW,OAAO,IAAI;AAEnD,MAAI;AACF,IAAAC,MAAI,KAAK,kCAAkC;AAAA,MACzC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,cAAc,eAAe,SAAS;AAE5C,IAAAA,MAAI,KAAK,2BAA2B;AAAA,MAClC;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,aAAa,MAAM;AACvB,cAAQ,aAAa;AAAA,QACnB,KAAK;AAAc,iBAAO;AAAA,QAC1B,KAAK;AAAW,iBAAO;AAAA,QACvB,KAAK;AAAa,iBAAO;AAAA,MAC3B;AAAA,IACF,GAAG;AAGH,UAAM,UAAU,MAAM,cAAc;AAAA,MAClC;AAAA,MACA;AAAA,MACA,EAAE,WAAW,YAAY;AAAA,IAC3B;AAEA,IAAAA,MAAI,KAAK,kCAAkC;AAAA,MACzC;AAAA,MACA,eAAe,QAAQ;AAAA,MACvB;AAAA,IACF,CAAC;AAGD,UAAMD,gBAAe,cAAc,YAAY,OAAO;AAGtD,UAAMA,gBAAe,gBAAgB,UAAU;AAE/C,IAAAC,MAAI,KAAK,mCAAmC;AAAA,MAC1C;AAAA,MACA,eAAe,QAAQ;AAAA,IACzB,CAAC;AAAA,EAEH,SAAS,OAAO;AACd,UAAM,eAAe,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAE1E,IAAAA,MAAI,MAAM,OAAO,gCAAgC;AAAA,MAC/C;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAMD,gBAAe;AAAA,MACnB;AAAA,MACA,yBAAyB,YAAY;AAAA,IACvC;AAAA,EACF;AACF;AAMO,IAAM,6BAA6B,IAAIE;AAAA,EAC5C;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AElHA,SAAS,gBAAAC,qBAAoB;AAC7B,SAAS,UAAAC,eAAc;AACvB,OAAOC,WAAS;AAMhB;AAIA,IAAMC,gBAAeC,QAAO,cAAc;AAG1C,IAAMC,iBAAgB,IAAI,cAAcF,cAAa,CAAC;AACtD,IAAMG,qBAAoB,IAAI,wBAAwB,EAAE;AASxD,eAAsB,wBAAwB,OAA8B;AAC1E,QAAM,EAAE,YAAY,YAAY,OAAO,IAAI;AAE3C,MAAI;AAEF,UAAM,SAAS,gBAAgB,MAAwB,KAAK;AAE5D,IAAAC,MAAI,KAAK,+BAA+B;AAAA,MACtC;AAAA,MACA;AAAA,MACA,kBAAkB,WAAW;AAAA,MAC7B,aAAa;AAAA,IACf,CAAC;AAGD,UAAM,UAAU,MAAMF,eAAc,gBAAgB,YAAY,MAAM;AAEtE,IAAAE,MAAI,KAAK,qBAAqB;AAAA,MAC5B;AAAA,MACA;AAAA,MACA,eAAe,QAAQ;AAAA,IACzB,CAAC;AAGD,UAAMD,mBAAkB,cAAc,YAAY,OAAO;AAEzD,IAAAC,MAAI,KAAK,mEAAmE;AAAA,MAC1E;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,UAAM,eACJ,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAEvD,IAAAA,MAAI,MAAM,OAAO,6BAA6B;AAAA,MAC5C;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAMD,mBAAkB;AAAA,MACtB;AAAA,MACA,8BAA8B,YAAY;AAAA,IAC5C;AAAA,EACF;AACF;AAGO,IAAM,gCAAgC,IAAIE;AAAA,EAC/C;AAAA,EACA;AAAA,EACA;AAAA,IACE,SAAS;AAAA,EACX;AACF;;;AChFA,SAAS,eAAe;;;ACAxB,SAAS,eAAe;AAuBxB,IAAM,IAAI,IAAI,QAAQ,0BAA0B;AAAA,EAC9C,OAAO;AAAA,EACP,UAAU;AAAA;AAAA,EACV,UAAUC;AACZ,CAAC;;;ADdD,IAAO,yBAAQ,IAAI,QAAQ,WAAW;;;AEbtC,SAAS,WAAAC,gBAAe;AAiBxB,IAAOC,0BAAQ,IAAIC,SAAQ,OAAO;;;A3CWlC,IAAM,WAAkB;AAAA,EACpB;AACJ;AAEA,IAAM,WAAsB;AAAA,EACxB;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmB;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,OAAM,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACrF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmB;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,OAAM,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACrF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,OAAM,QAAO,OAAM,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACtF,aAA+B,uBAAQ,IAAI,eAAe,CAAC;AAAA,EAC/D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA2BC,wBAAQ,IAAI,eAAe,CAAC;AAAA,EAC3D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBC;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,MAAK,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACpF,aAA2BD,wBAAQ,IAAI,eAAe,CAAC;AAAA,EAC3D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBE;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,OAAM,QAAO,OAAM,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACtF,aAA2BF,wBAAQ,IAAI,eAAe,CAAC;AAAA,EAC3D;AAAA,EACA;AAAA,IACI,UAAU;AAAA,MACN,SAAmB;AAAA,MACnB,MAAmB;AAAA,MACnB,SAAmBG;AAAA,MACnB,KAAmB;AAAA,MACnB,kBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACvB;AAAA,IACA,iBAAiB,EAAC,UAAS,MAAK,QAAO,OAAM,SAAQ,OAAM,YAAW,OAAM,QAAO,CAAC,EAAC;AAAA,IACrF,aAA2BH,wBAAQ,IAAI,eAAe,CAAC;AAAA,EAC3D;AACJ;AAEA,iBAAiB,QAAQ;AACzB,iBAAiB,QAAQ;AAEzB,MAAM,IAAI,YAAY,GAAG;",
  "names": ["log", "openaiApiKey", "createTest", "generateDigestTest", "endpoints_exports", "apiCall", "streamIn", "streamOut", "streamInOut", "TEST_ENDPOINTS", "init_endpoints", "endpoints_exports", "init_endpoints", "log", "db", "db", "log", "log", "secret", "openaiApiKey", "MapReduceDigestService", "log", "api", "APIError", "log", "db", "bookmarkRepo", "dailyDigestRepo", "dailyDigestService", "create", "api", "auth", "APIError", "log", "get", "list", "update", "remove", "getDetails", "generateDailyDigest", "getDailyDigest", "listDailyDigests", "generateYesterdaysDigest", "api", "APIError", "log", "SQLDatabase", "db", "db", "db", "me", "api", "auth", "APIError", "log", "updateProfile", "getUserIds", "api", "log", "userRepo", "db", "userCreated", "api", "log", "secret", "log", "Topic", "Topic", "Topic", "log", "log", "fs", "log", "path", "fs", "log", "log", "log", "secret", "transcriptionRepo", "log", "Subscription", "secret", "log", "log", "log", "secret", "transcriptionRepo", "log", "Subscription", "Subscription", "log", "bookmarkRepo", "log", "Subscription", "Subscription", "log", "secret", "Topic", "log", "log", "db", "secret", "log", "Subscription", "Subscription", "log", "log", "log", "secret", "webContentRepo", "log", "Subscription", "Subscription", "secret", "log", "openaiApiKey", "secret", "openaiService", "transcriptionRepo", "log", "Subscription", "generateYesterdaysDigest", "Service", "encore_service_default", "Service", "create", "get", "list", "update", "remove", "getDetails", "generateDailyDigest", "getDailyDigest", "listDailyDigests", "generateYesterdaysDigest", "me", "encore_service_default", "updateProfile", "getUserIds", "userCreated"]
}
